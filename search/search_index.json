{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"SQL Case Studies","text":"<p>Documenting SQL case studies from Danny Ma's 8 Week SQL Challenge for learning and practice purposes.</p>"},{"location":"#option-1-docker-postgresql-and-sqlpad","title":"Option 1: Docker, PostgreSQL and SQLPad","text":"<p>Install docker and docker compose:</p> <pre><code>docker compose up\n</code></pre> <p>SQLPad can accessed at http://localhost:3000 or the port specified in the compose.yml file.</p> <p>Stop and remove the containers with:</p> <pre><code>docker compose down\n</code></pre>"},{"location":"#option-2-python-amazon-athena-and-jupyter-notebook","title":"Option 2: Python, Amazon Athena, and Jupyter Notebook","text":""},{"location":"#virtual-environment","title":"Virtual Environment","text":"<p>The project manager used in this project is uv:</p> <pre><code>uv sync --frozen --all-groups\n</code></pre>"},{"location":"#amazon-athena","title":"Amazon Athena","text":"<p>The Athena class can be used to interact with Amazon Athena. To use this client, the AWS principal (e.g., an IAM role or IAM user) used must have the necessary permissions for Athena.</p> <p>Customized S3 permissions are needed if a non-default bucket is to be used to store the query results (see below for more details).</p> <p>The required permissions can be encapsulated in a boto3 session instance and passed as the first argument to the constructor of the <code>Athena</code> client. The create_session utility function can be used to create the session instance. The parameters are:</p> <ul> <li> <p><code>profile_name</code>: The AWS credentials profile name to use.</p> </li> <li> <p><code>role_arn</code>: The IAM role ARN to assume. If provided, the <code>profile_name</code> must have the <code>sts:AssumeRole</code> permission.</p> </li> <li> <p><code>duration_seconds</code>: The duration, in seconds, for which the temporary credentials are valid. If role-chaining occurs, the maximum duration is 1 hour.</p> </li> </ul> <pre><code>import boto3\nfrom src.utils import create_session\n\nboto3_session = create_session(\n    profile_name=\"aws-profile-name\",\n    role_arn=os.getenv(\"ATHENA_IAM_ROLE_ARN\"), \n)\n</code></pre>"},{"location":"#s3-bucket","title":"S3 Bucket","text":"<p>The data parquet files for the case studies must be stored in an S3 bucket. All DDL queries are stored in the <code>sql</code> directory under each case study directory. These must be adjusted to point to the correct S3 uris. The data files can be uploaded to an S3 bucket using the aws cli or the console.</p> <pre><code># Create a bucket\n$ aws s3api create-bucket --bucket sql-case-studies --profile profile-name\n# Upload all data files to the bucket\n$ aws s3 cp data/ s3://sql-case-studies/ --recursive --profile profile-name \n</code></pre> <p>Optionally, query results can be configured to be stored in a custom S3 bucket, instead of the default bucket (i.e., <code>aws-athena-query-results-accountid-region</code>).</p> <p>The query result S3 uri can be stored as an environment variable, e.g. <code>ATHENA_S3_OUTPUT=s3://bucket-name/path/to/output/</code>, which can then be passed as the <code>s3_output</code> argument to the <code>Athena</code> class constructor. The client creates the default bucket if the <code>s3_output</code> argument is not provided.</p> <pre><code>import os \nfrom src.athena import Athena\nfrom src.utils import create_session\n\nboto3_session = create_session(\n    profile_name=\"aws-profile-name\",\n    role_arn=os.getenv(\"ATHENA_IAM_ROLE_ARN\"), \n)\ns3_output = os.getenv('ATHENA_S3_OUTPUT', '')\n\nathena = Athena(boto3_session=boto3_session, s3_output=s3_output)\n</code></pre>"},{"location":"#jupyter-notebook","title":"Jupyter Notebook","text":"<p>Each case study folder contains a <code>notebooks</code> directory containing Jupyter notebooks that can be used to run SQL queries.</p>"},{"location":"athena_client/","title":"Athena Client","text":"<p>               Bases: <code>object</code></p> <p>A class to interact with AWS Athena.</p>"},{"location":"athena_client/#src.athena.Athena.__init__","title":"<code>__init__(boto3_session, s3_output=None)</code>","text":"<p>Initialize the Athena instance.</p> <p>Parameters:</p> Name Type Description Default <code>boto3_session</code> <code>Session</code> <p>A boto3 session with the necessary permissions to interact with Athena.</p> required <code>s3_output</code> <code>Optional[str]</code> <p>The S3 path where the query results will be stored.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from utils import create_session # See utils module\n&gt;&gt;&gt; from athena import Athena\n&gt;&gt;&gt; boto3_session = create_session(profile_name='profile_name', role_arn='arn:aws:iam::123456789012:role/role_name')\n&gt;&gt;&gt; s3_output = 's3://bucket-name/path/to/query-results/'\n&gt;&gt;&gt; athena = Athena(boto3_session=boto3_session, s3_output=s3_output)\n</code></pre>"},{"location":"athena_client/#src.athena.Athena.create_ctas_table","title":"<code>create_ctas_table(database, query, **kwargs)</code>","text":"<p>Create a table in Athena using the Create Table As Select (CTAS) approach. Additional arguments can be passed to the CTAS query; the most important arguments are typically:</p> <ul> <li> <p><code>ctas_table</code> Optional[str]: The name of the CTAS table. If None, a name with a random string is used.</p> </li> <li> <p><code>ctas_database</code> Optional[str]: The name of the database where the CTAS table will be created. If None, <code>database</code> is used.</p> </li> <li> <p><code>s3_output</code> Optional[str]: The S3 path where the CTAS table will be stored. If None, <code>s3_output</code> attribute of the current instance is used. This may not be     desirable if the CTAS table is somewhat permanent and you want to store it in a different location than the query results.</p> </li> <li> <p><code>storage_format</code> Optional[str]: The storage format for the CTAS query results, such as ORC, PARQUET, AVRO, JSON, or TEXTFILE. PARQUET by default.</p> </li> <li> <p><code>write_compression</code> Optional[str]: The compression type to use for any storage format that allows compression to be specified.</p> </li> <li> <p><code>partition_info</code> Optional[List[str]]: A list of columns by which the CTAS table will be partitioned.</p> </li> </ul> <p>See <code>awswrangler.athena.create_ctas_table</code> for more details.</p> <p>Parameters:</p> Name Type Description Default <code>database</code> <code>str</code> <p>The name of the database to create the table in.</p> required <code>query</code> <code>str</code> <p>The query to create the table without the CREATE TABLE statement.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to <code>awswrangler.athena.create_ctas_table</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, Union[str, _QueryMetadata]]</code> <p>A dictionary with the the CTAS database and table names. If wait is False, the query ID is included, otherwise a Query metadata object is added instead.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; query = '''\n&gt;&gt;&gt;         SELECT * FROM my_database.my_table\n&gt;&gt;&gt;         WHERE date &gt;= DATE '2021-01-01';\n&gt;&gt;&gt;         '''\n&gt;&gt;&gt; athena.create_ctas_table(\n&gt;&gt;&gt;            database='my_database',\n&gt;&gt;&gt;            query=query,\n&gt;&gt;&gt;            ctas_table='my_ctas_table',\n&gt;&gt;&gt;            wait=True,\n&gt;&gt;&gt;            storage_format='PARQUET',\n&gt;&gt;&gt;            write_compression='snappy',\n&gt;&gt;&gt;            partition_info=['date']\n&gt;&gt;&gt;        )\n</code></pre>"},{"location":"athena_client/#src.athena.Athena.create_database","title":"<code>create_database(database, **kwargs)</code>","text":"<p>Create a database in Athena.</p> <p>Parameters:</p> Name Type Description Default <code>database</code> <code>str</code> <p>The name of the database to create.</p> required <code>**kwargs</code> <code>Dict[str, Any]</code> <p>Additional arguments passed to awswrangler.athena.start_query_execution.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; athena.create_database(database='my_database', wait=True) # See awswrangler.athena.start_query_execution for additional arguments\n</code></pre>"},{"location":"athena_client/#src.athena.Athena.create_table","title":"<code>create_table(database, query, **kwargs)</code>","text":"<p>Create a table in Athena.</p> <p>Parameters:</p> Name Type Description Default <code>database</code> <code>str</code> <p>The name of the database to create the table in.</p> required <code>query</code> <code>str</code> <p>The query to create the table.</p> required <code>**kwargs</code> <code>Dict[str, Any]</code> <p>Additional arguments passed to awswrangler.athena.start_query_execution.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; ddl = '''\n&gt;&gt;&gt;       CREATE EXTERNAL TABLE IF NOT EXISTS my_database.my_table (\n&gt;&gt;&gt;           id CHAR(1),\n&gt;&gt;&gt;           date TIMESTAMP\n&gt;&gt;&gt;       )\n&gt;&gt;&gt;       STORED AS PARQUET\n&gt;&gt;&gt;       LOCATION 's3://bucket-name/path/to/data/'\n&gt;&gt;&gt;       TBLPROPERTIES ('parquet.compress'='SNAPPY');\n&gt;&gt;&gt;       '''\n&gt;&gt;&gt; athena.create_table(database='my_database', query=ddl, wait=True)\n</code></pre>"},{"location":"athena_client/#src.athena.Athena.create_view","title":"<code>create_view(database, query, **kwargs)</code>","text":"<p>Create a view in Athena.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The query to create the view.</p> required <code>**kwargs</code> <code>Dict[str, Any]</code> <p>Additional arguments passed to awswrangler.athena.start_query_execution.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; ddl = '''\n&gt;&gt;&gt;       CREATE OR REPLACE VIEW my_database.my_view AS\n&gt;&gt;&gt;       SELECT * FROM my_database.my_table;\n&gt;&gt;&gt;       '''\n&gt;&gt;&gt; athena.create_view(database='my_database', query=ddl, wait=True)\n</code></pre>"},{"location":"athena_client/#src.athena.Athena.drop_database","title":"<code>drop_database(database, **kwargs)</code>","text":"<p>Drop a database in Athena.</p> <p>Parameters:</p> Name Type Description Default <code>database</code> <code>str</code> <p>The name of the database to drop.</p> required <code>**kwargs</code> <code>Dict[str, Any]</code> <p>Additional arguments passed to awswrangler.athena.start_query_execution.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; athena.drop_database(database='my_database', wait=True)\n</code></pre>"},{"location":"athena_client/#src.athena.Athena.drop_table","title":"<code>drop_table(database, table, **kwargs)</code>","text":"<p>Drop a table in Athena in the specified database.</p> <p>Parameters:</p> Name Type Description Default <code>database</code> <code>str</code> <p>The name of the database to drop the table from.</p> required <code>table</code> <code>str</code> <p>The name of the table to drop.</p> required <code>**kwargs</code> <code>Dict[str, Any]</code> <p>Additional arguments passed to awswrangler.athena.start_query_execution.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; athena.drop_table(database='my_database', table='my_table', wait=True)\n</code></pre>"},{"location":"athena_client/#src.athena.Athena.drop_view","title":"<code>drop_view(database, view, **kwargs)</code>","text":"<p>Drop a view in Athena in the specified database.</p> <p>Parameters:</p> Name Type Description Default <code>database</code> <code>str</code> <p>The name of the database to drop the view from.</p> required <code>view</code> <code>str</code> <p>The name of the view to drop.</p> required <code>**kwargs</code> <code>Dict[str, Any]</code> <p>Additional arguments passed to awswrangler.athena.start_query_execution.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; athena.drop_view(database='my_database', view='my_view', wait=True)\n</code></pre>"},{"location":"athena_client/#src.athena.Athena.query","title":"<code>query(database, query, ctas_approach=False, **kwargs)</code>","text":"<p>Execute a query in Athena.</p> <p>Parameters:</p> Name Type Description Default <code>database</code> <code>str</code> <p>The name of the database to start the query in; the query can reference other databases.</p> required <code>query</code> <code>str</code> <p>The query to execute.</p> required <code>ctas_approach</code> <code>bool</code> <p>Use the Create Table As Select (CTAS) approach to execute the query.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to awswrangler.athena.read_sql_query.</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The result of the query.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; query = '''\n&gt;&gt;&gt;         SELECT * FROM my_database.my_table\n&gt;&gt;&gt;         WHERE date &gt;= DATE '2021-01-01';\n&gt;&gt;&gt;         '''\n&gt;&gt;&gt; athena.query(database='my_database', query=query, ctas_approach=False)\n</code></pre>"},{"location":"balanced_tree/","title":"Balanced Tree","text":"In\u00a0[42]: Copied! <pre>from IPython.core.interactiveshell import InteractiveShell\n\nInteractiveShell.ast_node_interactivity = \"all\"\n\nimport os\nimport sys\n\nimport matplotlib.pyplot as plt\n\nsys.path.append(\"../../../\")\nfrom src.athena import Athena\nfrom src.utils import create_session\n</pre> from IPython.core.interactiveshell import InteractiveShell  InteractiveShell.ast_node_interactivity = \"all\"  import os import sys  import matplotlib.pyplot as plt  sys.path.append(\"../../../\") from src.athena import Athena from src.utils import create_session In\u00a0[344]: Copied! <pre>boto3_session = create_session(\n    profile_name=\"dev\",\n    role_arn=os.getenv(\"ATHENA_IAM_ROLE_ARN\"),\n)\n\nwait = True\nctas_approach = False\n\ndatabase = \"balanced_tree\"\ntables = [\"product_details\", \"product_hierarchy\", \"product_prices\", \"sales\"]\nsql_path = \"../sql/\"\n\nathena = Athena(boto3_session=boto3_session, s3_output=os.getenv(\"ATHENA_S3_OUTPUT\"))\nathena\n</pre> boto3_session = create_session(     profile_name=\"dev\",     role_arn=os.getenv(\"ATHENA_IAM_ROLE_ARN\"), )  wait = True ctas_approach = False  database = \"balanced_tree\" tables = [\"product_details\", \"product_hierarchy\", \"product_prices\", \"sales\"] sql_path = \"../sql/\"  athena = Athena(boto3_session=boto3_session, s3_output=os.getenv(\"ATHENA_S3_OUTPUT\")) athena Out[344]: <pre>Athena(boto3_session=Session(region_name='us-east-1'), s3_output=s3://sql-case-studies/query_results)</pre> In\u00a0[5]: Copied! <pre>for table in tables:\n    athena.query(\n        database=database,\n        query=f\"\"\" \n                SELECT\n                    *\n                FROM\n                    {database}.{table} TABLESAMPLE BERNOULLI(30);\n              \"\"\",\n        ctas_approach=ctas_approach,\n    )\n</pre> for table in tables:     athena.query(         database=database,         query=f\"\"\"                  SELECT                     *                 FROM                     {database}.{table} TABLESAMPLE BERNOULLI(30);               \"\"\",         ctas_approach=ctas_approach,     ) Out[5]: product_id price product_name category_id segment_id style_id category_name segment_name style_name 0 c4a632 13 Navy Oversized Jeans - Womens 1 3 7 Womens Jeans Navy Oversized 1 e31d39 10 Cream Relaxed Jeans - Womens 1 3 9 Womens Jeans Cream Relaxed 2 72f5d4 19 Indigo Rain Jacket - Womens 1 4 11 Womens Jacket Indigo Rain 3 9ec847 54 Grey Fashion Jacket - Womens 1 4 12 Womens Jacket Grey Fashion 4 c8d436 10 Teal Button Up Shirt - Mens 2 5 14 Mens Shirt Teal Button Up Out[5]: id parent_id level_text level_name 0 1 &lt;NA&gt; Womens Category 1 3 1 Jeans Segment 2 10 4 Khaki Suit Style 3 13 5 White Tee Style 4 17 6 White Striped Style Out[5]: id product_id price 0 8 e83aa3 32 1 11 72f5d4 19 2 14 c8d436 10 3 15 2a2353 57 4 17 b9a74d 17 Out[5]: prod_id qty price discount member txn_id start_txn_time 0 2feb6b 2 29 17 t 54f307 2021-02-13 01:59:43.296 1 f084eb 3 36 21 t 26cc98 2021-01-19 01:39:00.345 2 c4a632 1 13 21 f ef648d 2021-01-27 02:18:17.164 3 5d267b 3 40 21 f ef648d 2021-01-27 02:18:17.164 4 2feb6b 2 29 23 t fba96f 2021-03-03 00:32:56.054 ... ... ... ... ... ... ... ... 4464 2a2353 1 57 14 f 72cd63 2021-01-13 12:47:38.630 4465 f084eb 3 36 14 f 72cd63 2021-01-13 12:47:38.630 4466 c4a632 3 13 13 t f15ab3 2021-03-20 12:01:22.944 4467 e83aa3 5 32 13 t f15ab3 2021-03-20 12:01:22.944 4468 5d267b 2 40 1 t 93620b 2021-03-01 07:11:24.662 <p>4469 rows \u00d7 7 columns</p> In\u00a0[20]: Copied! <pre>query = \"\"\" \nSELECT\n    s.prod_id,\n    d.product_name,\n    SUM(s.qty) AS total_qty\nFROM\n    balanced_tree.sales AS s\n        LEFT JOIN balanced_tree.product_details AS d ON s.prod_id = d.product_id\nGROUP BY\n    s.prod_id,\n    d.product_name\nORDER BY\n    total_qty DESC;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  SELECT     s.prod_id,     d.product_name,     SUM(s.qty) AS total_qty FROM     balanced_tree.sales AS s         LEFT JOIN balanced_tree.product_details AS d ON s.prod_id = d.product_id GROUP BY     s.prod_id,     d.product_name ORDER BY     total_qty DESC; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[20]: prod_id product_name total_qty 0 9ec847 Grey Fashion Jacket - Womens 3876 1 c4a632 Navy Oversized Jeans - Womens 3856 2 2a2353 Blue Polo Shirt - Mens 3819 3 5d267b White Tee Shirt - Mens 3800 4 f084eb Navy Solid Socks - Mens 3792 5 e83aa3 Black Straight Jeans - Womens 3786 6 2feb6b Pink Fluro Polkadot Socks - Mens 3770 7 72f5d4 Indigo Rain Jacket - Womens 3757 8 d5e9a6 Khaki Suit Jacket - Womens 3752 9 e31d39 Cream Relaxed Jeans - Womens 3707 10 b9a74d White Striped Socks - Mens 3655 11 c8d436 Teal Button Up Shirt - Mens 3646 In\u00a0[15]: Copied! <pre>query = \"\"\" \nSELECT\n    FORMAT('$%,d', SUM(qty * price)) AS total_rev_before_discount\nFROM\n    balanced_tree.sales;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  SELECT     FORMAT('$%,d', SUM(qty * price)) AS total_rev_before_discount FROM     balanced_tree.sales; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[15]: total_rev_before_discount 0 $1,289,453 In\u00a0[22]: Copied! <pre>query = \"\"\" \nSELECT\n    s.prod_id,\n    d.product_name,\n    SUM(s.qty * s.price * (s.discount / 100.0)) AS discount_amt\nFROM\n    balanced_tree.sales AS s \n        LEFT JOIN balanced_tree.product_details AS d ON s.prod_id = d.product_id\nGROUP BY\n    s.prod_id,\n    d.product_name\nORDER BY\n    discount_amt DESC;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  SELECT     s.prod_id,     d.product_name,     SUM(s.qty * s.price * (s.discount / 100.0)) AS discount_amt FROM     balanced_tree.sales AS s          LEFT JOIN balanced_tree.product_details AS d ON s.prod_id = d.product_id GROUP BY     s.prod_id,     d.product_name ORDER BY     discount_amt DESC; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[22]: prod_id product_name discount_amt 0 2a2353 Blue Polo Shirt - Mens 26819.07 1 9ec847 Grey Fashion Jacket - Womens 25391.88 2 5d267b White Tee Shirt - Mens 18377.60 3 f084eb Navy Solid Socks - Mens 16650.36 4 e83aa3 Black Straight Jeans - Womens 14744.96 5 2feb6b Pink Fluro Polkadot Socks - Mens 12952.27 6 d5e9a6 Khaki Suit Jacket - Womens 10243.05 7 72f5d4 Indigo Rain Jacket - Womens 8642.53 8 b9a74d White Striped Socks - Mens 7410.81 9 c4a632 Navy Oversized Jeans - Womens 6135.61 10 e31d39 Cream Relaxed Jeans - Womens 4463.40 11 c8d436 Teal Button Up Shirt - Mens 4397.60 In\u00a0[25]: Copied! <pre>query = \"\"\" \nSELECT\n    FORMAT('$%,.2f', SUM(qty * price * (discount / 100.0))) AS total_discount_amt\nFROM\n    balanced_tree.sales;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  SELECT     FORMAT('$%,.2f', SUM(qty * price * (discount / 100.0))) AS total_discount_amt FROM     balanced_tree.sales; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[25]: total_discount_amt 0 $156,229.14 In\u00a0[26]: Copied! <pre>query = \"\"\" \nSELECT\n    COUNT(DISTINCT txn_id) AS transactions_count\nFROM\n    balanced_tree.sales;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  SELECT     COUNT(DISTINCT txn_id) AS transactions_count FROM     balanced_tree.sales; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[26]: transactions_count 0 2500 In\u00a0[29]: Copied! <pre>query = \"\"\" \nWITH product_count_per_txn AS (\n    SELECT\n        txn_id,\n        COUNT(DISTINCT prod_id) AS product_count\n    FROM\n        balanced_tree.sales\n    GROUP BY\n        txn_id\n)\nSELECT\n    AVG(product_count) AS avg_unique_prod_per_txn\nFROM\n    product_count_per_txn;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  WITH product_count_per_txn AS (     SELECT         txn_id,         COUNT(DISTINCT prod_id) AS product_count     FROM         balanced_tree.sales     GROUP BY         txn_id ) SELECT     AVG(product_count) AS avg_unique_prod_per_txn FROM     product_count_per_txn; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[29]: avg_unique_prod_per_txn 0 6.038 In\u00a0[56]: Copied! <pre>query = \"\"\" \nWITH rev_per_txn AS (\n    SELECT\n        txn_id,\n        SUM(\n            qty * (price * (1 - (discount / 100.0)))\n        ) AS rev\n    FROM\n        balanced_tree.sales\n    GROUP BY\n        txn_id\n)\nSELECT\n    APPROX_PERCENTILE(rev, 0.25) AS pct_25,\n    APPROX_PERCENTILE(rev, 0.50) AS pct_50,\n    AVG(rev) AS avg,\n    APPROX_PERCENTILE(rev, 0.75) AS pct_75\nFROM\n    rev_per_txn;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  WITH rev_per_txn AS (     SELECT         txn_id,         SUM(             qty * (price * (1 - (discount / 100.0)))         ) AS rev     FROM         balanced_tree.sales     GROUP BY         txn_id ) SELECT     APPROX_PERCENTILE(rev, 0.25) AS pct_25,     APPROX_PERCENTILE(rev, 0.50) AS pct_50,     AVG(rev) AS avg,     APPROX_PERCENTILE(rev, 0.75) AS pct_75 FROM     rev_per_txn; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[56]: pct_25 pct_50 avg pct_75 0 327.416522 437.63246 453.289544 572.559528 In\u00a0[44]: Copied! <pre>query = \"\"\" \nSELECT\n    txn_id,\n    SUM(\n        qty * (price * (1 - (discount / 100.0)))\n    ) AS rev\nFROM\n    balanced_tree.sales\nGROUP BY\n    txn_id;\n\"\"\"\n\nrev_per_txn = athena.query(database=database, query=query, ctas_approach=ctas_approach)[\n    \"rev\"\n].values\n\n\nplt.figure(figsize=(8, 6))\nplt.violinplot(rev_per_txn, showmeans=True, showextrema=True, showmedians=True)\nplt.title(\"Revenue Per Transactions\")\nplt.ylabel(\"Dollars\")\nplt.show();\n</pre> query = \"\"\"  SELECT     txn_id,     SUM(         qty * (price * (1 - (discount / 100.0)))     ) AS rev FROM     balanced_tree.sales GROUP BY     txn_id; \"\"\"  rev_per_txn = athena.query(database=database, query=query, ctas_approach=ctas_approach)[     \"rev\" ].values   plt.figure(figsize=(8, 6)) plt.violinplot(rev_per_txn, showmeans=True, showextrema=True, showmedians=True) plt.title(\"Revenue Per Transactions\") plt.ylabel(\"Dollars\") plt.show(); In\u00a0[47]: Copied! <pre>query = \"\"\" \nWITH discount_per_txn AS (\n    SELECT\n        txn_id,\n        SUM(\n            qty * (price * (discount / 100.0))\n        ) AS discount\n    FROM\n        balanced_tree.sales\n    GROUP BY\n        txn_id\n)\nSELECT\n    APPROX_PERCENTILE(discount, 0.25) AS pct_25,\n    APPROX_PERCENTILE(discount, 0.50) AS pct_50,\n    AVG(discount) AS avg,\n    APPROX_PERCENTILE(discount, 0.75) AS pct_75\nFROM\n    discount_per_txn;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  WITH discount_per_txn AS (     SELECT         txn_id,         SUM(             qty * (price * (discount / 100.0))         ) AS discount     FROM         balanced_tree.sales     GROUP BY         txn_id ) SELECT     APPROX_PERCENTILE(discount, 0.25) AS pct_25,     APPROX_PERCENTILE(discount, 0.50) AS pct_50,     AVG(discount) AS avg,     APPROX_PERCENTILE(discount, 0.75) AS pct_75 FROM     discount_per_txn; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[47]: pct_25 pct_50 avg pct_75 0 25.514365 53.596724 62.491656 91.783839 In\u00a0[48]: Copied! <pre>query = \"\"\" \nSELECT\n    txn_id,\n    SUM(\n        qty * (price * (discount / 100.0))\n    ) AS discount\nFROM\n    balanced_tree.sales\nGROUP BY\n    txn_id;\n\"\"\"\n\ndiscount_per_txn = athena.query(\n    database=database, query=query, ctas_approach=ctas_approach\n)[\"discount\"].values\n\n\nplt.figure(figsize=(8, 6))\nplt.violinplot(discount_per_txn, showmeans=True, showextrema=True, showmedians=True)\nplt.title(\"Discount Per Transactions\")\nplt.ylabel(\"Dollars\")\nplt.show();\n</pre> query = \"\"\"  SELECT     txn_id,     SUM(         qty * (price * (discount / 100.0))     ) AS discount FROM     balanced_tree.sales GROUP BY     txn_id; \"\"\"  discount_per_txn = athena.query(     database=database, query=query, ctas_approach=ctas_approach )[\"discount\"].values   plt.figure(figsize=(8, 6)) plt.violinplot(discount_per_txn, showmeans=True, showextrema=True, showmedians=True) plt.title(\"Discount Per Transactions\") plt.ylabel(\"Dollars\") plt.show(); In\u00a0[59]: Copied! <pre>query = \"\"\" \nSELECT\n    SUM(CASE WHEN member = 't' THEN 1.0 ELSE 0.0 END) / COUNT(*) * 100.0 AS pct_member,\n    SUM(CASE WHEN member = 'f' THEN 1.0 ELSE 0.0 END) / COUNT(*) * 100.0 AS pct_non_member\nFROM\n    balanced_tree.sales;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  SELECT     SUM(CASE WHEN member = 't' THEN 1.0 ELSE 0.0 END) / COUNT(*) * 100.0 AS pct_member,     SUM(CASE WHEN member = 'f' THEN 1.0 ELSE 0.0 END) / COUNT(*) * 100.0 AS pct_non_member FROM     balanced_tree.sales; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[59]: pct_member pct_non_member 0 60.026499 39.973501 In\u00a0[77]: Copied! <pre>query = \"\"\" \nWITH rev AS (\n    SELECT\n        member,\n        txn_id,\n        SUM(\n            qty * (price * (1 - (discount / 100.0)))\n        ) AS rev\n    FROM\n        balanced_tree.sales\n    GROUP BY\n        member,\n        txn_id\n)\nSELECT\n    member,\n    AVG(rev) AS avg_rev\nFROM\n    rev\nGROUP BY\n    member;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  WITH rev AS (     SELECT         member,         txn_id,         SUM(             qty * (price * (1 - (discount / 100.0)))         ) AS rev     FROM         balanced_tree.sales     GROUP BY         member,         txn_id ) SELECT     member,     AVG(rev) AS avg_rev FROM     rev GROUP BY     member; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[77]: member avg_rev 0 f 452.007769 1 t 454.136963 In\u00a0[88]: Copied! <pre>query = \"\"\" \nSELECT\n    s.prod_id,\n    d.product_name,\n    FORMAT('$%,d', SUM(s.qty * s.price)) AS total_rev\nFROM\n    balanced_tree.sales AS s\n        LEFT JOIN balanced_tree.product_details AS d ON s.prod_id = d.product_id\nGROUP BY\n    s.prod_id,\n    d.product_name\nORDER BY\n    SUM(s.qty * s.price) DESC\nLIMIT \n    3;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  SELECT     s.prod_id,     d.product_name,     FORMAT('$%,d', SUM(s.qty * s.price)) AS total_rev FROM     balanced_tree.sales AS s         LEFT JOIN balanced_tree.product_details AS d ON s.prod_id = d.product_id GROUP BY     s.prod_id,     d.product_name ORDER BY     SUM(s.qty * s.price) DESC LIMIT      3; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[88]: prod_id product_name total_rev 0 2a2353 Blue Polo Shirt - Mens $217,683 1 9ec847 Grey Fashion Jacket - Womens $209,304 2 5d267b White Tee Shirt - Mens $152,000 In\u00a0[98]: Copied! <pre>query = \"\"\" \nSELECT\n    d.segment_name,\n    FORMAT('$%,d', SUM(s.qty * s.price)) AS total_rev,\n    FORMAT('%,d', SUM(s.qty)) AS total_qty,\n    FORMAT('$%,.2f', SUM(s.qty * s.price * (s.discount / 100.0))) AS total_discount\nFROM\n    balanced_tree.sales AS s\n        LEFT JOIN balanced_tree.product_details AS d ON s.prod_id = d.product_id\nGROUP BY\n    d.segment_name\nORDER BY\n    d.segment_name;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  SELECT     d.segment_name,     FORMAT('$%,d', SUM(s.qty * s.price)) AS total_rev,     FORMAT('%,d', SUM(s.qty)) AS total_qty,     FORMAT('$%,.2f', SUM(s.qty * s.price * (s.discount / 100.0))) AS total_discount FROM     balanced_tree.sales AS s         LEFT JOIN balanced_tree.product_details AS d ON s.prod_id = d.product_id GROUP BY     d.segment_name ORDER BY     d.segment_name; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[98]: segment_name total_rev total_qty total_discount 0 Jacket $366,983 11,385 $44,277.46 1 Jeans $208,350 11,349 $25,343.97 2 Shirt $406,143 11,265 $49,594.27 3 Socks $307,977 11,217 $37,013.44 In\u00a0[148]: Copied! <pre>def top_selling_prod_query(group_col: str, top_n: int) -&gt; str:\n    return f\"\"\" \n            WITH qty_sold AS (\n                SELECT\n                    d.{group_col},\n                    d.product_name,\n                    SUM(s.qty) AS qty_sold\n                FROM\n                    balanced_tree.sales AS s\n                        LEFT JOIN balanced_tree.product_details AS d ON s.prod_id = d.product_id\n                GROUP BY\n                    d.{group_col},\n                    d.product_name\n            ),\n\n            ranked_data AS (\n                SELECT\n                    {group_col},\n                    product_name,\n                    qty_sold,\n                    DENSE_RANK() OVER(PARTITION BY {group_col} ORDER BY qty_sold DESC) AS ranks\n                FROM\n                    qty_sold\n            )\n            SELECT\n                {group_col},\n                product_name,\n                qty_sold\n            FROM\n                ranked_data\n            WHERE\n                1 = 1\n                AND ranks &lt;= {top_n}\n            ORDER BY\n                {group_col},\n                qty_sold DESC;\n            \"\"\"\n\n\nathena.query(\n    database=database,\n    query=top_selling_prod_query(\"segment_name\", 3),\n    ctas_approach=ctas_approach,\n)\n</pre> def top_selling_prod_query(group_col: str, top_n: int) -&gt; str:     return f\"\"\"              WITH qty_sold AS (                 SELECT                     d.{group_col},                     d.product_name,                     SUM(s.qty) AS qty_sold                 FROM                     balanced_tree.sales AS s                         LEFT JOIN balanced_tree.product_details AS d ON s.prod_id = d.product_id                 GROUP BY                     d.{group_col},                     d.product_name             ),              ranked_data AS (                 SELECT                     {group_col},                     product_name,                     qty_sold,                     DENSE_RANK() OVER(PARTITION BY {group_col} ORDER BY qty_sold DESC) AS ranks                 FROM                     qty_sold             )             SELECT                 {group_col},                 product_name,                 qty_sold             FROM                 ranked_data             WHERE                 1 = 1                 AND ranks &lt;= {top_n}             ORDER BY                 {group_col},                 qty_sold DESC;             \"\"\"   athena.query(     database=database,     query=top_selling_prod_query(\"segment_name\", 3),     ctas_approach=ctas_approach, ) Out[148]: segment_name product_name qty_sold 0 Jacket Grey Fashion Jacket - Womens 3876 1 Jacket Indigo Rain Jacket - Womens 3757 2 Jacket Khaki Suit Jacket - Womens 3752 3 Jeans Navy Oversized Jeans - Womens 3856 4 Jeans Black Straight Jeans - Womens 3786 5 Jeans Cream Relaxed Jeans - Womens 3707 6 Shirt Blue Polo Shirt - Mens 3819 7 Shirt White Tee Shirt - Mens 3800 8 Shirt Teal Button Up Shirt - Mens 3646 9 Socks Navy Solid Socks - Mens 3792 10 Socks Pink Fluro Polkadot Socks - Mens 3770 11 Socks White Striped Socks - Mens 3655 In\u00a0[117]: Copied! <pre>query = \"\"\" \nSELECT\n    d.category_name,\n    FORMAT('$%,d', SUM(s.qty * s.price)) AS total_rev,\n    FORMAT('%,d', SUM(s.qty)) AS total_qty,\n    FORMAT('$%,.2f', SUM(s.qty * s.price * (s.discount / 100.0))) AS total_discount\nFROM\n    balanced_tree.sales AS s \n        LEFT JOIN balanced_tree.product_details AS d ON s.prod_id = d.product_id\nGROUP BY\n    d.category_name\nORDER BY\n    d.category_name DESC;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  SELECT     d.category_name,     FORMAT('$%,d', SUM(s.qty * s.price)) AS total_rev,     FORMAT('%,d', SUM(s.qty)) AS total_qty,     FORMAT('$%,.2f', SUM(s.qty * s.price * (s.discount / 100.0))) AS total_discount FROM     balanced_tree.sales AS s          LEFT JOIN balanced_tree.product_details AS d ON s.prod_id = d.product_id GROUP BY     d.category_name ORDER BY     d.category_name DESC; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[117]: category_name total_rev total_qty total_discount 0 Womens $575,333 22,734 $69,621.43 1 Mens $714,120 22,482 $86,607.71 In\u00a0[149]: Copied! <pre>athena.query(\n    database=database,\n    query=top_selling_prod_query(\"category_name\", 2),\n    ctas_approach=ctas_approach,\n)\n</pre> athena.query(     database=database,     query=top_selling_prod_query(\"category_name\", 2),     ctas_approach=ctas_approach, ) Out[149]: category_name product_name qty_sold 0 Mens Blue Polo Shirt - Mens 3819 1 Mens White Tee Shirt - Mens 3800 2 Womens Grey Fashion Jacket - Womens 3876 3 Womens Navy Oversized Jeans - Womens 3856 In\u00a0[164]: Copied! <pre>def rev_pct_query(group_col_1: str, group_col_2: str) -&gt; str:\n    return f\"\"\" \n            WITH rev_data AS (\n                SELECT\n                    d.{group_col_1},\n                    d.{group_col_2},\n                    SUM(s.qty * (s.price * (1 - (s.discount / 100.0)))) AS rev\n                FROM\n                    balanced_tree.sales AS s\n                        LEFT JOIN balanced_tree.product_details AS d ON s.prod_id = d.product_id\n                GROUP BY\n                    d.{group_col_1},\n                    d.{group_col_2}\n            )\n            SELECT\n                {group_col_1},\n                {group_col_2},\n                FORMAT('$%,.2f', rev) AS rev,\n                CONCAT(\n                    FORMAT('%.2f', ROUND((rev / SUM(rev) OVER(PARTITION BY {group_col_1})) * 100.0, 2)), \n                    '%'\n                ) AS rev_pct_split\n            FROM\n                rev_data\n            ORDER BY\n                {group_col_1},\n                {group_col_2};\n            \"\"\"\n\n\nathena.query(\n    database=database,\n    query=rev_pct_query(\"segment_name\", \"product_name\"),\n    ctas_approach=ctas_approach,\n)\n</pre> def rev_pct_query(group_col_1: str, group_col_2: str) -&gt; str:     return f\"\"\"              WITH rev_data AS (                 SELECT                     d.{group_col_1},                     d.{group_col_2},                     SUM(s.qty * (s.price * (1 - (s.discount / 100.0)))) AS rev                 FROM                     balanced_tree.sales AS s                         LEFT JOIN balanced_tree.product_details AS d ON s.prod_id = d.product_id                 GROUP BY                     d.{group_col_1},                     d.{group_col_2}             )             SELECT                 {group_col_1},                 {group_col_2},                 FORMAT('$%,.2f', rev) AS rev,                 CONCAT(                     FORMAT('%.2f', ROUND((rev / SUM(rev) OVER(PARTITION BY {group_col_1})) * 100.0, 2)),                      '%'                 ) AS rev_pct_split             FROM                 rev_data             ORDER BY                 {group_col_1},                 {group_col_2};             \"\"\"   athena.query(     database=database,     query=rev_pct_query(\"segment_name\", \"product_name\"),     ctas_approach=ctas_approach, ) Out[164]: segment_name product_name rev rev_pct_split 0 Jacket Grey Fashion Jacket - Womens $183,912.12 56.99% 1 Jacket Indigo Rain Jacket - Womens $62,740.47 19.44% 2 Jacket Khaki Suit Jacket - Womens $76,052.95 23.57% 3 Jeans Black Straight Jeans - Womens $106,407.04 58.14% 4 Jeans Cream Relaxed Jeans - Womens $32,606.60 17.82% 5 Jeans Navy Oversized Jeans - Womens $43,992.39 24.04% 6 Shirt Blue Polo Shirt - Mens $190,863.93 53.53% 7 Shirt Teal Button Up Shirt - Mens $32,062.40 8.99% 8 Shirt White Tee Shirt - Mens $133,622.40 37.48% 9 Socks Navy Solid Socks - Mens $119,861.64 44.24% 10 Socks Pink Fluro Polkadot Socks - Mens $96,377.73 35.57% 11 Socks White Striped Socks - Mens $54,724.19 20.20% In\u00a0[165]: Copied! <pre>athena.query(\n    database=database,\n    query=rev_pct_query(\"category_name\", \"segment_name\"),\n    ctas_approach=ctas_approach,\n)\n</pre> athena.query(     database=database,     query=rev_pct_query(\"category_name\", \"segment_name\"),     ctas_approach=ctas_approach, ) Out[165]: category_name segment_name rev rev_pct_split 0 Mens Shirt $356,548.73 56.82% 1 Mens Socks $270,963.56 43.18% 2 Womens Jacket $322,705.54 63.81% 3 Womens Jeans $183,006.03 36.19% In\u00a0[189]: Copied! <pre>rev_formula = \"s.qty * (s.price * (1 - (s.discount / 100.0)))\"\n\nquery = f\"\"\"\nSELECT\n    d.category_name,\n    FORMAT('$%,.2f', SUM({rev_formula})) AS rev,\n    CONCAT(\n        FORMAT(\n            '%.2f',\n            ROUND(SUM({rev_formula}) / SUM(SUM({rev_formula})) OVER() * 100.0, 2)\n        ),\n        '%'\n    ) AS rev_pct_split\nFROM\n    balanced_tree.sales AS s\n        LEFT JOIN balanced_tree.product_details AS d ON s.prod_id = d.product_id\nGROUP BY\n    d.category_name;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> rev_formula = \"s.qty * (s.price * (1 - (s.discount / 100.0)))\"  query = f\"\"\" SELECT     d.category_name,     FORMAT('$%,.2f', SUM({rev_formula})) AS rev,     CONCAT(         FORMAT(             '%.2f',             ROUND(SUM({rev_formula}) / SUM(SUM({rev_formula})) OVER() * 100.0, 2)         ),         '%'     ) AS rev_pct_split FROM     balanced_tree.sales AS s         LEFT JOIN balanced_tree.product_details AS d ON s.prod_id = d.product_id GROUP BY     d.category_name; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[189]: category_name rev rev_pct_split 0 Womens $505,711.57 44.63% 1 Mens $627,512.29 55.37% In\u00a0[229]: Copied! <pre>query = \"\"\" \nSELECT\n    s.prod_id,\n    d.product_name,\n    CONCAT(\n        FORMAT('%.2f', TRY_CAST(COUNT(DISTINCT s.txn_id) AS DOUBLE) / TRY_CAST((SELECT COUNT(DISTINCT txn_id) FROM balanced_tree.sales) AS DOUBLE) * 100.0),\n        '%'\n    ) AS penetration\nFROM\n    balanced_tree.sales AS s\n        LEFT JOIN balanced_tree.product_details AS d ON s.prod_id = d.product_id\nGROUP BY\n    s.prod_id,\n    d.product_name\nORDER BY\n    TRY_CAST(COUNT(DISTINCT s.txn_id) AS DOUBLE) / TRY_CAST((SELECT COUNT(DISTINCT txn_id) FROM balanced_tree.sales) AS DOUBLE) DESC;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  SELECT     s.prod_id,     d.product_name,     CONCAT(         FORMAT('%.2f', TRY_CAST(COUNT(DISTINCT s.txn_id) AS DOUBLE) / TRY_CAST((SELECT COUNT(DISTINCT txn_id) FROM balanced_tree.sales) AS DOUBLE) * 100.0),         '%'     ) AS penetration FROM     balanced_tree.sales AS s         LEFT JOIN balanced_tree.product_details AS d ON s.prod_id = d.product_id GROUP BY     s.prod_id,     d.product_name ORDER BY     TRY_CAST(COUNT(DISTINCT s.txn_id) AS DOUBLE) / TRY_CAST((SELECT COUNT(DISTINCT txn_id) FROM balanced_tree.sales) AS DOUBLE) DESC; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[229]: prod_id product_name penetration 0 f084eb Navy Solid Socks - Mens 51.24% 1 9ec847 Grey Fashion Jacket - Womens 51.00% 2 c4a632 Navy Oversized Jeans - Womens 50.96% 3 2a2353 Blue Polo Shirt - Mens 50.72% 4 5d267b White Tee Shirt - Mens 50.72% 5 2feb6b Pink Fluro Polkadot Socks - Mens 50.32% 6 72f5d4 Indigo Rain Jacket - Womens 50.00% 7 d5e9a6 Khaki Suit Jacket - Womens 49.88% 8 e83aa3 Black Straight Jeans - Womens 49.84% 9 e31d39 Cream Relaxed Jeans - Womens 49.72% 10 b9a74d White Striped Socks - Mens 49.72% 11 c8d436 Teal Button Up Shirt - Mens 49.68% In\u00a0[217]: Copied! <pre>query = \"\"\" \nWITH txn_by_prod AS (\n    SELECT \n        prod_id,\n        COUNT(DISTINCT txn_id) AS txn_count_by_product\n    FROM \n        balanced_tree.sales\n    GROUP BY \n        prod_id\n),\n\ntotal_txn AS (\n    SELECT\n        COUNT(DISTINCT txn_id) AS total_txn_count\n    FROM \n        balanced_tree.sales\n)\nSELECT\n    d.product_id,\n    d.product_name,\n    CONCAT(\n        FORMAT('%.2f', TRY_CAST(t.txn_count_by_product AS DOUBLE) / TRY_CAST(total_t.total_txn_count AS DOUBLE) * 100.0),\n        '%'\n    ) AS penetration\nFROM \n    txn_by_prod AS t\n        CROSS JOIN total_txn AS total_t\n        INNER JOIN balanced_tree.product_details AS d ON t.prod_id = d.product_id\nORDER BY \n    TRY_CAST(t.txn_count_by_product AS DOUBLE) / TRY_CAST(total_t.total_txn_count AS DOUBLE) DESC;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  WITH txn_by_prod AS (     SELECT          prod_id,         COUNT(DISTINCT txn_id) AS txn_count_by_product     FROM          balanced_tree.sales     GROUP BY          prod_id ),  total_txn AS (     SELECT         COUNT(DISTINCT txn_id) AS total_txn_count     FROM          balanced_tree.sales ) SELECT     d.product_id,     d.product_name,     CONCAT(         FORMAT('%.2f', TRY_CAST(t.txn_count_by_product AS DOUBLE) / TRY_CAST(total_t.total_txn_count AS DOUBLE) * 100.0),         '%'     ) AS penetration FROM      txn_by_prod AS t         CROSS JOIN total_txn AS total_t         INNER JOIN balanced_tree.product_details AS d ON t.prod_id = d.product_id ORDER BY      TRY_CAST(t.txn_count_by_product AS DOUBLE) / TRY_CAST(total_t.total_txn_count AS DOUBLE) DESC; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[217]: product_id product_name penetration 0 f084eb Navy Solid Socks - Mens 51.24% 1 9ec847 Grey Fashion Jacket - Womens 51.00% 2 c4a632 Navy Oversized Jeans - Womens 50.96% 3 2a2353 Blue Polo Shirt - Mens 50.72% 4 5d267b White Tee Shirt - Mens 50.72% 5 2feb6b Pink Fluro Polkadot Socks - Mens 50.32% 6 72f5d4 Indigo Rain Jacket - Womens 50.00% 7 d5e9a6 Khaki Suit Jacket - Womens 49.88% 8 e83aa3 Black Straight Jeans - Womens 49.84% 9 e31d39 Cream Relaxed Jeans - Womens 49.72% 10 b9a74d White Striped Socks - Mens 49.72% 11 c8d436 Teal Button Up Shirt - Mens 49.68% <p>This approach works because the result of the <code>total_txn</code> CTE is a single-row, single-column value (<code>total_txn_count</code>). When using a <code>CROSS JOIN</code>, also known as a Cartesian product, this value is matched with each row from the <code>txn_by_prod</code> CTE. This allows the single value from <code>total_txn</code> to be appended to every row in <code>txn_by_prod</code>, effectively making <code>total_txn_count</code> available for calculations or comparisons across all rows in <code>txn_by_prod</code>.</p> <ol> <li><code>txn_by_prod</code> CTE:<ul> <li>Aggregates transaction counts by product (<code>prod_id</code>) and returns one row per product with its corresponding transaction count.</li> </ul> </li> <li><code>total_txn</code> CTE:<ul> <li>Calculates the total number of distinct transactions across all products, resulting in a single scalar value.</li> </ul> </li> <li><code>CROSS JOIN</code>:<ul> <li>Combines every row in <code>txn_by_prod</code> with the single value from <code>total_txn</code> (<code>total_txn_count</code>), ensuring that the total transaction count is appended to each product's row.</li> </ul> </li> </ol> <p>This approach effectively replicates the single result of <code>total_txn</code> across all rows of <code>txn_by_prod</code>. It is particularly useful for cases where the total value needs to be compared or used in calculations for each product.</p> In\u00a0[221]: Copied! <pre>query = \"\"\" \nWITH txn_by_prod AS (\n    SELECT \n        prod_id,\n        COUNT(DISTINCT txn_id) AS txn_count_by_product\n    FROM \n        balanced_tree.sales\n    GROUP BY \n        prod_id\n),\n\ntotal_txn AS (\n    SELECT\n        COUNT(DISTINCT txn_id) AS total_txn_count\n    FROM \n        balanced_tree.sales\n)\nSELECT\n    *\nFROM\n    txn_by_prod\n        CROSS JOIN total_txn;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  WITH txn_by_prod AS (     SELECT          prod_id,         COUNT(DISTINCT txn_id) AS txn_count_by_product     FROM          balanced_tree.sales     GROUP BY          prod_id ),  total_txn AS (     SELECT         COUNT(DISTINCT txn_id) AS total_txn_count     FROM          balanced_tree.sales ) SELECT     * FROM     txn_by_prod         CROSS JOIN total_txn; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[221]: prod_id txn_count_by_product total_txn_count 0 f084eb 1281 2500 1 c4a632 1274 2500 2 72f5d4 1250 2500 3 e83aa3 1246 2500 4 5d267b 1268 2500 5 2feb6b 1258 2500 6 c8d436 1242 2500 7 d5e9a6 1247 2500 8 2a2353 1268 2500 9 e31d39 1243 2500 10 b9a74d 1243 2500 11 9ec847 1275 2500 In\u00a0[288]: Copied! <pre>query = \"\"\" \nSELECT\n    ARRAY[CAST(d.product_id AS VARCHAR)]  AS combo,\n    CAST(d.product_id AS VARCHAR)         AS product,\n    1                                     AS product_counter\nFROM \n    balanced_tree.product_details AS d;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  SELECT     ARRAY[CAST(d.product_id AS VARCHAR)]  AS combo,     CAST(d.product_id AS VARCHAR)         AS product,     1                                     AS product_counter FROM      balanced_tree.product_details AS d; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[288]: combo product product_counter 0 [c4a632] c4a632 1 1 [e83aa3] e83aa3 1 2 [e31d39] e31d39 1 3 [d5e9a6] d5e9a6 1 4 [72f5d4] 72f5d4 1 5 [9ec847] 9ec847 1 6 [5d267b] 5d267b 1 7 [c8d436] c8d436 1 8 [2a2353] 2a2353 1 9 [f084eb] f084eb 1 10 [b9a74d] b9a74d 1 11 [2feb6b] 2feb6b 1 In\u00a0[297]: Copied! <pre>query = \"\"\" \nWITH RECURSIVE output_table (combo, product, product_counter) AS (\n\n    -- === 1) BASE CASE ===\n\n    SELECT\n        ARRAY[CAST(d.product_id AS VARCHAR)]  AS combo,\n        CAST(d.product_id AS VARCHAR)         AS product,\n        1                                     AS product_counter\n    FROM \n        balanced_tree.product_details AS d\n\n    UNION ALL\n\n    -- === 2) RECURSIVE STEP ===\n\n    SELECT\n        output_table.combo || ARRAY[CAST(d.product_id AS VARCHAR)]  AS combo,\n        CAST(d.product_id AS VARCHAR)                               AS product,\n        output_table.product_counter + 1                            AS product_counter\n    FROM \n        output_table \n        INNER JOIN balanced_tree.product_details AS d ON d.product_id &gt; output_table.product\n    WHERE \n        output_table.product_counter &lt; 3\n)\n\nSELECT \n    *\nFROM \n    output_table\nWHERE \n    product_counter = 3;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  WITH RECURSIVE output_table (combo, product, product_counter) AS (      -- === 1) BASE CASE ===      SELECT         ARRAY[CAST(d.product_id AS VARCHAR)]  AS combo,         CAST(d.product_id AS VARCHAR)         AS product,         1                                     AS product_counter     FROM          balanced_tree.product_details AS d      UNION ALL      -- === 2) RECURSIVE STEP ===      SELECT         output_table.combo || ARRAY[CAST(d.product_id AS VARCHAR)]  AS combo,         CAST(d.product_id AS VARCHAR)                               AS product,         output_table.product_counter + 1                            AS product_counter     FROM          output_table          INNER JOIN balanced_tree.product_details AS d ON d.product_id &gt; output_table.product     WHERE          output_table.product_counter &lt; 3 )  SELECT      * FROM      output_table WHERE      product_counter = 3; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[297]: combo product product_counter 0 [5d267b, 72f5d4, c4a632] c4a632 3 1 [2a2353, 72f5d4, c4a632] c4a632 3 2 [2feb6b, 72f5d4, c4a632] c4a632 3 3 [72f5d4, 9ec847, c4a632] c4a632 3 4 [5d267b, 9ec847, c4a632] c4a632 3 ... ... ... ... 215 [2a2353, 9ec847, b9a74d] b9a74d 3 216 [2feb6b, 9ec847, b9a74d] b9a74d 3 217 [2a2353, 5d267b, b9a74d] b9a74d 3 218 [2feb6b, 5d267b, b9a74d] b9a74d 3 219 [2a2353, 2feb6b, b9a74d] b9a74d 3 <p>220 rows \u00d7 3 columns</p> In\u00a0[298]: Copied! <pre>query = \"\"\" \nSELECT\n    txn_id,\n    ARRAY_AGG(CAST(s.prod_id AS VARCHAR)) AS products\nFROM \n    balanced_tree.sales AS s\nGROUP BY \n    txn_id;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  SELECT     txn_id,     ARRAY_AGG(CAST(s.prod_id AS VARCHAR)) AS products FROM      balanced_tree.sales AS s GROUP BY      txn_id; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[298]: txn_id products 0 ef648d [c4a632, e83aa3, d5e9a6, 72f5d4, 5d267b, f084e... 1 e9a1dd [72f5d4, 9ec847, 2a2353, f084eb] 2 cf6517 [d5e9a6, 72f5d4, 5d267b, b9a74d] 3 47251d [72f5d4, 5d267b, 2a2353, f084eb] 4 c75ea6 [e83aa3, e31d39, 72f5d4, 9ec847, 5d267b, f084e... ... ... ... 2495 08000a [c4a632, d5e9a6, 9ec847, 2a2353, f084eb, b9a74d] 2496 5efab7 [c4a632, d5e9a6, 5d267b, 2a2353, f084eb, b9a74d] 2497 f905a2 [e31d39, d5e9a6, 72f5d4, 9ec847, 5d267b, c8d43... 2498 ba59c1 [c4a632, e83aa3, 72f5d4, 9ec847, 5d267b, c8d43... 2499 93620b [e83aa3, d5e9a6, 5d267b] <p>2500 rows \u00d7 2 columns</p> In\u00a0[299]: Copied! <pre>query = \"\"\" \nWITH RECURSIVE output_table (combo, product, product_counter) AS (\n\n    -- === 1) BASE CASE ===\n\n    SELECT\n        ARRAY[CAST(d.product_id AS VARCHAR)]  AS combo,\n        CAST(d.product_id AS VARCHAR)         AS product,\n        1                                     AS product_counter\n    FROM \n        balanced_tree.product_details AS d\n\n    UNION ALL\n\n    -- === 2) RECURSIVE STEP ===\n\n    SELECT\n        output_table.combo || ARRAY[CAST(d.product_id AS VARCHAR)]  AS combo,\n        CAST(d.product_id AS VARCHAR)                               AS product,\n        output_table.product_counter + 1                            AS product_counter\n    FROM \n        output_table \n        INNER JOIN balanced_tree.product_details AS d ON d.product_id &gt; output_table.product\n    WHERE \n        output_table.product_counter &lt; 3\n),\n\ncte_transaction_products (txn_id, products) AS (\n    SELECT\n        txn_id,\n        ARRAY_AGG(CAST(s.prod_id AS VARCHAR)) AS products\n    FROM \n        balanced_tree.sales AS s\n    GROUP BY \n        txn_id\n)\n\nSELECT\n    tp.txn_id,\n    combos.combo,\n    tp.products\nFROM \n    cte_transaction_products AS tp\n    CROSS JOIN (\n        SELECT \n            combo\n        FROM \n            output_table\n        WHERE \n            product_counter = 3\n    ) AS combos (combo)\nWHERE \n    CARDINALITY(ARRAY_INTERSECT(combos.combo, tp.products)) = CARDINALITY(combos.combo);\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  WITH RECURSIVE output_table (combo, product, product_counter) AS (      -- === 1) BASE CASE ===      SELECT         ARRAY[CAST(d.product_id AS VARCHAR)]  AS combo,         CAST(d.product_id AS VARCHAR)         AS product,         1                                     AS product_counter     FROM          balanced_tree.product_details AS d      UNION ALL      -- === 2) RECURSIVE STEP ===      SELECT         output_table.combo || ARRAY[CAST(d.product_id AS VARCHAR)]  AS combo,         CAST(d.product_id AS VARCHAR)                               AS product,         output_table.product_counter + 1                            AS product_counter     FROM          output_table          INNER JOIN balanced_tree.product_details AS d ON d.product_id &gt; output_table.product     WHERE          output_table.product_counter &lt; 3 ),  cte_transaction_products (txn_id, products) AS (     SELECT         txn_id,         ARRAY_AGG(CAST(s.prod_id AS VARCHAR)) AS products     FROM          balanced_tree.sales AS s     GROUP BY          txn_id )  SELECT     tp.txn_id,     combos.combo,     tp.products FROM      cte_transaction_products AS tp     CROSS JOIN (         SELECT              combo         FROM              output_table         WHERE              product_counter = 3     ) AS combos (combo) WHERE      CARDINALITY(ARRAY_INTERSECT(combos.combo, tp.products)) = CARDINALITY(combos.combo); \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[299]: txn_id combo products 0 9a36b3 [5d267b, 72f5d4, c4a632] [c4a632, e83aa3, e31d39, 72f5d4, 9ec847, 5d267... 1 f4dfc4 [5d267b, 72f5d4, c4a632] [c4a632, 72f5d4, 5d267b, 2a2353] 2 d044e6 [5d267b, 72f5d4, c4a632] [c4a632, e83aa3, 72f5d4, 5d267b, c8d436, f084eb] 3 84d86c [5d267b, 72f5d4, c4a632] [c4a632, e83aa3, d5e9a6, 72f5d4, 5d267b, c8d43... 4 4bb50e [5d267b, 72f5d4, c4a632] [c4a632, e31d39, 72f5d4, 5d267b, f084eb, 2feb6b] ... ... ... ... 69438 f1c999 [2a2353, d5e9a6, f084eb] [e83aa3, e31d39, d5e9a6, 9ec847, 2a2353, f084e... 69439 f1c999 [2feb6b, d5e9a6, f084eb] [e83aa3, e31d39, d5e9a6, 9ec847, 2a2353, f084e... 69440 f1c999 [2a2353, 9ec847, f084eb] [e83aa3, e31d39, d5e9a6, 9ec847, 2a2353, f084e... 69441 f1c999 [2feb6b, 9ec847, f084eb] [e83aa3, e31d39, d5e9a6, 9ec847, 2a2353, f084e... 69442 f1c999 [2a2353, 2feb6b, f084eb] [e83aa3, e31d39, d5e9a6, 9ec847, 2a2353, f084e... <p>69443 rows \u00d7 3 columns</p> In\u00a0[300]: Copied! <pre>query = \"\"\" \nWITH RECURSIVE output_table (combo, product, product_counter) AS (\n\n    -- === 1) BASE CASE ===\n\n    SELECT\n        ARRAY[CAST(d.product_id AS VARCHAR)]  AS combo,\n        CAST(d.product_id AS VARCHAR)         AS product,\n        1                                     AS product_counter\n    FROM \n        balanced_tree.product_details AS d\n\n    UNION ALL\n\n    -- === 2) RECURSIVE STEP ===\n\n    SELECT\n        output_table.combo || ARRAY[CAST(d.product_id AS VARCHAR)]  AS combo,\n        CAST(d.product_id AS VARCHAR)                               AS product,\n        output_table.product_counter + 1                            AS product_counter\n    FROM \n        output_table \n        INNER JOIN balanced_tree.product_details AS d ON d.product_id &gt; output_table.product\n    WHERE \n        output_table.product_counter &lt; 3\n),\n\ncte_transaction_products (txn_id, products) AS (\n    SELECT\n        txn_id,\n        ARRAY_AGG(CAST(s.prod_id AS VARCHAR)) AS products\n    FROM \n        balanced_tree.sales AS s\n    GROUP BY \n        txn_id\n),\n\ncte_combo_transactions (txn_id, combo, products) AS (\n    SELECT\n        tp.txn_id,\n        combos.combo,\n        tp.products\n    FROM \n        cte_transaction_products AS tp\n        CROSS JOIN (\n            SELECT \n                combo\n            FROM \n                output_table\n            WHERE \n                product_counter = 3\n        ) AS combos (combo)\n    WHERE \n        CARDINALITY(ARRAY_INTERSECT(combos.combo, tp.products)) = CARDINALITY(combos.combo)\n)\n\nSELECT\n    combo,\n    COUNT(DISTINCT txn_id) AS transaction_count,\n    RANK() OVER (ORDER BY COUNT(DISTINCT txn_id) DESC) AS combo_rank\nFROM \n    cte_combo_transactions\nGROUP BY \n    combo;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  WITH RECURSIVE output_table (combo, product, product_counter) AS (      -- === 1) BASE CASE ===      SELECT         ARRAY[CAST(d.product_id AS VARCHAR)]  AS combo,         CAST(d.product_id AS VARCHAR)         AS product,         1                                     AS product_counter     FROM          balanced_tree.product_details AS d      UNION ALL      -- === 2) RECURSIVE STEP ===      SELECT         output_table.combo || ARRAY[CAST(d.product_id AS VARCHAR)]  AS combo,         CAST(d.product_id AS VARCHAR)                               AS product,         output_table.product_counter + 1                            AS product_counter     FROM          output_table          INNER JOIN balanced_tree.product_details AS d ON d.product_id &gt; output_table.product     WHERE          output_table.product_counter &lt; 3 ),  cte_transaction_products (txn_id, products) AS (     SELECT         txn_id,         ARRAY_AGG(CAST(s.prod_id AS VARCHAR)) AS products     FROM          balanced_tree.sales AS s     GROUP BY          txn_id ),  cte_combo_transactions (txn_id, combo, products) AS (     SELECT         tp.txn_id,         combos.combo,         tp.products     FROM          cte_transaction_products AS tp         CROSS JOIN (             SELECT                  combo             FROM                  output_table             WHERE                  product_counter = 3         ) AS combos (combo)     WHERE          CARDINALITY(ARRAY_INTERSECT(combos.combo, tp.products)) = CARDINALITY(combos.combo) )  SELECT     combo,     COUNT(DISTINCT txn_id) AS transaction_count,     RANK() OVER (ORDER BY COUNT(DISTINCT txn_id) DESC) AS combo_rank FROM      cte_combo_transactions GROUP BY      combo; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[300]: combo transaction_count combo_rank 0 [5d267b, 9ec847, c8d436] 352 1 1 [72f5d4, e83aa3, f084eb] 349 2 2 [5d267b, c4a632, c8d436] 347 3 3 [2a2353, 9ec847, c8d436] 347 3 4 [2a2353, 9ec847, b9a74d] 347 3 ... ... ... ... 215 [2feb6b, 5d267b, e31d39] 290 216 216 [2feb6b, 72f5d4, d5e9a6] 289 217 217 [5d267b, b9a74d, e31d39] 288 218 218 [72f5d4, c4a632, d5e9a6] 287 219 219 [d5e9a6, e31d39, e83aa3] 283 220 <p>220 rows \u00d7 3 columns</p> In\u00a0[301]: Copied! <pre>query = \"\"\" \nWITH RECURSIVE output_table (combo, product, product_counter) AS (\n\n    -- === 1) BASE CASE ===\n\n    SELECT\n        ARRAY[CAST(d.product_id AS VARCHAR)]  AS combo,\n        CAST(d.product_id AS VARCHAR)         AS product,\n        1                                     AS product_counter\n    FROM \n        balanced_tree.product_details AS d\n\n    UNION ALL\n\n    -- === 2) RECURSIVE STEP ===\n\n    SELECT\n        output_table.combo || ARRAY[CAST(d.product_id AS VARCHAR)]  AS combo,\n        CAST(d.product_id AS VARCHAR)                               AS product,\n        output_table.product_counter + 1                            AS product_counter\n    FROM \n        output_table \n        INNER JOIN balanced_tree.product_details AS d ON d.product_id &gt; output_table.product\n    WHERE \n        output_table.product_counter &lt; 3\n),\n\ncte_transaction_products (txn_id, products) AS (\n    SELECT\n        txn_id,\n        ARRAY_AGG(CAST(s.prod_id AS VARCHAR)) AS products\n    FROM \n        balanced_tree.sales AS S\n    GROUP BY \n        txn_id\n),\n\ncte_combo_transactions (txn_id, combo, products) AS (\n    SELECT\n        tp.txn_id,\n        combos.combo,\n        tp.products\n    FROM \n        cte_transaction_products AS tp\n        CROSS JOIN (\n            SELECT \n                combo\n            FROM \n                output_table\n            WHERE \n                product_counter = 3\n        ) AS combos (combo)\n    WHERE \n        CARDINALITY(ARRAY_INTERSECT(combos.combo, tp.products)) = CARDINALITY(combos.combo)\n),\n\ncte_ranked_combos (combo, transaction_count, combo_rank) AS (\n    SELECT\n        combo,\n        COUNT(DISTINCT txn_id) AS transaction_count,\n        RANK() OVER (ORDER BY COUNT(DISTINCT txn_id) DESC) AS combo_rank\n    FROM \n        cte_combo_transactions\n    GROUP BY \n        combo\n)\n\nSELECT\n    cct.txn_id,\n    t.prod_id\nFROM \n    cte_combo_transactions AS cct\n    INNER JOIN cte_ranked_combos AS crc ON cct.combo = crc.combo\n    CROSS JOIN UNNEST(crc.combo) AS t(prod_id)\nWHERE \n    crc.combo_rank = 1;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  WITH RECURSIVE output_table (combo, product, product_counter) AS (      -- === 1) BASE CASE ===      SELECT         ARRAY[CAST(d.product_id AS VARCHAR)]  AS combo,         CAST(d.product_id AS VARCHAR)         AS product,         1                                     AS product_counter     FROM          balanced_tree.product_details AS d      UNION ALL      -- === 2) RECURSIVE STEP ===      SELECT         output_table.combo || ARRAY[CAST(d.product_id AS VARCHAR)]  AS combo,         CAST(d.product_id AS VARCHAR)                               AS product,         output_table.product_counter + 1                            AS product_counter     FROM          output_table          INNER JOIN balanced_tree.product_details AS d ON d.product_id &gt; output_table.product     WHERE          output_table.product_counter &lt; 3 ),  cte_transaction_products (txn_id, products) AS (     SELECT         txn_id,         ARRAY_AGG(CAST(s.prod_id AS VARCHAR)) AS products     FROM          balanced_tree.sales AS S     GROUP BY          txn_id ),  cte_combo_transactions (txn_id, combo, products) AS (     SELECT         tp.txn_id,         combos.combo,         tp.products     FROM          cte_transaction_products AS tp         CROSS JOIN (             SELECT                  combo             FROM                  output_table             WHERE                  product_counter = 3         ) AS combos (combo)     WHERE          CARDINALITY(ARRAY_INTERSECT(combos.combo, tp.products)) = CARDINALITY(combos.combo) ),  cte_ranked_combos (combo, transaction_count, combo_rank) AS (     SELECT         combo,         COUNT(DISTINCT txn_id) AS transaction_count,         RANK() OVER (ORDER BY COUNT(DISTINCT txn_id) DESC) AS combo_rank     FROM          cte_combo_transactions     GROUP BY          combo )  SELECT     cct.txn_id,     t.prod_id FROM      cte_combo_transactions AS cct     INNER JOIN cte_ranked_combos AS crc ON cct.combo = crc.combo     CROSS JOIN UNNEST(crc.combo) AS t(prod_id) WHERE      crc.combo_rank = 1; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[301]: txn_id prod_id 0 dfec5f 5d267b 1 dfec5f 9ec847 2 dfec5f c8d436 3 28d244 5d267b 4 28d244 9ec847 ... ... ... 1051 f77659 9ec847 1052 f77659 c8d436 1053 72cd63 5d267b 1054 72cd63 9ec847 1055 72cd63 c8d436 <p>1056 rows \u00d7 2 columns</p> In\u00a0[311]: Copied! <pre>query = \"\"\" \nWITH RECURSIVE output_table (combo, product, product_counter) AS (\n\n    -- === 1) BASE CASE ===\n\n    SELECT\n        ARRAY[CAST(d.product_id AS VARCHAR)]  AS combo,\n        CAST(d.product_id AS VARCHAR)         AS product,\n        1                                     AS product_counter\n    FROM \n        balanced_tree.product_details AS d\n\n    UNION ALL\n\n    -- === 2) RECURSIVE STEP ===\n\n    SELECT\n        output_table.combo || ARRAY[CAST(d.product_id AS VARCHAR)]  AS combo,\n        CAST(d.product_id AS VARCHAR)                               AS product,\n        output_table.product_counter + 1                            AS product_counter\n    FROM \n        output_table \n        INNER JOIN balanced_tree.product_details AS d ON d.product_id &gt; output_table.product\n    WHERE \n        output_table.product_counter &lt; 3\n),\n\ncte_transaction_products (txn_id, products) AS (\n    SELECT\n        txn_id,\n        ARRAY_AGG(CAST(s.prod_id AS VARCHAR)) AS products\n    FROM \n        balanced_tree.sales AS s\n    GROUP BY \n        txn_id\n),\n\ncte_combo_transactions (txn_id, combo, products) AS (\n    SELECT\n        tp.txn_id,\n        combos.combo,\n        tp.products\n    FROM \n        cte_transaction_products AS tp\n        CROSS JOIN (\n            SELECT \n                combo\n            FROM \n                output_table\n            WHERE \n                product_counter = 3\n        ) AS combos (combo)\n    WHERE \n        CARDINALITY(ARRAY_INTERSECT(combos.combo, tp.products)) = CARDINALITY(combos.combo)\n),\n\ncte_ranked_combos (combo, transaction_count, combo_rank) AS (\n    SELECT\n        combo,\n        COUNT(DISTINCT txn_id)                                  AS transaction_count,\n        RANK() OVER (ORDER BY COUNT(DISTINCT txn_id) DESC)      AS combo_rank\n    FROM \n        cte_combo_transactions\n    GROUP BY \n        combo\n),\n\ncte_most_common_combo_product_transactions (txn_id, prod_id) AS (\n    SELECT\n        cct.txn_id,\n        t.prod_id\n    FROM \n        cte_combo_transactions AS cct\n        INNER JOIN cte_ranked_combos AS crc ON cct.combo = crc.combo\n        CROSS JOIN UNNEST(crc.combo) AS t(prod_id)\n    WHERE \n        crc.combo_rank = 1\n)\n\nSELECT\n    d.product_id                                                                    AS product_id,\n    d.product_name                                                                  AS product_name,\n    COUNT(DISTINCT s.txn_id)                                                        AS combo_transaction_count,\n    SUM(s.qty)                                                                      AS quantity,\n    FORMAT('$%,d', ROUND(SUM(s.qty * s.price), 2))                                  AS revenue_after_discount,\n    FORMAT('$%,.2f', ROUND(SUM(s.qty * s.price * (s.discount / 100.0)), 2))         AS discount,\n    FORMAT('$%,.2f', ROUND(SUM(s.qty * s.price * (1 - s.discount / 100.0)), 2))     AS net_revenue\nFROM \n    balanced_tree.sales AS s\n    INNER JOIN cte_most_common_combo_product_transactions AS top_combo ON s.txn_id = top_combo.txn_id AND s.prod_id = top_combo.prod_id\n    INNER JOIN balanced_tree.product_details AS d ON s.prod_id = d.product_id\nGROUP BY\n    d.product_id,\n    d.product_name\nORDER BY\n    combo_transaction_count DESC,\n    d.product_id ASC;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  WITH RECURSIVE output_table (combo, product, product_counter) AS (      -- === 1) BASE CASE ===      SELECT         ARRAY[CAST(d.product_id AS VARCHAR)]  AS combo,         CAST(d.product_id AS VARCHAR)         AS product,         1                                     AS product_counter     FROM          balanced_tree.product_details AS d      UNION ALL      -- === 2) RECURSIVE STEP ===      SELECT         output_table.combo || ARRAY[CAST(d.product_id AS VARCHAR)]  AS combo,         CAST(d.product_id AS VARCHAR)                               AS product,         output_table.product_counter + 1                            AS product_counter     FROM          output_table          INNER JOIN balanced_tree.product_details AS d ON d.product_id &gt; output_table.product     WHERE          output_table.product_counter &lt; 3 ),  cte_transaction_products (txn_id, products) AS (     SELECT         txn_id,         ARRAY_AGG(CAST(s.prod_id AS VARCHAR)) AS products     FROM          balanced_tree.sales AS s     GROUP BY          txn_id ),  cte_combo_transactions (txn_id, combo, products) AS (     SELECT         tp.txn_id,         combos.combo,         tp.products     FROM          cte_transaction_products AS tp         CROSS JOIN (             SELECT                  combo             FROM                  output_table             WHERE                  product_counter = 3         ) AS combos (combo)     WHERE          CARDINALITY(ARRAY_INTERSECT(combos.combo, tp.products)) = CARDINALITY(combos.combo) ),  cte_ranked_combos (combo, transaction_count, combo_rank) AS (     SELECT         combo,         COUNT(DISTINCT txn_id)                                  AS transaction_count,         RANK() OVER (ORDER BY COUNT(DISTINCT txn_id) DESC)      AS combo_rank     FROM          cte_combo_transactions     GROUP BY          combo ),  cte_most_common_combo_product_transactions (txn_id, prod_id) AS (     SELECT         cct.txn_id,         t.prod_id     FROM          cte_combo_transactions AS cct         INNER JOIN cte_ranked_combos AS crc ON cct.combo = crc.combo         CROSS JOIN UNNEST(crc.combo) AS t(prod_id)     WHERE          crc.combo_rank = 1 )  SELECT     d.product_id                                                                    AS product_id,     d.product_name                                                                  AS product_name,     COUNT(DISTINCT s.txn_id)                                                        AS combo_transaction_count,     SUM(s.qty)                                                                      AS quantity,     FORMAT('$%,d', ROUND(SUM(s.qty * s.price), 2))                                  AS revenue_after_discount,     FORMAT('$%,.2f', ROUND(SUM(s.qty * s.price * (s.discount / 100.0)), 2))         AS discount,     FORMAT('$%,.2f', ROUND(SUM(s.qty * s.price * (1 - s.discount / 100.0)), 2))     AS net_revenue FROM      balanced_tree.sales AS s     INNER JOIN cte_most_common_combo_product_transactions AS top_combo ON s.txn_id = top_combo.txn_id AND s.prod_id = top_combo.prod_id     INNER JOIN balanced_tree.product_details AS d ON s.prod_id = d.product_id GROUP BY     d.product_id,     d.product_name ORDER BY     combo_transaction_count DESC,     d.product_id ASC; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[311]: product_id product_name combo_transaction_count quantity revenue_after_discount discount net_revenue 0 5d267b White Tee Shirt - Mens 352 1007 $40,280 $5,049.20 $35,230.80 1 9ec847 Grey Fashion Jacket - Womens 352 1062 $57,348 $6,997.86 $50,350.14 2 c8d436 Teal Button Up Shirt - Mens 352 1054 $10,540 $1,325.30 $9,214.70 In\u00a0[328]: Copied! <pre>query = \"\"\" \nSELECT\n    id,\n    id AS category_id,\n    CAST(NULL AS INTEGER) AS segment_id,\n    CAST(NULL AS INTEGER) AS style_id,\n    level_text AS category_name,\n    CAST(NULL AS VARCHAR) AS segment_name,  \n    CAST(NULL AS VARCHAR) AS style_name   \nFROM\n    balanced_tree.product_hierarchy\nWHERE\n    parent_id IS NULL;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  SELECT     id,     id AS category_id,     CAST(NULL AS INTEGER) AS segment_id,     CAST(NULL AS INTEGER) AS style_id,     level_text AS category_name,     CAST(NULL AS VARCHAR) AS segment_name,       CAST(NULL AS VARCHAR) AS style_name    FROM     balanced_tree.product_hierarchy WHERE     parent_id IS NULL; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[328]: id category_id segment_id style_id category_name segment_name style_name 0 1 1 &lt;NA&gt; &lt;NA&gt; Womens &lt;NA&gt; &lt;NA&gt; 1 2 2 &lt;NA&gt; &lt;NA&gt; Mens &lt;NA&gt; &lt;NA&gt; In\u00a0[355]: Copied! <pre>query = \"\"\" \nWITH RECURSIVE output_table (\n    id, \n    category_id, \n    segment_id, \n    style_id, \n    category_name, \n    segment_name, \n    style_name\n) AS (\n\n  -- 1) BASE CASE\n\n  SELECT\n      ph.id,\n      ph.id AS category_id,\n      CAST(NULL AS INTEGER) AS segment_id,\n      CAST(NULL AS INTEGER) AS style_id,\n      ph.level_text AS category_name,\n      CAST(NULL AS VARCHAR) AS segment_name,  \n      CAST(NULL AS VARCHAR) AS style_name    \n  FROM \n      balanced_tree.product_hierarchy AS ph\n  WHERE \n      parent_id IS NULL\n\n  UNION ALL\n\n  -- 2) RECURSIVE STEP\n\n  SELECT\n      child.id,\n      output_table.category_id,\n      \n      -- Assign segment_id if the child belongs to a category\n\n      CASE\n        WHEN child.level_name = 'Segment' THEN child.id\n        ELSE output_table.segment_id\n      END AS segment_id,\n\n      -- Assign style_id if the child belongs to a segment\n\n      CASE\n        WHEN child.level_name = 'Style' THEN child.id\n        ELSE output_table.style_id\n      END AS style_id,\n\n      output_table.category_name,\n\n      -- Assign segment_name\n\n      CASE\n        WHEN child.level_name = 'Segment' THEN child.level_text\n        ELSE output_table.segment_name\n      END AS segment_name,\n\n      -- Assign style_name\n\n      CASE\n        WHEN child.level_name = 'Style' THEN child.level_text\n        ELSE output_table.style_name\n      END AS style_name\n  FROM \n      output_table INNER JOIN balanced_tree.product_hierarchy AS child ON output_table.id = child.parent_id\n)\n\nSELECT\n    * \nFROM\n    output_table;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  WITH RECURSIVE output_table (     id,      category_id,      segment_id,      style_id,      category_name,      segment_name,      style_name ) AS (    -- 1) BASE CASE    SELECT       ph.id,       ph.id AS category_id,       CAST(NULL AS INTEGER) AS segment_id,       CAST(NULL AS INTEGER) AS style_id,       ph.level_text AS category_name,       CAST(NULL AS VARCHAR) AS segment_name,         CAST(NULL AS VARCHAR) AS style_name       FROM        balanced_tree.product_hierarchy AS ph   WHERE        parent_id IS NULL    UNION ALL    -- 2) RECURSIVE STEP    SELECT       child.id,       output_table.category_id,              -- Assign segment_id if the child belongs to a category        CASE         WHEN child.level_name = 'Segment' THEN child.id         ELSE output_table.segment_id       END AS segment_id,        -- Assign style_id if the child belongs to a segment        CASE         WHEN child.level_name = 'Style' THEN child.id         ELSE output_table.style_id       END AS style_id,        output_table.category_name,        -- Assign segment_name        CASE         WHEN child.level_name = 'Segment' THEN child.level_text         ELSE output_table.segment_name       END AS segment_name,        -- Assign style_name        CASE         WHEN child.level_name = 'Style' THEN child.level_text         ELSE output_table.style_name       END AS style_name   FROM        output_table INNER JOIN balanced_tree.product_hierarchy AS child ON output_table.id = child.parent_id )  SELECT     *  FROM     output_table; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[355]: id category_id segment_id style_id category_name segment_name style_name 0 1 1 &lt;NA&gt; &lt;NA&gt; Womens &lt;NA&gt; &lt;NA&gt; 1 2 2 &lt;NA&gt; &lt;NA&gt; Mens &lt;NA&gt; &lt;NA&gt; 2 4 1 4 &lt;NA&gt; Womens Jacket &lt;NA&gt; 3 3 1 3 &lt;NA&gt; Womens Jeans &lt;NA&gt; 4 6 2 6 &lt;NA&gt; Mens Socks &lt;NA&gt; 5 5 2 5 &lt;NA&gt; Mens Shirt &lt;NA&gt; 6 12 1 4 12 Womens Jacket Grey Fashion 7 11 1 4 11 Womens Jacket Indigo Rain 8 10 1 4 10 Womens Jacket Khaki Suit 9 9 1 3 9 Womens Jeans Cream Relaxed 10 8 1 3 8 Womens Jeans Black Straight 11 7 1 3 7 Womens Jeans Navy Oversized 12 18 2 6 18 Mens Socks Pink Fluro Polkadot 13 17 2 6 17 Mens Socks White Striped 14 16 2 6 16 Mens Socks Navy Solid 15 15 2 5 15 Mens Shirt Blue Polo 16 14 2 5 14 Mens Shirt Teal Button Up 17 13 2 5 13 Mens Shirt White Tee In\u00a0[354]: Copied! <pre>query = \"\"\" \nWITH RECURSIVE output_table (\n    id, \n    category_id, \n    segment_id, \n    style_id, \n    category_name, \n    segment_name, \n    style_name\n) AS (\n\n  -- 1) BASE CASE\n\n  SELECT\n      ph.id,\n      ph.id AS category_id,\n      CAST(NULL AS INTEGER) AS segment_id,\n      CAST(NULL AS INTEGER) AS style_id,\n      ph.level_text AS category_name,\n      CAST(NULL AS VARCHAR) AS segment_name,  \n      CAST(NULL AS VARCHAR) AS style_name    \n  FROM \n      balanced_tree.product_hierarchy AS ph\n  WHERE \n      parent_id IS NULL\n\n  UNION ALL\n\n  -- 2) RECURSIVE STEP\n\n  SELECT\n      child.id,\n      output_table.category_id,\n      \n      -- Assign segment_id if the child belongs to a category\n\n      CASE\n        WHEN child.level_name = 'Segment' THEN child.id\n        ELSE output_table.segment_id\n      END AS segment_id,\n\n      -- Assign style_id if the child belongs to a segment\n\n      CASE\n        WHEN child.level_name = 'Style' THEN child.id\n        ELSE output_table.style_id\n      END AS style_id,\n\n      output_table.category_name,\n\n      -- Assign segment_name\n\n      CASE\n        WHEN child.level_name = 'Segment' THEN child.level_text\n        ELSE output_table.segment_name\n      END AS segment_name,\n\n      -- Assign style_name\n\n      CASE\n        WHEN child.level_name = 'Style' THEN child.level_text\n        ELSE output_table.style_name\n      END AS style_name\n  FROM \n      output_table INNER JOIN balanced_tree.product_hierarchy AS child ON output_table.id = child.parent_id\n)\n\n-- 3) Final Select: Join with prices and generate product_name\n\nSELECT\n    pp.product_id,\n    pp.price,\n    CONCAT_WS(' - ', style_name, segment_name, category_name) AS product_name,\n    category_id,\n    segment_id,\n    style_id,\n    category_name,\n    segment_name,\n    style_name\nFROM \n    output_table INNER JOIN balanced_tree.product_prices AS pp ON output_table.id = pp.id\nORDER BY\n    price DESC;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  WITH RECURSIVE output_table (     id,      category_id,      segment_id,      style_id,      category_name,      segment_name,      style_name ) AS (    -- 1) BASE CASE    SELECT       ph.id,       ph.id AS category_id,       CAST(NULL AS INTEGER) AS segment_id,       CAST(NULL AS INTEGER) AS style_id,       ph.level_text AS category_name,       CAST(NULL AS VARCHAR) AS segment_name,         CAST(NULL AS VARCHAR) AS style_name       FROM        balanced_tree.product_hierarchy AS ph   WHERE        parent_id IS NULL    UNION ALL    -- 2) RECURSIVE STEP    SELECT       child.id,       output_table.category_id,              -- Assign segment_id if the child belongs to a category        CASE         WHEN child.level_name = 'Segment' THEN child.id         ELSE output_table.segment_id       END AS segment_id,        -- Assign style_id if the child belongs to a segment        CASE         WHEN child.level_name = 'Style' THEN child.id         ELSE output_table.style_id       END AS style_id,        output_table.category_name,        -- Assign segment_name        CASE         WHEN child.level_name = 'Segment' THEN child.level_text         ELSE output_table.segment_name       END AS segment_name,        -- Assign style_name        CASE         WHEN child.level_name = 'Style' THEN child.level_text         ELSE output_table.style_name       END AS style_name   FROM        output_table INNER JOIN balanced_tree.product_hierarchy AS child ON output_table.id = child.parent_id )  -- 3) Final Select: Join with prices and generate product_name  SELECT     pp.product_id,     pp.price,     CONCAT_WS(' - ', style_name, segment_name, category_name) AS product_name,     category_id,     segment_id,     style_id,     category_name,     segment_name,     style_name FROM      output_table INNER JOIN balanced_tree.product_prices AS pp ON output_table.id = pp.id ORDER BY     price DESC; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[354]: product_id price product_name category_id segment_id style_id category_name segment_name style_name 0 2a2353 57 Blue Polo - Shirt - Mens 2 5 15 Mens Shirt Blue Polo 1 9ec847 54 Grey Fashion - Jacket - Womens 1 4 12 Womens Jacket Grey Fashion 2 5d267b 40 White Tee - Shirt - Mens 2 5 13 Mens Shirt White Tee 3 f084eb 36 Navy Solid - Socks - Mens 2 6 16 Mens Socks Navy Solid 4 e83aa3 32 Black Straight - Jeans - Womens 1 3 8 Womens Jeans Black Straight 5 2feb6b 29 Pink Fluro Polkadot - Socks - Mens 2 6 18 Mens Socks Pink Fluro Polkadot 6 d5e9a6 23 Khaki Suit - Jacket - Womens 1 4 10 Womens Jacket Khaki Suit 7 72f5d4 19 Indigo Rain - Jacket - Womens 1 4 11 Womens Jacket Indigo Rain 8 b9a74d 17 White Striped - Socks - Mens 2 6 17 Mens Socks White Striped 9 c4a632 13 Navy Oversized - Jeans - Womens 1 3 7 Womens Jeans Navy Oversized 10 c8d436 10 Teal Button Up - Shirt - Mens 2 5 14 Mens Shirt Teal Button Up 11 e31d39 10 Cream Relaxed - Jeans - Womens 1 3 9 Womens Jeans Cream Relaxed"},{"location":"balanced_tree/#global","title":"Global\u00b6","text":""},{"location":"balanced_tree/#problem-statement","title":"Problem Statement\u00b6","text":"<p>Balanced Tree Clothing Company specializes in offering an optimized range of clothing and lifestyle wear tailored for the modern adventurer.</p> <p>The company's CEO, Danny, has requested an analysis of sales performance to assist the merchandising team in generating a financial report that can be shared with the broader business. This analysis will focus on key metrics and insights to support decision-making and strategic planning.</p>"},{"location":"balanced_tree/#entity-relationship-diagram","title":"Entity Relationship Diagram\u00b6","text":""},{"location":"balanced_tree/#product-details","title":"Product Details\u00b6","text":"<ul> <li><p>product_id: Unique identifier for the product.</p> </li> <li><p>price: Price of the product in the store.</p> </li> <li><p>product_name: Name of the product.</p> </li> <li><p>category_id: Unique identifier for the category.</p> </li> <li><p>segment_id: Unique identifier for the segment.</p> </li> <li><p>style_id: Unique identifier for the style.</p> </li> <li><p>category_name: Name of the category.</p> </li> <li><p>segment_name: Name of the segment.</p> </li> <li><p>style_name: Name of the style.</p> </li> </ul>"},{"location":"balanced_tree/#product-hierarchy","title":"Product Hierarchy\u00b6","text":"<ul> <li><p>id: Unique identifier for the entry in the hierarchy.</p> </li> <li><p>parent_id: Parent identifier for the current level (NULL for top-level categories).</p> </li> <li><p>level_text: Name or description of the hierarchy level (e.g., product type or style).</p> </li> <li><p>level_name: Name of the hierarchy level (e.g., Category, Segment, or Style).</p> </li> </ul>"},{"location":"balanced_tree/#product-prices","title":"Product Prices\u00b6","text":"<ul> <li><p>id: Unique identifier for the price record.</p> </li> <li><p>product_id: Unique identifier for the product.</p> </li> <li><p>price: Price of the product in the store.</p> </li> </ul>"},{"location":"balanced_tree/#sales","title":"Sales\u00b6","text":"<ul> <li><p>prod_id: Unique identifier for the product.</p> </li> <li><p>qty: Quantity of the product purchased in the transaction.</p> </li> <li><p>price: Price of the product in the transaction.</p> </li> <li><p>discount: Discount percentage applied to the product.</p> </li> <li><p>member: Membership status of the buyer (<code>t</code> for true, <code>f</code> for false).</p> </li> <li><p>txn_id: Unique identifier for the transaction.</p> </li> <li><p>start_txn_time: Timestamp of when the transaction started.</p> </li> </ul>"},{"location":"balanced_tree/#tables","title":"Tables\u00b6","text":""},{"location":"balanced_tree/#sales-analysis","title":"Sales Analysis\u00b6","text":""},{"location":"balanced_tree/#q1","title":"Q1\u00b6","text":"<p>What was the total quantity sold for all products?</p>"},{"location":"balanced_tree/#q2","title":"Q2\u00b6","text":"<p>What is the total generated revenue for all products before discounts?</p>"},{"location":"balanced_tree/#q3","title":"Q3\u00b6","text":"<p>What was the total discount amount for all products?</p>"},{"location":"balanced_tree/#transactions-analysis","title":"Transactions Analysis\u00b6","text":""},{"location":"balanced_tree/#q1","title":"Q1\u00b6","text":"<p>How many unique transactions were there?</p>"},{"location":"balanced_tree/#q2","title":"Q2\u00b6","text":"<p>What is the average unique products purchased in each transaction?</p>"},{"location":"balanced_tree/#q3","title":"Q3\u00b6","text":"<p>What are the 25th, 50th and 75th percentile values for the revenue per transaction?</p> <p>Note: We consider the revenue after applying the discount.</p>"},{"location":"balanced_tree/#q4","title":"Q4\u00b6","text":"<p>What is the average discount value per transaction?</p>"},{"location":"balanced_tree/#q5","title":"Q5\u00b6","text":"<p>What is the percentage split of all transactions for members vs non-members?</p>"},{"location":"balanced_tree/#q6","title":"Q6\u00b6","text":"<p>What is the average revenue for member transactions and non-member transactions?</p>"},{"location":"balanced_tree/#product-analysis","title":"Product Analysis\u00b6","text":""},{"location":"balanced_tree/#q1","title":"Q1\u00b6","text":"<p>What are the top 3 products by total revenue before discount?</p> <p>Note: We consider the revenue before applying the discount. Formatting is applied to the output but the calculations should be used in the <code>ORDER BY</code> clause instead of the formatted values.</p>"},{"location":"balanced_tree/#q2","title":"Q2\u00b6","text":"<p>What is the total quantity, revenue and discount for each segment?</p>"},{"location":"balanced_tree/#q3","title":"Q3\u00b6","text":"<p>What is the top selling product for each segment?</p>"},{"location":"balanced_tree/#q4","title":"Q4\u00b6","text":"<p>What is the total quantity, revenue and discount for each category?</p>"},{"location":"balanced_tree/#q5","title":"Q5\u00b6","text":"<p>What is the top selling product for each category?</p>"},{"location":"balanced_tree/#q6","title":"Q6\u00b6","text":"<p>What is the percentage split of revenue by product for each segment?</p> <p>Note: We consider the revenue after applying the discount.</p>"},{"location":"balanced_tree/#q7","title":"Q7\u00b6","text":"<p>What is the percentage split of revenue by segment for each category?</p> <p>Note: We consider the revenue after applying the discount.</p>"},{"location":"balanced_tree/#q8","title":"Q8\u00b6","text":"<p>What is the percentage split of total revenue by category?</p> <p>Note: We consider the revenue after applying the discount.</p>"},{"location":"balanced_tree/#q9","title":"Q9\u00b6","text":"<p>What is the total transaction \"penetration\" for each product? (hint: penetration = number of transactions where at least 1 quantity of a product was purchased divided by total number of transactions)</p> <p>$$ \\text{Penetration of Product $i$} = \\frac{\\text{Number of transactions in which at least 1 quantity of product $i$ was purchased}}{\\text{Total number of transactions}}</p>"},{"location":"balanced_tree/#double-pass-cte-approach","title":"Double Pass CTE Approach\u00b6","text":""},{"location":"balanced_tree/#q10","title":"Q10\u00b6","text":"<p>What is the most common combination of at least 1 quantity of any 3 products in a 1 single transaction?</p>"},{"location":"balanced_tree/#step-1-recursive-cte","title":"Step 1: Recursive CTE\u00b6","text":"<p>Use a recursive CTE to build all unique 3\u2010product combinations from the <code>balanced_tree.product_details</code> table.</p>"},{"location":"balanced_tree/#base-case","title":"Base Case\u00b6","text":"<p>The base case selects all unique products from the <code>product_details</code> table.</p>"},{"location":"balanced_tree/#recursive-step","title":"Recursive Step\u00b6","text":"<p>The recursion will lead to a table with $\\binom{n}{k}$ rows, where $n$ is the number of unique product IDs and $k$ is the number of products in each combination.</p> <p>For each of those existing combos (say it has 1 or 2 products right now), we try to append a new product to the combo if:</p> <ol> <li>The new product ID is greater than the last product ID in the combo (<code>d.product_id &gt; output_table.product</code>).</li> <li>The current <code>product_counter</code> is still less than 3, so we don\u2019t exceed 3\u2010product combos.</li> </ol> <p>This ensures we only create unique combos (no duplicates and no reordering like <code>[P1, P2]</code> vs <code>[P2, P1]</code>, we always move \"upward\" in product IDs).</p> <p>We stop when we reach 3 products in a combo:</p> <ol> <li>Because we only keep recursing while <code>product_counter &lt; 3</code>, we stop at combos of exactly 3 products.</li> <li>In the final <code>SELECT</code>, we filter <code>WHERE product_counter = 3</code> to get only those complete 3\u2010product combinations.</li> </ol>"},{"location":"balanced_tree/#example","title":"Example\u00b6","text":"<p>Given products <code>A, B, C, D</code> sorted by ID, the base case yields single\u2010product combos:</p> combo product product_counter [A] A 1 [B] B 1 [C] C 1 [D] D 1"},{"location":"balanced_tree/#first-recursive-pass-from-1product-to-2product-combos","title":"First Recursive Pass (from 1\u2010product to 2\u2010product combos)\u00b6","text":"<ul> <li><p>Take <code>[A], A, 1</code> and try to append each product ID greater than <code>A</code>:</p> <ul> <li>Append <code>B</code> \u2192 <code>[A, B], B, 2</code></li> <li>Append <code>C</code> \u2192 <code>[A, C], C, 2</code></li> <li>Append <code>D</code> \u2192 <code>[A, D], D, 2</code></li> </ul> </li> <li><p>Take <code>[B], B, 1</code> and append each product ID greater than <code>B</code>:</p> <ul> <li><code>[B, C], C, 2</code></li> <li><code>[B, D], D, 2</code></li> </ul> </li> <li><p>Take <code>[C], C, 1</code> and append each product ID greater than <code>C</code>:</p> <ul> <li><code>[C, D], D, 2</code></li> </ul> </li> <li><p>Take <code>[D], D, 1</code> \u2192 no greater IDs remain, so no new rows from <code>[D]</code>.</p> </li> </ul> <p>Now <code>output_table</code> has all 2\u2010product combos. The recursion continues because <code>product_counter &lt; 3</code>:</p> combo product product_counter [A, B] B 2 [A, C] C 2 [A, D] D 2 [B, C] C 2 [B, D] D 2 [C, D] D 2"},{"location":"balanced_tree/#second-recursive-pass-from-2product-to-3product-combos","title":"Second Recursive Pass (from 2\u2010product to 3\u2010product combos)\u00b6","text":"<ul> <li>Start with <code>[A, B], B, 2</code>; append any product ID greater than <code>B</code>:<ul> <li><code>[A, B, C], C, 3</code></li> <li><code>[A, B, D], D, 3</code></li> </ul> </li> <li><code>[A, C], C, 2</code>:<ul> <li><code>[A, C, D], D, 3</code></li> </ul> </li> <li><code>[A, D], D, 2</code>:<ul> <li>(No product greater than <code>D</code> \u2192 no new combo)</li> </ul> </li> <li><code>[B, C], C, 2</code>:<ul> <li><code>[B, C, D], D, 3</code></li> </ul> </li> <li><code>[B, D], D, 2</code>:<ul> <li>(No product greater than <code>D</code>)</li> </ul> </li> <li><code>[C, D], D, 2</code>:<ul> <li>(No product greater than <code>D</code>)</li> </ul> </li> </ul> <p>Now we have 3\u2010product combos:</p> combo product product_counter [A, B, C] C 3 [A, B, D] D 3 [A, C, D] D 3 [B, C, D] D 3 <p>Because <code>product_counter</code> is now <code>3</code>, we stop the recursion.</p>"},{"location":"balanced_tree/#why-does-the-dproduct_id-output_tableproduct-condition-remove-duplicates","title":"Why Does the <code>d.product_id &gt; output_table.product</code> Condition Remove Duplicates?\u00b6","text":"<ul> <li>That condition ensures that each next product has a strictly higher ID (i.e., by lexicographical ordering) than the last appended product, so we never pick the same product twice and never produce the same combo in a different order.</li> <li>For example, if we already have <code>[A]</code>, we only add <code>B, C, D</code> if <code>B &gt; A</code>, <code>C &gt; A</code>, and so on. We never go back and pair <code>[B, A]</code>, so duplicated combos are avoided.</li> </ul>"},{"location":"balanced_tree/#step-2-gather-each-transactions-products","title":"Step 2: Gather Each Transaction\u2019s Products\u00b6","text":"<p>Create a table that groups <code>products_id</code> by <code>txn_id</code> into an array. In other words, for each transaction, we will have an array of the products purchased in that transaction.</p>"},{"location":"balanced_tree/#step-3-identify-transactions-that-contain-each-3product-combo-cte_combo_transactions","title":"Step 3: Identify Transactions That Contain Each 3\u2010Product Combo (<code>cte_combo_transactions</code>)\u00b6","text":""},{"location":"balanced_tree/#cartesian-product-to-pair-each-transaction-with-each-3product-combo","title":"Cartesian Product to Pair Each Transaction with Each 3\u2010Product Combo\u00b6","text":"<p>We <code>CROSS JOIN</code> the 3\u2010product combos with the transactions CTE above, then filter for transactions that contain all 3 products. This essentially pairs each transaction with each 3\u2010product combo, allowing us to check if the transaction contains all 3 products in the combo.</p>"},{"location":"balanced_tree/#set-intersection-to-check-containment","title":"Set Intersection to Check Containment\u00b6","text":"<p>Since Athena/Trino do not support the <code>&lt;@</code> array operator (Postgres\u2010only), we check containment using the <code>CARDINALITY(ARRAY_INTERSECT(...))</code> trick.</p> <ul> <li><p>The function <code>CARDINALITY(X)</code> returns the cardinality (size) of the array <code>X</code>.</p> </li> <li><p>The function <code>ARRAY_INTERSECT(A, B)</code> returns an array of the elements in the intersection of arrays <code>A</code> and <code>B</code>, without duplicates.</p> </li> </ul> <p>If the cardinality of the intersection of the transaction\u2019s products and the 3\u2010product combo is 3, then the transaction contains all 3 products in the combo.</p>"},{"location":"balanced_tree/#step-4-rank-each-3product-combo-by-frequency-cte_ranked_combos","title":"Step 4: Rank Each 3\u2010Product Combo by Frequency (<code>cte_ranked_combos</code>)\u00b6","text":"<p>Count how many transactions each combo appears in and rank them from most frequent to least frequent.</p> <ul> <li><p>Group by the 3\u2010product combo</p> <ul> <li><p>Count the number of transactions each combo appears in</p> </li> <li><p>Rank the combos by frequency by ordering the count above in descending order</p> </li> </ul> </li> </ul>"},{"location":"balanced_tree/#step-5-unnest-the-most-common-combo-into-row-format-cte_most_common_combo_product_transactions","title":"Step 5: UNNEST the Most Common Combo into Row Format (<code>cte_most_common_combo_product_transactions</code>)\u00b6","text":"<p>We only want rows for the top\u2010ranked (i.e., <code>rank = 1</code>) combo(s). We UNNEST that 3\u2010product array into 3 separate rows for every transaction that contains it.</p> <ul> <li><code>cte_combo_transactions combos</code> has one row per transaction per 3\u2010product combo that appears in that transaction.</li> <li><code>ranks.combo</code> is the top\u2010ranked 3\u2010product array.</li> <li><code>UNNEST(ranks.combo) AS t(prod_id)</code> splits that 3\u2010product array into 3 separate rows, one for each product ID in the combo.</li> </ul> <p>If there are N transactions that have the top 3\u2010product combo, we\u2019ll end up with 3 \u00d7 N rows, since each transaction row is \u201cexploded\u201d into 3 product rows.</p> <pre>  352 transactions\n\u00d7   3 products in the combo\n---------------------------\n= 1056 total rows\n</pre> <p>In other words, each transaction that has the top 3\u2010product combo will produce exactly 3 rows in this step, one row per product ID in the combo.</p>"},{"location":"balanced_tree/#step-6-final-calculation-quantity-revenue-discount-net-revenue","title":"Step 6: Final Calculation (Quantity, Revenue, Discount, Net Revenue)\u00b6","text":"<p>Join the transactions from Step 5 to the <code>balanced_tree.sales</code> table and <code>balanced_tree.product_details</code> to calculate final metrics:</p>"},{"location":"balanced_tree/#bonus-challenge","title":"Bonus Challenge\u00b6","text":"<p>Use a single SQL query to transform the <code>product_hierarchy</code> and <code>product_prices</code> datasets to the <code>product_details</code> table.</p>"},{"location":"balanced_tree/#step-1-recursive-cte-to-build-the-hierarchy","title":"Step 1: Recursive CTE to Build the Hierarchy\u00b6","text":""},{"location":"balanced_tree/#base-case","title":"Base Case\u00b6","text":"<p>The base case initializes the recursion by selecting the top-level categories in the hierarchy (where <code>parent_id IS NULL</code>):</p> <ul> <li><p>We select only rows whose <code>parent_id IS NULL</code>.</p> </li> <li><p>These correspond to top\u2010level nodes--- Categories.</p> </li> <li><p>For each such row:</p> <ul> <li><p><code>id</code> and <code>category_id</code> are set to the same <code>ph.id</code> (since it\u2019s a Category node).</p> </li> <li><p><code>segment_id</code> and <code>style_id</code> are NULL (not yet assigned).</p> </li> <li><p><code>category_name</code> is <code>ph.level_text</code> (the name of this Category), while <code>segment_name</code> and <code>style_name</code> are NULL.</p> </li> </ul> </li> </ul> <p>This yields a starting table (<code>output_table</code>) with one row per Category.</p>"},{"location":"balanced_tree/#recursive-step","title":"Recursive Step\u00b6","text":"<p>After the base case, the query unions the above with its recursive step:</p> <ol> <li><p>Match each existing row in <code>output_table</code> to its child nodes by joining on <code>output_table.id = child.parent_id</code>:</p> <ul> <li><p>Think of <code>output_table.id</code> as the \u201ccurrent node\u201d in the hierarchy.</p> </li> <li><p>The <code>child</code> rows are the ones in <code>product_hierarchy</code> table whose <code>parent_id</code> matches that \u201ccurrent node.\u201d</p> </li> </ul> </li> <li><p>For each child we find, create a new row in the recursive output:</p> <ul> <li><p><code>child.id</code> becomes the new <code>id</code>.</p> </li> <li><p>We inherit the existing <code>category_id</code> from the parent row in <code>output_table</code>, because all descendants of that top\u2010level node belong to the same Category.</p> </li> <li><p>If the <code>child.level_name</code> is <code>'Segment'</code>, we assign <code>segment_id = child.id</code>. Otherwise, we keep the same <code>segment_id</code> from the parent row (since we haven\u2019t reached a segment yet).</p> </li> <li><p>If the <code>child.level_name</code> is <code>'Style'</code>, we assign <code>style_id = child.id</code>. Otherwise, we keep the parent row\u2019s <code>style_id</code>.</p> </li> <li><p>Similarly for names:</p> <ul> <li><p>If <code>child.level_name = 'Segment'</code>, we overwrite <code>segment_name</code> with <code>child.level_text</code>.</p> </li> <li><p>If <code>child.level_name = 'Style'</code>, we overwrite <code>style_name</code>.</p> </li> <li><p>Everything else stays inherited from the parent row.</p> </li> </ul> </li> </ul> </li> <li><p>Repeat this step recursively, starting from the new rows we just generated. Each child row can itself be a parent to deeper nodes (grandchildren, great\u2010grandchildren, etc.) as long as there is a <code>parent_id</code> link.</p> </li> </ol>"},{"location":"balanced_tree/#example","title":"Example\u00b6","text":"<p>Imagine a simplified <code>product_hierarchy</code>:</p> id parent_id level_name level_text 1 NULL Category Men 2 NULL Category Women 3 1 Segment Shirts 4 3 Style Casual 5 3 Style Formal 6 1 Segment Pants 7 6 Style Denim"},{"location":"balanced_tree/#base-case","title":"Base Case\u00b6","text":"<p>We grab rows where <code>parent_id IS NULL</code>, i.e. <code>id = 1</code> (\u201cMen\u201d) and <code>id = 2</code> (\u201cWomen\u201d). So initially, <code>output_table</code> has:</p> id category_id segment_id style_id category_name segment_name style_name 1 1 NULL NULL Men NULL NULL 2 2 NULL NULL Women NULL NULL"},{"location":"balanced_tree/#1st-recursive-pass","title":"1st Recursive Pass\u00b6","text":"<ul> <li><p>Take <code>id = 1</code> (Men) from <code>output_table</code>; find all children: rows in <code>product_hierarchy</code> with <code>parent_id = 1</code>.</p> <ul> <li><p>We see <code>id = 3 (Shirts)</code> and <code>id = 6 (Pants)</code>. Both are <code>Segment</code> level.</p> </li> <li><p>That yields:</p> </li> </ul> </li> </ul> child.id child.level_name child.level_text 3 Segment Shirts 6 Segment Pants <p>For each, we create a row:</p> <ul> <li><p>For child=3 (\"Shirts\"):</p> <ul> <li><code>id = 3</code></li> <li><code>category_id</code> = 1 (inherited from parent)</li> <li><code>segment_id = 3</code> (since <code>child.level_name = 'Segment'</code>)</li> <li><code>style_id = NULL</code> (unchanged)</li> <li><code>category_name = 'Men'</code> (inherited)</li> <li><code>segment_name = 'Shirts'</code> (assigned because <code>'Segment'</code>)</li> <li><code>style_name = NULL</code></li> </ul> </li> <li><p>For child=6 (\"Pants\"):</p> <ul> <li><code>id = 6</code></li> <li><code>category_id</code> = 1</li> <li><code>segment_id = 6</code></li> <li><code>style_id = NULL</code></li> <li><code>category_name = 'Men'</code></li> <li><code>segment_name = 'Pants'</code></li> <li><code>style_name = NULL</code></li> </ul> </li> <li><p>Take <code>id = 2</code> (Women); find all children: rows with <code>parent_id = 2</code>.</p> <ul> <li>Suppose we have none. Then no new rows come from <code>id = 2</code>.</li> </ul> </li> </ul> <p>Now <code>output_table</code> also includes these new rows:</p> id category_id segment_id style_id category_name segment_name style_name 3 1 3 NULL Men Shirts NULL 6 1 6 NULL Men Pants NULL"},{"location":"balanced_tree/#2nd-recursive-pass","title":"2nd Recursive Pass\u00b6","text":"<ul> <li>Consider the new row <code>id = 3 (Segment = Shirts)</code>. Look for children where <code>parent_id = 3</code>.<ul> <li>Found <code>id = 4 (Style = Casual)</code>, <code>id = 5 (Style = Formal)</code>.</li> <li>Each yields new rows:</li> </ul> </li> </ul> id category_id segment_id style_id category_name segment_name style_name 4 1 3 4 Men Shirts Casual 5 1 3 5 Men Shirts Formal <ul> <li>Consider the new row <code>id = 6</code> (Segment=Pants). Look for children where <code>parent_id = 6</code>.<ul> <li>Found <code>id = 7 (Style = Denim)</code>:</li> </ul> </li> </ul> id category_id segment_id style_id category_name segment_name style_name 7 1 6 7 Men Pants Denim <ul> <li>If <code>id = 2 (Women)</code> had children, we\u2019d generate those as well, continuing the same pattern.</li> </ul> <p>Eventually, once we have no more children for any newly added row, the recursion stops.</p> <p>End result:</p> <p>For every node in the hierarchy, we get one row containing:</p> <ul> <li>The node\u2019s own <code>id</code> (whether it\u2019s a Category, Segment, or Style).</li> <li>The \u201cinherited\u201d <code>category_id</code>, <code>segment_id</code>, <code>style_id</code>.</li> <li>The \u201cinherited\u201d names for category, segment, style up to this point in the tree.</li> </ul>"},{"location":"balanced_tree/#step-2-join-with-product_prices-to-get-the-final-product_details-table","title":"Step 2: Join with <code>product_prices</code> to Get the Final <code>product_details</code> Table\u00b6","text":"<ul> <li><p>For each row in the hierarchy (each node), match it with its <code>price</code> from <code>product_prices</code>.</p> </li> <li><p>Build a <code>product_name</code> by concatenating <code>style_name - segment_name - category_name</code> (in that order), or skipping NULL parts if <code>style_name</code> or <code>segment_name</code> were never set.</p> </li> </ul>"},{"location":"bitcoin_eda/","title":"EDA","text":"In\u00a0[1]: Copied! <pre>from IPython.core.interactiveshell import InteractiveShell\n\nInteractiveShell.ast_node_interactivity = \"all\"\n\nimport os\nimport sys\n\nsys.path.append(\"../../../\")\nfrom src.athena import Athena\nfrom src.utils import create_session\n</pre> from IPython.core.interactiveshell import InteractiveShell  InteractiveShell.ast_node_interactivity = \"all\"  import os import sys  sys.path.append(\"../../../\") from src.athena import Athena from src.utils import create_session In\u00a0[6]: Copied! <pre>boto3_session = create_session(\n    profile_name=\"dev\",\n    role_arn=os.getenv(\"ATHENA_IAM_ROLE_ARN\"),\n)\n\nwait = True\nctas_approach = False\n\ndatabase = \"trading\"\ntable = \"daily_btc\"\n\nathena = Athena(boto3_session=boto3_session, s3_output=os.getenv(\"ATHENA_S3_OUTPUT\"))\nathena\n</pre> boto3_session = create_session(     profile_name=\"dev\",     role_arn=os.getenv(\"ATHENA_IAM_ROLE_ARN\"), )  wait = True ctas_approach = False  database = \"trading\" table = \"daily_btc\"  athena = Athena(boto3_session=boto3_session, s3_output=os.getenv(\"ATHENA_S3_OUTPUT\")) athena Out[6]: <pre>Athena(boto3_session=Session(region_name='us-east-1'), s3_output=s3://sql-case-studies/query_results)</pre> In\u00a0[7]: Copied! <pre>athena.query(\n    database=database,\n    query=f\"\"\" \n            SELECT\n                *\n            FROM\n                {database}.{table} TABLESAMPLE BERNOULLI(50);\n            \"\"\",\n    ctas_approach=ctas_approach,\n)\n</pre> athena.query(     database=database,     query=f\"\"\"              SELECT                 *             FROM                 {database}.{table} TABLESAMPLE BERNOULLI(50);             \"\"\",     ctas_approach=ctas_approach, ) Out[7]: market_date open_price high_price low_price close_price adjusted_close_price volume 0 2014-09-18 456.859985 456.859985 413.104004 424.440002 424.440002 3.448320e+07 1 2014-09-19 424.102997 427.834991 384.532013 394.795990 394.795990 3.791970e+07 2 2014-09-20 394.673004 423.295990 389.882996 408.903992 408.903992 3.686360e+07 3 2014-09-25 423.156006 423.519989 409.467987 411.574005 411.574005 2.681440e+07 4 2014-09-28 399.471008 401.016998 374.332001 377.181000 377.181000 2.361330e+07 ... ... ... ... ... ... ... ... 1180 2021-02-20 55887.335938 57505.226563 54626.558594 56099.519531 56099.519531 6.814546e+10 1181 2021-02-21 56068.566406 58330.570313 55672.609375 57539.945313 57539.945313 5.189759e+10 1182 2021-02-22 57532.738281 57533.390625 48967.566406 54207.320313 54207.320313 9.205242e+10 1183 2021-02-23 54204.929688 54204.929688 45290.589844 48824.425781 48824.425781 1.061025e+11 1184 2021-02-24 50940.621094 51225.078125 47254.687500 50460.234375 50460.234375 8.836479e+10 <p>1185 rows \u00d7 7 columns</p> In\u00a0[8]: Copied! <pre>q1_query = \"\"\" \nSELECT \n    MAX(market_date) AS max_date,\n    MIN(market_date) AS min_date,\n    DATE_DIFF('day', MIN(market_date), MAX(market_date)) AS days_difference,\n    DATE_DIFF('month', MIN(market_date), MAX(market_date)) AS months_difference,\n    DATE_DIFF('year', MIN(market_date), MAX(market_date)) AS years_difference\nFROM\n    trading.daily_btc;\n\"\"\"\n\nathena.query(database=database, query=q1_query, ctas_approach=ctas_approach)\n</pre> q1_query = \"\"\"  SELECT      MAX(market_date) AS max_date,     MIN(market_date) AS min_date,     DATE_DIFF('day', MIN(market_date), MAX(market_date)) AS days_difference,     DATE_DIFF('month', MIN(market_date), MAX(market_date)) AS months_difference,     DATE_DIFF('year', MIN(market_date), MAX(market_date)) AS years_difference FROM     trading.daily_btc; \"\"\"  athena.query(database=database, query=q1_query, ctas_approach=ctas_approach) Out[8]: max_date min_date days_difference months_difference years_difference 0 2021-02-24 2014-09-17 2352 77 6 <p>In postgresql, we can use the <code>AGE(timestamp, timestamp)</code> function to calculate the difference between two dates:</p> <pre>SELECT \n    MAX(market_date) AS max_date,\n    MIN(market_date) AS min_date,\n    AGE(MAX(market_date), MIN(market_date)) AS date_range\nFROM\n    trading.daily_btc;\n</pre> max_date min_date date_range 2021-02-24 2014-09-17 { \"years\": 6, \"months\": 5, \"days\": 7 } In\u00a0[9]: Copied! <pre>q2_union_all_query = \"\"\"\n(\n  SELECT\n    market_date,\n    close_price \n  FROM\n    trading.daily_btc\n  ORDER BY\n    close_price DESC NULLS LAST\n  LIMIT\n    1\n)\nUNION ALL\n(\n  SELECT\n    market_date,\n    close_price \n  FROM\n    trading.daily_btc\n  ORDER BY\n    close_price ASC NULLS LAST\n  LIMIT\n    1\n);\n\"\"\"\n\nathena.query(database=database, query=q2_union_all_query, ctas_approach=ctas_approach)\n</pre> q2_union_all_query = \"\"\" (   SELECT     market_date,     close_price    FROM     trading.daily_btc   ORDER BY     close_price DESC NULLS LAST   LIMIT     1 ) UNION ALL (   SELECT     market_date,     close_price    FROM     trading.daily_btc   ORDER BY     close_price ASC NULLS LAST   LIMIT     1 ); \"\"\"  athena.query(database=database, query=q2_union_all_query, ctas_approach=ctas_approach) Out[9]: market_date close_price 0 2021-02-21 57539.945313 1 2015-01-14 178.102997 <p>The <code>query1 UNION [ALL] query2</code> statement is used to combine the result sets of 2 or more SELECT statements. It removes duplicate rows between the various SELECT statements unless <code>ALL</code> is specified.</p> In\u00a0[10]: Copied! <pre>q2_window_query = \"\"\"\nWITH btc_data AS (\n  SELECT\n    market_date,\n    close_price,\n    RANK() OVER (ORDER BY close_price ASC NULLS LAST) AS min_close_price_rank,\n    RANK() OVER (ORDER BY close_price DESC NULLS LAST) AS max_close_price_rank\n  FROM\n    trading.daily_btc\n)\nSELECT\n  market_date,\n  close_price\nFROM\n  btc_data\nWHERE\n  1 IN (min_close_price_rank, max_close_price_rank)\nORDER BY\n  close_price DESC;\n\"\"\"\n\nathena.query(database=database, query=q2_window_query, ctas_approach=ctas_approach)\n</pre> q2_window_query = \"\"\" WITH btc_data AS (   SELECT     market_date,     close_price,     RANK() OVER (ORDER BY close_price ASC NULLS LAST) AS min_close_price_rank,     RANK() OVER (ORDER BY close_price DESC NULLS LAST) AS max_close_price_rank   FROM     trading.daily_btc ) SELECT   market_date,   close_price FROM   btc_data WHERE   1 IN (min_close_price_rank, max_close_price_rank) ORDER BY   close_price DESC; \"\"\"  athena.query(database=database, query=q2_window_query, ctas_approach=ctas_approach) Out[10]: market_date close_price 0 2021-02-21 57539.945313 1 2015-01-14 178.102997 <p>The CTE <code>btc_data</code> is used to rank the <code>close_price</code> in ascending and descending order.</p> In\u00a0[11]: Copied! <pre>q2_cte_query = \"\"\"\nSELECT\n  market_date,\n  close_price,\n  RANK() OVER (ORDER BY close_price ASC NULLS LAST) AS min_close_price_rank,\n  RANK() OVER (ORDER BY close_price DESC NULLS LAST) AS max_close_price_rank\nFROM\n  trading.daily_btc;\n\"\"\"\n\nathena.query(database=database, query=q2_cte_query, ctas_approach=ctas_approach)\n</pre> q2_cte_query = \"\"\" SELECT   market_date,   close_price,   RANK() OVER (ORDER BY close_price ASC NULLS LAST) AS min_close_price_rank,   RANK() OVER (ORDER BY close_price DESC NULLS LAST) AS max_close_price_rank FROM   trading.daily_btc; \"\"\"  athena.query(database=database, query=q2_cte_query, ctas_approach=ctas_approach) Out[11]: market_date close_price min_close_price_rank max_close_price_rank 0 2021-02-21 57539.945313 2349 1 1 2021-02-20 56099.519531 2348 2 2 2021-02-19 55888.132813 2347 3 3 2021-02-22 54207.320313 2346 4 4 2021-02-17 52149.007813 2345 5 ... ... ... ... ... 2348 2015-01-14 178.102997 1 2349 2349 2020-10-13 NaN 2350 2350 2350 2020-10-09 NaN 2350 2350 2351 2020-04-17 NaN 2350 2350 2352 2020-10-12 NaN 2350 2350 <p>2353 rows \u00d7 4 columns</p> In\u00a0[12]: Copied! <pre>q3_query = \"\"\"\nWITH btc_data AS (\n  SELECT\n    market_date,\n    close_price,\n    volume,\n    RANK() OVER (ORDER BY volume DESC NULLS LAST) AS volume_rank\n  FROM\n    trading.daily_btc\n)\nSELECT\n  market_date,\n  close_price,\n  volume\nFROM\n  btc_data\nWHERE \n  volume_rank = 1\nORDER BY\n  volume DESC;\n\"\"\"\n\nathena.query(database=database, query=q3_query, ctas_approach=ctas_approach)\n</pre> q3_query = \"\"\" WITH btc_data AS (   SELECT     market_date,     close_price,     volume,     RANK() OVER (ORDER BY volume DESC NULLS LAST) AS volume_rank   FROM     trading.daily_btc ) SELECT   market_date,   close_price,   volume FROM   btc_data WHERE    volume_rank = 1 ORDER BY   volume DESC; \"\"\"  athena.query(database=database, query=q3_query, ctas_approach=ctas_approach) Out[12]: market_date close_price volume 0 2021-01-11 35566.65625 1.233206e+11 In\u00a0[13]: Copied! <pre>q4_query = \"\"\"\nWITH counts_data AS (\n  SELECT\n    SUM(\n      CASE\n        WHEN low_price &lt; 0.9 * open_price THEN 1 ELSE 0\n      END\n    ) AS num_days_lower,\n    COUNT(*) AS total_days\n  FROM\n    trading.daily_btc\n  WHERE\n    volume IS NOT NULL\n)\nSELECT\n  num_days_lower,\n  ROUND(100 * num_days_lower / total_days) AS pct\nFROM\n  counts_data;\n\"\"\"\n\nathena.query(database=database, query=q4_query, ctas_approach=ctas_approach)\n</pre> q4_query = \"\"\" WITH counts_data AS (   SELECT     SUM(       CASE         WHEN low_price &lt; 0.9 * open_price THEN 1 ELSE 0       END     ) AS num_days_lower,     COUNT(*) AS total_days   FROM     trading.daily_btc   WHERE     volume IS NOT NULL ) SELECT   num_days_lower,   ROUND(100 * num_days_lower / total_days) AS pct FROM   counts_data; \"\"\"  athena.query(database=database, query=q4_query, ctas_approach=ctas_approach) Out[13]: num_days_lower pct 0 79 3 In\u00a0[14]: Copied! <pre>q5_query = \"\"\"\nSELECT\n  ROUND(AVG(\n    CASE\n      WHEN close_price IS NULL OR open_price IS NULL THEN NULL\n      WHEN close_price &gt; open_price THEN 1\n      ELSE 0\n    END\n  ), 4) AS pct_closer_greater_open\nFROM\n  trading.daily_btc;\n\"\"\"\n\nathena.query(database=database, query=q5_query, ctas_approach=ctas_approach)\n</pre> q5_query = \"\"\" SELECT   ROUND(AVG(     CASE       WHEN close_price IS NULL OR open_price IS NULL THEN NULL       WHEN close_price &gt; open_price THEN 1       ELSE 0     END   ), 4) AS pct_closer_greater_open FROM   trading.daily_btc; \"\"\"  athena.query(database=database, query=q5_query, ctas_approach=ctas_approach) Out[14]: pct_closer_greater_open 0 0.5462 In\u00a0[15]: Copied! <pre>q6_query = \"\"\"\nSELECT\n  market_date,\n  high_price,\n  low_price,\n  (high_price - low_price) AS diff\nFROM\n  trading.daily_btc\nORDER BY\n  (high_price - low_price) DESC NULLS LAST\nLIMIT 1;\n\"\"\"\n\nathena.query(database=database, query=q6_query, ctas_approach=ctas_approach)\n</pre> q6_query = \"\"\" SELECT   market_date,   high_price,   low_price,   (high_price - low_price) AS diff FROM   trading.daily_btc ORDER BY   (high_price - low_price) DESC NULLS LAST LIMIT 1; \"\"\"  athena.query(database=database, query=q6_query, ctas_approach=ctas_approach) Out[15]: market_date high_price low_price diff 0 2021-02-23 54204.929688 45290.589844 8914.339844 In\u00a0[16]: Copied! <pre>q7_start_end_query = \"\"\"\nSELECT\n  MAX(CASE WHEN market_date = DATE '2016-01-01' THEN close_price END) AS start_value,\n  MAX(CASE WHEN market_date = DATE '2021-02-01' THEN close_price END) AS end_value,\n  DATE_DIFF('day', DATE '2016-01-01', DATE '2021-02-01') / 365.25 AS years\nFROM\n  trading.daily_btc;\n\"\"\"\n\nathena.query(database=database, query=q7_start_end_query, ctas_approach=ctas_approach)\n</pre> q7_start_end_query = \"\"\" SELECT   MAX(CASE WHEN market_date = DATE '2016-01-01' THEN close_price END) AS start_value,   MAX(CASE WHEN market_date = DATE '2021-02-01' THEN close_price END) AS end_value,   DATE_DIFF('day', DATE '2016-01-01', DATE '2021-02-01') / 365.25 AS years FROM   trading.daily_btc; \"\"\"  athena.query(database=database, query=q7_start_end_query, ctas_approach=ctas_approach) Out[16]: start_value end_value years 0 434.334015 33537.175781 5.086927 <ul> <li><p><code>CASE WHEN market_date = DATE '2016-01-01' THEN close_price END</code>: This statement checks if the market_date matches '2016-01-01' and returns the close_price for that date.</p> </li> <li><p><code>MAX(...)</code>: Aggregates the values returned by the <code>CASE</code> statement. Since there should be only one <code>close_price</code> per date, <code>MAX</code> is used to handle aggregation without duplication.</p> </li> <li><p><code>DATE_DIFF('day', DATE '2016-01-01', DATE '2021-02-01') / 365.25</code>: This statement calculates the number of years between the two dates. In postgresql, we could use <code>EXTRACT(EPOCH FROM AGE(DATE '2021-02-01', DATE '2016-01-01')) / (365.25 * 24 * 60 * 60)</code> to calculate the number of years.</p> </li> </ul> <p>The next step is to calculate the CAGR and multiply the initial investment by the CAGR to determine the final value:</p> In\u00a0[17]: Copied! <pre>q7_cagr_query = \"\"\"\nWITH start_end_values AS (\n  SELECT\n    MAX(CASE WHEN market_date = DATE '2016-01-01' THEN close_price END) AS start_value,\n    MAX(CASE WHEN market_date = DATE '2021-02-01' THEN close_price END) AS end_value,\n    DATE_DIFF('day', DATE '2016-01-01', DATE '2021-02-01') / 365.25 AS years\n  FROM\n    trading.daily_btc\n)\nSELECT\n  ROUND((POWER((end_value / start_value), 1.0 / years) - 1) * 100, 4) AS cagr_in_pct,\n  ROUND(10000 * POWER((1 + (POWER((end_value / start_value), 1.0 / years) - 1)), years), 4) AS final_value\nFROM\n  start_end_values;\n\"\"\"\n\nathena.query(database=database, query=q7_cagr_query, ctas_approach=ctas_approach)\n</pre> q7_cagr_query = \"\"\" WITH start_end_values AS (   SELECT     MAX(CASE WHEN market_date = DATE '2016-01-01' THEN close_price END) AS start_value,     MAX(CASE WHEN market_date = DATE '2021-02-01' THEN close_price END) AS end_value,     DATE_DIFF('day', DATE '2016-01-01', DATE '2021-02-01') / 365.25 AS years   FROM     trading.daily_btc ) SELECT   ROUND((POWER((end_value / start_value), 1.0 / years) - 1) * 100, 4) AS cagr_in_pct,   ROUND(10000 * POWER((1 + (POWER((end_value / start_value), 1.0 / years) - 1)), years), 4) AS final_value FROM   start_end_values; \"\"\"  athena.query(database=database, query=q7_cagr_query, ctas_approach=ctas_approach) Out[17]: cagr_in_pct final_value 0 135.0114 772151.7225"},{"location":"bitcoin_eda/#global","title":"Global\u00b6","text":""},{"location":"bitcoin_eda/#bitcoin-daily-price-data","title":"Bitcoin Daily Price Data\u00b6","text":"Column Name Description market_date Cryptocurrency markets trade daily with no holidays open_price $ USD price at the beginning of the day high_price Intra-day highest sell price in $ USD low_price Intra-day lowest sell price in $ USD close_price $ USD price at the end of the day adjusted_close_price $ USD price after splits and dividend distributions volume The daily amount of traded units of cryptocurrency"},{"location":"bitcoin_eda/#q1","title":"Q1\u00b6","text":"<p>What is the earliest and latest market_date values?</p>"},{"location":"bitcoin_eda/#q2","title":"Q2\u00b6","text":"<p>What was the historic all-time high and low values for the <code>close_price</code> and their dates?</p>"},{"location":"bitcoin_eda/#union-all","title":"Union ALL\u00b6","text":""},{"location":"bitcoin_eda/#window-function","title":"Window Function\u00b6","text":""},{"location":"bitcoin_eda/#q3","title":"Q3\u00b6","text":"<p>Which date had the most <code>volume</code> traded and what was the <code>close_price</code> for that day?</p>"},{"location":"bitcoin_eda/#q4","title":"Q4\u00b6","text":"<p>How many days had a <code>low_price</code> price which was $10\\%$ less than the <code>open_price</code>?</p>"},{"location":"bitcoin_eda/#q5","title":"Q5\u00b6","text":"<p>What percentage of days have a higher <code>close_price</code> than <code>open_price</code>?</p>"},{"location":"bitcoin_eda/#q6","title":"Q6\u00b6","text":"<p>What was the largest difference between <code>high_price</code> and <code>low_price</code> and which date did it occur?</p>"},{"location":"bitcoin_eda/#q7","title":"Q7\u00b6","text":"<p>If we invested $\\$10,000$ on the 1st January 2016 - how much is our investment worth in 1st of February 2021? Use the <code>close_price</code> for this calculation.</p> <p>The first step is to compute the compound annual growth rate:</p> <p>$$ \\begin{align*} \\text{CAGR} &amp; = \\Big[ \\left( \\frac{\\text{end value}}{\\text{start value}} \\right)^{\\frac{1}{n}} - 1 \\Big] \\times 100 \\end{align*} $$</p> <ul> <li>$n$ is the number of years</li> <li>$\\text{start value}$ is the initial price</li> <li>$\\text{end value}$ is the final price</li> </ul> <p>To accomplish this, we use the following query:</p>"},{"location":"bitcoin_window_functions/","title":"Window Functions","text":"In\u00a0[1]: Copied! <pre>from IPython.core.interactiveshell import InteractiveShell\n\nInteractiveShell.ast_node_interactivity = \"all\"\n\nimport os\nimport sys\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nsys.path.append(\"../../../\")\nfrom src.athena import Athena\nfrom src.utils import create_session\n</pre> from IPython.core.interactiveshell import InteractiveShell  InteractiveShell.ast_node_interactivity = \"all\"  import os import sys  import matplotlib.pyplot as plt import pandas as pd  sys.path.append(\"../../../\") from src.athena import Athena from src.utils import create_session In\u00a0[2]: Copied! <pre>boto3_session = create_session(\n    profile_name=\"dev\",\n    role_arn=os.getenv(\"ATHENA_IAM_ROLE_ARN\"),\n)\n\nwait = True\nctas_approach = False\n\ntable = \"daily_btc\"\ndatabase = \"trading\"\n\nathena = Athena(boto3_session=boto3_session, s3_output=os.getenv(\"ATHENA_S3_OUTPUT\"))\nathena\n</pre> boto3_session = create_session(     profile_name=\"dev\",     role_arn=os.getenv(\"ATHENA_IAM_ROLE_ARN\"), )  wait = True ctas_approach = False  table = \"daily_btc\" database = \"trading\"  athena = Athena(boto3_session=boto3_session, s3_output=os.getenv(\"ATHENA_S3_OUTPUT\")) athena Out[2]: <pre>Athena(boto3_session=Session(region_name='us-east-1'), s3_output=s3://sql-case-studies/query_results)</pre> In\u00a0[5]: Copied! <pre>query = \"\"\" \nSELECT \n    *\nFROM \n    trading.daily_btc\nWHERE (open_price + high_price + low_price + close_price + adjusted_close_price + volume) IS NULL;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  SELECT      * FROM      trading.daily_btc WHERE (open_price + high_price + low_price + close_price + adjusted_close_price + volume) IS NULL; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[5]: market_date open_price high_price low_price close_price adjusted_close_price volume 0 2020-04-17 NaN NaN NaN NaN NaN NaN 1 2020-10-09 NaN NaN NaN NaN NaN NaN 2 2020-10-12 NaN NaN NaN NaN NaN NaN 3 2020-10-13 NaN NaN NaN NaN NaN NaN In\u00a0[6]: Copied! <pre>query = \"\"\"\nCREATE OR REPLACE VIEW updated_daily_btc AS\nSELECT\n    market_date,\n    COALESCE(\n        open_price,\n        LAG(open_price, 1) OVER w,\n        LAG(open_price, 2) OVER w\n    ) AS open_price,\n    COALESCE(\n        high_price,\n        LAG(high_price, 1) OVER w,\n        LAG(high_price, 2) OVER w\n    ) AS high_price,\n    COALESCE(\n        low_price,\n        LAG(low_price, 1) OVER w,\n        LAG(low_price, 2) OVER w\n    ) AS low_price,\n    COALESCE(\n        close_price,\n        LAG(close_price, 1) OVER w,\n        LAG(close_price, 2) OVER w\n    ) AS close_price,\n    COALESCE(\n        adjusted_close_price,\n        LAG(adjusted_close_price, 1) OVER w,\n        LAG(adjusted_close_price, 2) OVER w\n    ) AS adjusted_close_price,\n    COALESCE(\n        volume,\n        LAG(volume, 1) OVER w,\n        LAG(volume, 2) OVER w\n    ) AS volume\nFROM \n    trading.daily_btc\nWINDOW\n    w AS (ORDER BY market_date);\n\"\"\"\n\nathena.create_view(database=database, query=query, wait=wait)\n</pre> query = \"\"\" CREATE OR REPLACE VIEW updated_daily_btc AS SELECT     market_date,     COALESCE(         open_price,         LAG(open_price, 1) OVER w,         LAG(open_price, 2) OVER w     ) AS open_price,     COALESCE(         high_price,         LAG(high_price, 1) OVER w,         LAG(high_price, 2) OVER w     ) AS high_price,     COALESCE(         low_price,         LAG(low_price, 1) OVER w,         LAG(low_price, 2) OVER w     ) AS low_price,     COALESCE(         close_price,         LAG(close_price, 1) OVER w,         LAG(close_price, 2) OVER w     ) AS close_price,     COALESCE(         adjusted_close_price,         LAG(adjusted_close_price, 1) OVER w,         LAG(adjusted_close_price, 2) OVER w     ) AS adjusted_close_price,     COALESCE(         volume,         LAG(volume, 1) OVER w,         LAG(volume, 2) OVER w     ) AS volume FROM      trading.daily_btc WINDOW     w AS (ORDER BY market_date); \"\"\"  athena.create_view(database=database, query=query, wait=wait) <pre>Query executed successfully\n</pre> <p>Check that the missing rows are imputed:</p> In\u00a0[7]: Copied! <pre>query = \"\"\"\nSELECT\n    *\nFROM\n    trading.updated_daily_btc\nWHERE market_date IN (\n    DATE '2020-04-16',\n    DATE '2020-04-17',\n    DATE '2020-10-08',\n    DATE '2020-10-09',\n    DATE '2020-10-11',\n    DATE '2020-10-12',\n    DATE '2020-10-13'\n);\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\" SELECT     * FROM     trading.updated_daily_btc WHERE market_date IN (     DATE '2020-04-16',     DATE '2020-04-17',     DATE '2020-10-08',     DATE '2020-10-09',     DATE '2020-10-11',     DATE '2020-10-12',     DATE '2020-10-13' ); \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[7]: market_date open_price high_price low_price close_price adjusted_close_price volume 0 2020-04-16 6640.454102 7134.450684 6555.504395 7116.804199 7116.804199 4.678324e+10 1 2020-04-17 6640.454102 7134.450684 6555.504395 7116.804199 7116.804199 4.678324e+10 2 2020-10-08 10677.625000 10939.799805 10569.823242 10923.627930 10923.627930 2.196212e+10 3 2020-10-09 10677.625000 10939.799805 10569.823242 10923.627930 10923.627930 2.196212e+10 4 2020-10-11 11296.082031 11428.813477 11288.627930 11384.181641 11384.181641 1.996863e+10 5 2020-10-12 11296.082031 11428.813477 11288.627930 11384.181641 11384.181641 1.996863e+10 6 2020-10-13 11296.082031 11428.813477 11288.627930 11384.181641 11384.181641 1.996863e+10 In\u00a0[8]: Copied! <pre>query = \"\"\" \nWITH volume_data AS (\n    SELECT\n        market_date,\n        volume\n    FROM\n        trading.updated_daily_btc\n    LIMIT 5\n)\nSELECT\n    vd1.market_date AS vd1_market_date,\n    vd1.volume AS vd1_volume,\n    vd2.market_date AS vd2_market_date,\n    vd2.volume AS vd2_volume\nFROM\n    volume_data AS vd1 \n    INNER JOIN volume_data AS vd2 ON vd1.market_date &gt;= vd2.market_date\nORDER BY\n    vd1.market_date,\n    vd1.volume,\n    vd2.market_date,\n    vd2.volume;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  WITH volume_data AS (     SELECT         market_date,         volume     FROM         trading.updated_daily_btc     LIMIT 5 ) SELECT     vd1.market_date AS vd1_market_date,     vd1.volume AS vd1_volume,     vd2.market_date AS vd2_market_date,     vd2.volume AS vd2_volume FROM     volume_data AS vd1      INNER JOIN volume_data AS vd2 ON vd1.market_date &gt;= vd2.market_date ORDER BY     vd1.market_date,     vd1.volume,     vd2.market_date,     vd2.volume; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[8]: vd1_market_date vd1_volume vd2_market_date vd2_volume 0 2014-09-17 21056800.0 2014-09-17 21056800.0 1 2014-09-18 34483200.0 2014-09-17 21056800.0 2 2014-09-18 34483200.0 2014-09-18 34483200.0 3 2014-09-19 37919700.0 2014-09-17 21056800.0 4 2014-09-19 37919700.0 2014-09-18 34483200.0 5 2014-09-19 37919700.0 2014-09-19 37919700.0 6 2014-09-20 36863600.0 2014-09-17 21056800.0 7 2014-09-20 36863600.0 2014-09-18 34483200.0 8 2014-09-20 36863600.0 2014-09-19 37919700.0 9 2014-09-20 36863600.0 2014-09-20 36863600.0 10 2014-09-21 26580100.0 2014-09-17 21056800.0 11 2014-09-21 26580100.0 2014-09-18 34483200.0 12 2014-09-21 26580100.0 2014-09-19 37919700.0 13 2014-09-21 26580100.0 2014-09-20 36863600.0 14 2014-09-21 26580100.0 2014-09-21 26580100.0 In\u00a0[9]: Copied! <pre>query = \"\"\"\nWITH volume_data AS (\n    SELECT\n        market_date,\n        volume\n    FROM\n        trading.updated_daily_btc\n)\nSELECT\n    vd1.market_date AS market_date,\n    vd1.volume AS volume,\n    SUM(vd2.volume) AS running_total_volume\nFROM\n    volume_data AS vd1 \n    INNER JOIN volume_data AS vd2 ON vd1.market_date &gt;= vd2.market_date\nGROUP BY\n    vd1.market_date,\n    vd1.volume\nORDER BY\n    vd1.market_date, \n    vd1.volume;\n\"\"\"\n\ncum_volume = athena.query(database=database, query=query, ctas_approach=ctas_approach)\n\nwith pd.option_context(\"display.float_format\", lambda val: f\"{val:,.0f}\"):\n    display(cum_volume)\n</pre> query = \"\"\" WITH volume_data AS (     SELECT         market_date,         volume     FROM         trading.updated_daily_btc ) SELECT     vd1.market_date AS market_date,     vd1.volume AS volume,     SUM(vd2.volume) AS running_total_volume FROM     volume_data AS vd1      INNER JOIN volume_data AS vd2 ON vd1.market_date &gt;= vd2.market_date GROUP BY     vd1.market_date,     vd1.volume ORDER BY     vd1.market_date,      vd1.volume; \"\"\"  cum_volume = athena.query(database=database, query=query, ctas_approach=ctas_approach)  with pd.option_context(\"display.float_format\", lambda val: f\"{val:,.0f}\"):     display(cum_volume) market_date volume running_total_volume 0 2014-09-17 21,056,800 21,056,800 1 2014-09-18 34,483,200 55,540,000 2 2014-09-19 37,919,700 93,459,700 3 2014-09-20 36,863,600 130,323,300 4 2014-09-21 26,580,100 156,903,400 ... ... ... ... 2348 2021-02-20 68,145,460,026 24,518,108,881,462 2349 2021-02-21 51,897,585,191 24,570,006,466,653 2350 2021-02-22 92,052,420,332 24,662,058,886,985 2351 2021-02-23 106,102,492,824 24,768,161,379,809 2352 2021-02-24 88,364,793,856 24,856,526,173,665 <p>2353 rows \u00d7 3 columns</p> In\u00a0[10]: Copied! <pre>query = \"\"\" \nWITH volume_data AS (\n    SELECT\n        market_date,\n        volume\n    FROM \n      trading.updated_daily_btc\n    ORDER BY \n      market_date\n)\nSELECT\n  market_date,\n  volume,\n  SUM(volume) OVER (ORDER BY market_date RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cumulative_sum\nFROM \n  volume_data;\n\"\"\"\n\ncum_volume = athena.query(database=database, query=query, ctas_approach=ctas_approach)\n\nwith pd.option_context(\"display.float_format\", lambda val: f\"{val:,.0f}\"):\n    display(cum_volume)\n</pre> query = \"\"\"  WITH volume_data AS (     SELECT         market_date,         volume     FROM        trading.updated_daily_btc     ORDER BY        market_date ) SELECT   market_date,   volume,   SUM(volume) OVER (ORDER BY market_date RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cumulative_sum FROM    volume_data; \"\"\"  cum_volume = athena.query(database=database, query=query, ctas_approach=ctas_approach)  with pd.option_context(\"display.float_format\", lambda val: f\"{val:,.0f}\"):     display(cum_volume) market_date volume cumulative_sum 0 2014-09-17 21,056,800 21,056,800 1 2014-09-18 34,483,200 55,540,000 2 2014-09-19 37,919,700 93,459,700 3 2014-09-20 36,863,600 130,323,300 4 2014-09-21 26,580,100 156,903,400 ... ... ... ... 2348 2021-02-20 68,145,460,026 24,518,108,881,462 2349 2021-02-21 51,897,585,191 24,570,006,466,653 2350 2021-02-22 92,052,420,332 24,662,058,886,985 2351 2021-02-23 106,102,492,824 24,768,161,379,809 2352 2021-02-24 88,364,793,856 24,856,526,173,665 <p>2353 rows \u00d7 3 columns</p> <p>The window function used here is</p> <pre>SUM(volume) OVER (ORDER BY market_date RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)\n</pre> In\u00a0[11]: Copied! <pre>query = \"\"\" \nWITH window_calc AS (\n    SELECT\n        market_date,\n        volume,\n        AVG(volume) OVER (\n            ORDER BY market_date \n            RANGE BETWEEN INTERVAL '7' DAY PRECEDING AND INTERVAL '1' DAY PRECEDING\n        ) AS past_week_avg_volume\n    FROM\n        trading.updated_daily_btc\n)\nSELECT\n    market_date,\n    volume,\n    past_week_avg_volume,\n    CASE\n        WHEN volume &gt; past_week_avg_volume THEN 'Above Past Week Average'\n        ELSE 'Below Past Week Average'\n    END AS volume_trend\nFROM\n    window_calc\nORDER BY\n    market_date DESC;\n\"\"\"\n\nvolume_trend = athena.query(database=database, query=query, ctas_approach=ctas_approach)\n\nwith pd.option_context(\"display.float_format\", lambda val: f\"{val:,.4f}\"):\n    display(volume_trend)\n</pre> query = \"\"\"  WITH window_calc AS (     SELECT         market_date,         volume,         AVG(volume) OVER (             ORDER BY market_date              RANGE BETWEEN INTERVAL '7' DAY PRECEDING AND INTERVAL '1' DAY PRECEDING         ) AS past_week_avg_volume     FROM         trading.updated_daily_btc ) SELECT     market_date,     volume,     past_week_avg_volume,     CASE         WHEN volume &gt; past_week_avg_volume THEN 'Above Past Week Average'         ELSE 'Below Past Week Average'     END AS volume_trend FROM     window_calc ORDER BY     market_date DESC; \"\"\"  volume_trend = athena.query(database=database, query=query, ctas_approach=ctas_approach)  with pd.option_context(\"display.float_format\", lambda val: f\"{val:,.4f}\"):     display(volume_trend) market_date volume past_week_avg_volume volume_trend 0 2021-02-24 88,364,793,856.0000 73,509,817,753.4286 Above Past Week Average 1 2021-02-23 106,102,492,824.0000 69,359,402,048.0000 Above Past Week Average 2 2021-02-22 92,052,420,332.0000 67,219,042,452.8571 Above Past Week Average 3 2021-02-21 51,897,585,191.0000 69,983,483,886.7143 Below Past Week Average 4 2021-02-20 68,145,460,026.0000 70,284,197,619.4286 Below Past Week Average ... ... ... ... ... 2348 2014-09-21 26,580,100.0000 32,580,825.0000 Below Past Week Average 2349 2014-09-20 36,863,600.0000 31,153,233.3333 Above Past Week Average 2350 2014-09-19 37,919,700.0000 27,770,000.0000 Above Past Week Average 2351 2014-09-18 34,483,200.0000 21,056,800.0000 Above Past Week Average 2352 2014-09-17 21,056,800.0000 NaN Below Past Week Average <p>2353 rows \u00d7 4 columns</p> <p>We compare each day's volume with the average volume of the past 7 days (note: the average calculation does not include the current row). If the volume is higher than the average, the <code>volume_trend</code> is 'Above Past Week Average'. If the volume is lower than the average, the <code>volume_trend</code> is 'Below Past Week Average'.</p> <pre>AVG(volume) OVER (\n    ORDER BY market_date \n    RANGE BETWEEN INTERVAL '7' DAY PRECEDING AND INTERVAL '1' DAY PRECEDING\n) AS past_week_avg_volume\n</pre> <p>The <code>RANGE</code> mode uses the logical range of <code>market_date</code> values and not the physical range of rows. This ensures that we average over the past 7 days, even if there are missing or mupltiple rows for a day (which is not the case in this data set but may happen in other datasets).</p> <p>Note: Athena's data manipulation language generally support Trino syntax. In postgresql, we do not need to use the <code>INTERVERAL</code> keyword when specifying the range. The following code will work in postgresql:</p> <pre>  AVG(volume) OVER (\n    ORDER BY market_date\n    RANGE BETWEEN '7 DAYS' PRECEDING and '1 DAY' PRECEDING\n  ) AS past_week_avg_volume\n</pre> In\u00a0[12]: Copied! <pre>query = \"\"\" \nSELECT\n    market_date,\n    close_price,\n    -- Moving Average\n    ROUND(AVG(close_price) OVER w_14, 2) AS avg_14,\n    ROUND(AVG(close_price) OVER w_28, 2) AS avg_28,\n    ROUND(AVG(close_price) OVER w_60, 2) AS avg_60,\n    ROUND(AVG(close_price) OVER w_150, 2) AS avg_150,\n    -- Standard Deviation\n    ROUND(STDDEV(close_price) OVER w_14, 2) AS std_14,\n    ROUND(STDDEV(close_price) OVER w_28, 2) AS std_28,\n    ROUND(STDDEV(close_price) OVER w_60, 2) AS std_60,\n    ROUND(STDDEV(close_price) OVER w_150, 2) AS std_150,\n    -- Max\n    ROUND(MAX(close_price) OVER w_14, 2) AS max_14,\n    ROUND(MAX(close_price) OVER w_28, 2) AS max_28,\n    ROUND(MAX(close_price) OVER w_60, 2) AS max_60,\n    ROUND(MAX(close_price) OVER w_150, 2) AS max_150,\n    -- Min\n    ROUND(MIN(close_price) OVER w_14, 2) AS min_14,\n    ROUND(MIN(close_price) OVER w_28, 2) AS min_28,\n    ROUND(MIN(close_price) OVER w_60, 2) AS min_60,\n    ROUND(MIN(close_price) OVER w_150, 2) AS min_150\nFROM \n    trading.updated_daily_btc\nWINDOW\n    w_14 AS (ORDER BY market_date RANGE BETWEEN INTERVAL '14' DAY PRECEDING AND INTERVAL '1' DAY PRECEDING),\n    w_28 AS (ORDER BY market_date RANGE BETWEEN INTERVAL '28' DAY PRECEDING AND INTERVAL '1' DAY PRECEDING),\n    w_60 AS (ORDER BY market_date RANGE BETWEEN INTERVAL '60' DAY PRECEDING AND INTERVAL '1' DAY PRECEDING),\n    w_150 AS (ORDER BY market_date RANGE BETWEEN INTERVAL '150' DAY PRECEDING AND INTERVAL '1' DAY PRECEDING)\nORDER BY \n    market_date ASC;\n\"\"\"\n\nprice_stats = athena.query(database=database, query=query, ctas_approach=ctas_approach)\n\nwith pd.option_context(\"display.float_format\", lambda val: f\"${val:,.4f}\"):\n    display(price_stats)\n</pre> query = \"\"\"  SELECT     market_date,     close_price,     -- Moving Average     ROUND(AVG(close_price) OVER w_14, 2) AS avg_14,     ROUND(AVG(close_price) OVER w_28, 2) AS avg_28,     ROUND(AVG(close_price) OVER w_60, 2) AS avg_60,     ROUND(AVG(close_price) OVER w_150, 2) AS avg_150,     -- Standard Deviation     ROUND(STDDEV(close_price) OVER w_14, 2) AS std_14,     ROUND(STDDEV(close_price) OVER w_28, 2) AS std_28,     ROUND(STDDEV(close_price) OVER w_60, 2) AS std_60,     ROUND(STDDEV(close_price) OVER w_150, 2) AS std_150,     -- Max     ROUND(MAX(close_price) OVER w_14, 2) AS max_14,     ROUND(MAX(close_price) OVER w_28, 2) AS max_28,     ROUND(MAX(close_price) OVER w_60, 2) AS max_60,     ROUND(MAX(close_price) OVER w_150, 2) AS max_150,     -- Min     ROUND(MIN(close_price) OVER w_14, 2) AS min_14,     ROUND(MIN(close_price) OVER w_28, 2) AS min_28,     ROUND(MIN(close_price) OVER w_60, 2) AS min_60,     ROUND(MIN(close_price) OVER w_150, 2) AS min_150 FROM      trading.updated_daily_btc WINDOW     w_14 AS (ORDER BY market_date RANGE BETWEEN INTERVAL '14' DAY PRECEDING AND INTERVAL '1' DAY PRECEDING),     w_28 AS (ORDER BY market_date RANGE BETWEEN INTERVAL '28' DAY PRECEDING AND INTERVAL '1' DAY PRECEDING),     w_60 AS (ORDER BY market_date RANGE BETWEEN INTERVAL '60' DAY PRECEDING AND INTERVAL '1' DAY PRECEDING),     w_150 AS (ORDER BY market_date RANGE BETWEEN INTERVAL '150' DAY PRECEDING AND INTERVAL '1' DAY PRECEDING) ORDER BY      market_date ASC; \"\"\"  price_stats = athena.query(database=database, query=query, ctas_approach=ctas_approach)  with pd.option_context(\"display.float_format\", lambda val: f\"${val:,.4f}\"):     display(price_stats) market_date close_price avg_14 avg_28 avg_60 avg_150 std_14 std_28 std_60 std_150 max_14 max_28 max_60 max_150 min_14 min_28 min_60 min_150 0 2014-09-17 $457.3340 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1 2014-09-18 $424.4400 $457.3300 $457.3300 $457.3300 $457.3300 NaN NaN NaN NaN $457.3300 $457.3300 $457.3300 $457.3300 $457.3300 $457.3300 $457.3300 $457.3300 2 2014-09-19 $394.7960 $440.8900 $440.8900 $440.8900 $440.8900 $23.2600 $23.2600 $23.2600 $23.2600 $457.3300 $457.3300 $457.3300 $457.3300 $424.4400 $424.4400 $424.4400 $424.4400 3 2014-09-20 $408.9040 $425.5200 $425.5200 $425.5200 $425.5200 $31.2800 $31.2800 $31.2800 $31.2800 $457.3300 $457.3300 $457.3300 $457.3300 $394.8000 $394.8000 $394.8000 $394.8000 4 2014-09-21 $398.8210 $421.3700 $421.3700 $421.3700 $421.3700 $26.8600 $26.8600 $26.8600 $26.8600 $457.3300 $457.3300 $457.3300 $457.3300 $394.8000 $394.8000 $394.8000 $394.8000 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 2348 2021-02-20 $56,099.5195 $47,418.8600 $40,661.7700 $36,361.2200 $23,705.9400 $4,517.8900 $7,732.8600 $7,644.9200 $11,792.5300 $55,888.1300 $55,888.1300 $55,888.1300 $55,888.1300 $38,903.4400 $30,432.5500 $23,241.3500 $10,225.8600 2349 2021-02-21 $57,539.9453 $48,621.2600 $41,520.0600 $36,899.8300 $24,011.7700 $4,420.1600 $8,069.9700 $7,878.5500 $12,032.9800 $56,099.5200 $56,099.5200 $56,099.5200 $56,099.5200 $38,903.4400 $30,432.5500 $23,241.3500 $10,565.4900 2350 2021-02-22 $54,207.3203 $49,952.4300 $42,421.8600 $37,471.4700 $24,323.7300 $4,060.0400 $8,404.1900 $8,111.5900 $12,290.5600 $57,539.9500 $57,539.9500 $57,539.9500 $57,539.9500 $44,918.1800 $30,432.5500 $23,735.9500 $10,565.4900 2351 2021-02-23 $48,824.4258 $50,524.6400 $43,201.8900 $37,979.3200 $24,613.7600 $4,054.4700 $8,449.7800 $8,190.5600 $12,478.8400 $57,539.9500 $57,539.9500 $57,539.9500 $57,539.9500 $44,918.1800 $30,432.5500 $24,664.7900 $10,565.4900 2352 2021-02-24 $50,460.2344 $50,692.0200 $43,782.4200 $38,381.9900 $24,867.5600 $3,920.8700 $8,248.2400 $8,118.4400 $12,581.7900 $57,539.9500 $57,539.9500 $57,539.9500 $57,539.9500 $44,918.1800 $30,432.5500 $26,272.2900 $10,565.4900 <p>2353 rows \u00d7 18 columns</p> <p>Because the query above invovles multiple window functions, we can name each windowing behavior in a <code>WINDOW</code> clause that can be referenced in each <code>OVER</code>. This is supported by Trino SQL.</p> In\u00a0[13]: Copied! <pre>query = \"\"\"\nWITH window_calc AS (\n    SELECT\n        market_date,\n        ROUND(close_price, 2) AS close_price,\n        -- Moving Average\n        ROUND(AVG(close_price) OVER w_1_14, 2) AS avg_1_14,\n        ROUND(AVG(close_price) OVER w_15_28, 2) AS avg_15_28,\n        ROUND(AVG(close_price) OVER w_29_60, 2) AS avg_29_60,\n        ROUND(AVG(close_price) OVER w_61_150, 2) AS avg_61_150\n    FROM\n        trading.updated_daily_btc\n    WINDOW\n        w_1_14 AS (ORDER BY market_date RANGE BETWEEN INTERVAL '14' DAY PRECEDING AND INTERVAL '1' DAY PRECEDING),\n        w_15_28 AS (ORDER BY market_date RANGE BETWEEN INTERVAL '28' DAY PRECEDING AND INTERVAL '15' DAY PRECEDING),\n        w_29_60 AS (ORDER BY market_date RANGE BETWEEN INTERVAL '60' DAY PRECEDING AND INTERVAL '29' DAY PRECEDING),\n        w_61_150 AS (ORDER BY market_date RANGE BETWEEN INTERVAL '150' DAY PRECEDING AND INTERVAL '61' DAY PRECEDING)\n)\nSELECT\n    market_date,\n    close_price,\n    (0.5 * avg_1_14 + 0.3 * avg_15_28 + 0.15 * avg_29_60 + 0.05 * avg_61_150) AS weighted_avg\nFROM\n    window_calc\nORDER BY\n    market_date ASC;\n\"\"\"\n\nweighted_avg = athena.query(database=database, query=query, ctas_approach=ctas_approach)\n\nwith pd.option_context(\"display.float_format\", lambda val: f\"${val:,.4f}\"):\n    display(weighted_avg)\n</pre> query = \"\"\" WITH window_calc AS (     SELECT         market_date,         ROUND(close_price, 2) AS close_price,         -- Moving Average         ROUND(AVG(close_price) OVER w_1_14, 2) AS avg_1_14,         ROUND(AVG(close_price) OVER w_15_28, 2) AS avg_15_28,         ROUND(AVG(close_price) OVER w_29_60, 2) AS avg_29_60,         ROUND(AVG(close_price) OVER w_61_150, 2) AS avg_61_150     FROM         trading.updated_daily_btc     WINDOW         w_1_14 AS (ORDER BY market_date RANGE BETWEEN INTERVAL '14' DAY PRECEDING AND INTERVAL '1' DAY PRECEDING),         w_15_28 AS (ORDER BY market_date RANGE BETWEEN INTERVAL '28' DAY PRECEDING AND INTERVAL '15' DAY PRECEDING),         w_29_60 AS (ORDER BY market_date RANGE BETWEEN INTERVAL '60' DAY PRECEDING AND INTERVAL '29' DAY PRECEDING),         w_61_150 AS (ORDER BY market_date RANGE BETWEEN INTERVAL '150' DAY PRECEDING AND INTERVAL '61' DAY PRECEDING) ) SELECT     market_date,     close_price,     (0.5 * avg_1_14 + 0.3 * avg_15_28 + 0.15 * avg_29_60 + 0.05 * avg_61_150) AS weighted_avg FROM     window_calc ORDER BY     market_date ASC; \"\"\"  weighted_avg = athena.query(database=database, query=query, ctas_approach=ctas_approach)  with pd.option_context(\"display.float_format\", lambda val: f\"${val:,.4f}\"):     display(weighted_avg) market_date close_price weighted_avg 0 2014-09-17 $457.3300 NaN 1 2014-09-18 $424.4400 NaN 2 2014-09-19 $394.8000 NaN 3 2014-09-20 $408.9000 NaN 4 2014-09-21 $398.8200 NaN ... ... ... ... 2348 2021-02-20 $56,099.5200 $39,534.0265 2349 2021-02-21 $57,539.9500 $40,335.8425 2350 2021-02-22 $54,207.3200 $41,192.5110 2351 2021-02-23 $48,824.4300 $41,822.6715 2352 2021-02-24 $50,460.2300 $42,249.2420 <p>2353 rows \u00d7 3 columns</p> In\u00a0[14]: Copied! <pre>fig, ax = plt.subplots(figsize=(15, 7))\nplt.plot(weighted_avg[\"market_date\"], weighted_avg[\"close_price\"], label=\"Close Price\")\nplt.plot(\n    weighted_avg[\"market_date\"], weighted_avg[\"weighted_avg\"], label=\"Weighted Average\"\n)\nplt.legend()\nplt.title(\"Close Price vs Weighted Average Bitcoin Price\")\nplt.show();\n</pre> fig, ax = plt.subplots(figsize=(15, 7)) plt.plot(weighted_avg[\"market_date\"], weighted_avg[\"close_price\"], label=\"Close Price\") plt.plot(     weighted_avg[\"market_date\"], weighted_avg[\"weighted_avg\"], label=\"Weighted Average\" ) plt.legend() plt.title(\"Close Price vs Weighted Average Bitcoin Price\") plt.show(); In\u00a0[15]: Copied! <pre>query = \"\"\"\nSELECT\n    market_date,\n    close_price\nFROM\n    trading.updated_daily_btc;\n\"\"\"\n\nclose_price = athena.query(database=database, query=query, ctas_approach=ctas_approach)\n\nwindow_size = 14\n\n# Calculate the smoothing factor \u03bb\nsmoothing_factor = 2 / (window_size + 1)\n\n# Compute the exponential weighted average (EWA) for the 'close_price' column\nclose_price[\"ewa_close_price\"] = (\n    close_price[\"close_price\"].ewm(alpha=smoothing_factor, adjust=False).mean()\n)\n\nfig, ax = plt.subplots(figsize=(15, 7))\nplt.plot(close_price[\"market_date\"], close_price[\"close_price\"], label=\"Close Price\")\nplt.plot(\n    close_price[\"market_date\"],\n    close_price[\"ewa_close_price\"],\n    label=\"Exponential Weighted Average\",\n)\nplt.legend()\nplt.title(\"Close Price vs Exponential Weighted Average Bitcoin Price\")\nplt.show();\n</pre> query = \"\"\" SELECT     market_date,     close_price FROM     trading.updated_daily_btc; \"\"\"  close_price = athena.query(database=database, query=query, ctas_approach=ctas_approach)  window_size = 14  # Calculate the smoothing factor \u03bb smoothing_factor = 2 / (window_size + 1)  # Compute the exponential weighted average (EWA) for the 'close_price' column close_price[\"ewa_close_price\"] = (     close_price[\"close_price\"].ewm(alpha=smoothing_factor, adjust=False).mean() )  fig, ax = plt.subplots(figsize=(15, 7)) plt.plot(close_price[\"market_date\"], close_price[\"close_price\"], label=\"Close Price\") plt.plot(     close_price[\"market_date\"],     close_price[\"ewa_close_price\"],     label=\"Exponential Weighted Average\", ) plt.legend() plt.title(\"Close Price vs Exponential Weighted Average Bitcoin Price\") plt.show(); In\u00a0[16]: Copied! <pre>athena.drop_view(database=database, view=\"updated_daily_btc\", wait=wait)\n</pre> athena.drop_view(database=database, view=\"updated_daily_btc\", wait=wait) <pre>Query executed successfully\n</pre>"},{"location":"bitcoin_window_functions/#global","title":"Global\u00b6","text":""},{"location":"bitcoin_window_functions/#fill-null-values","title":"Fill <code>NULL</code> Values\u00b6","text":"<p>Using the propagation of <code>NULL</code> values, we can find all rows with <code>NULL</code> values in any <code>NUMERIC</code> column by summing all columns in a <code>WHERE</code> clause. If the sum is <code>NULL</code>, then the row has a <code>NULL</code> value in at least one column.</p>"},{"location":"bitcoin_window_functions/#fill-null-values-with-lag-coalesce","title":"Fill <code>NULL</code> Values with <code>LAG</code> &amp; <code>Coalesce</code>\u00b6","text":""},{"location":"bitcoin_window_functions/#lag","title":"LAG\u00b6","text":"<p>The <code>LAG</code> function takes an input expression, i.e., a column, expression, or subquery, and returns the value of that expression evaluated at the row that is <code>offset</code> rows before the current row. If there is no such row, the function returns the <code>default_value</code> if it is specified, otherwise it returns <code>NULL</code>.</p> <pre>LAG(expression [,offset [,default_value]]) \nOVER (\n    [PARTITION BY partition_expression, ... ]\n    ORDER BY sort_expression [ASC | DESC], ...\n)\n</pre> <ul> <li><p><code>expression</code>: The expression must return a single value, e.g., a column, expression, or subquery, and it cannot be a window function.</p> </li> <li><p><code>offset</code>: The offset is a positive integer that specifies the number of rows that come before the current row from which to access data. The offset can be an expression, subquery, or column. It defaults to 1 if not specified.</p> </li> <li><p><code>default_value</code>: The default value is the value that the function returns if there is no row that is <code>offset</code> rows before the current row.</p> </li> </ul>"},{"location":"bitcoin_window_functions/#coalesce","title":"COALESCE\u00b6","text":"<p>The <code>COALESCE</code> function returns the first non-<code>NULL</code> value in the list of arguments. If all arguments are <code>NULL</code>, the function returns <code>NULL</code>. The function evaluates arguments in order and returns the first value that is not <code>NULL</code>; if a non-<code>NULL</code> value is found, all subsequent arguments are not evaluated.</p> <pre>COALESCE (argument_1, argument_2, \u2026)\n</pre> <p>Since there may be more than one consecutive missing rows, we supply two <code>LAG</code> functions to fill the <code>NULL</code> values:</p> <ul> <li><p><code>LAG(column, 1)</code>: This fills each <code>NULL</code> row with the previous day's value.</p> </li> <li><p><code>LAG(column, 2)</code>: This fills the <code>NULL</code> row with the value from two days ago.</p> </li> </ul> <p>We can create a view:</p>"},{"location":"bitcoin_window_functions/#cumulative-volume","title":"Cumulative Volume\u00b6","text":""},{"location":"bitcoin_window_functions/#self-join","title":"Self-Join\u00b6","text":"<p>The first approach to compute the cumulative (running) trade volume is to use a self-join:</p>"},{"location":"bitcoin_window_functions/#cartesian-product","title":"Cartesian Product\u00b6","text":"<p>The self-join results in a cartesian product, also known as a cross join:</p> <pre>INNER JOIN volume_data AS vd2 ON vd1.market_date &gt;= vd2.market_date\n</pre> <p>The self-join creates pairs of market dates and volumes, where each date is paired with all previous dates. Each date (i.e., a row in <code>vd1.market_date</code>) is paired with all earlier and equal dates (i.e., multiple rows in <code>vd2.market_date</code>), providing a basis to sum volumes up to the current date.</p>"},{"location":"bitcoin_window_functions/#order-by","title":"Order By\u00b6","text":"<p>The <code>ORDER BY</code> clause in the query sorts the results:</p> <ol> <li><p><code>vd1.market_date</code>: This sorts the primary rows by <code>market_date</code> in <code>vd1</code>, ensuring that we are looking at the primary dates in chronological order.</p> </li> <li><p><code>vd1.volume</code>: If there are multiple entries for the same <code>vd1.market_date</code>, as they are paired with all earlier dates, this will sort those entries by <code>volume</code>.</p> </li> <li><p><code>vd2.market_date</code>: This sorts the paired rows (dates in <code>vd2</code> that are earlier than or equal to <code>vd1.market_date</code>) in chronological order within each primary date.</p> </li> <li><p><code>vd2.volume</code>: If there are multiple entries with unique volume values for the same pair of <code>vd1.market_date</code> and <code>vd2.market_date</code>, this will sort those entries by volume.</p> </li> </ol> <p>To completely the cumulative volume calculation, we simply add a <code>SUM</code> function to sum the <code>vd2.volume</code> column for each unique combination of <code>vd1.market_date</code> and <code>vd1.volume</code>:</p>"},{"location":"bitcoin_window_functions/#window-function","title":"Window Function\u00b6","text":"<p>Another approach to compute the cumulative volume is to use a window function:</p>"},{"location":"bitcoin_window_functions/#order-by","title":"ORDER BY\u00b6","text":"<p>The <code>ORDER BY market_date</code> clause specifies that the rows should be ordered by the <code>market_date</code> column in ascending order, i.e., from the earliest to the latest market date.</p>"},{"location":"bitcoin_window_functions/#range-between-unbounded-preceding-and-current-row","title":"RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\u00b6","text":"<p>This defines the frame of rows over which the window function operates.</p> <ul> <li><code>RANGE BETWEEN UNBOUNDED PRECEDING</code> means that the window starts from the first row in the partition (since no partition is defined, it starts from the first row of the entire result set).</li> <li><code>CURRENT ROW</code> means that the window ends at the current row.</li> </ul> <p>Effectively, this clause ensures that for each row, the window includes all previous rows up to and including the current row. This is why it is used to calculate a cumulative sum, as it sums the <code>volume</code> from the row with the earliest <code>market_date</code> to the current row.</p>"},{"location":"bitcoin_window_functions/#window-frame-clause","title":"Window Frame Clause\u00b6","text":"<p>The <code>RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW</code> clause is part of the window frame clause, which defines the set of rows over which the window function operates. The window frame clause has the following components:</p> <ul> <li><p>Window Frame Modes</p> <ul> <li><p>RANGE: Defines the window frame based on a logical range of values. For example, <code>RANGE BETWEEN 5 PRECEDING AND CURRENT ROW</code> includes the current row and rows within a range of values before it. This mode works with data types with well-defined ranges, such as dates and numbers.</p> </li> <li><p>ROWS: Defines the window frame based on a physical number of rows. For example, <code>ROWS BETWEEN 5 PRECEDING AND CURRENT ROW</code> includes the current row and the five rows preceding it.</p> </li> <li><p>GROUPS: Assigns each row of the result set into a group, similar to the <code>GROUP BY</code> clause. The frame starts or ends a specified number of peer groups before or after the current row's peer group. A peer group is a set of rows that are equivalent in the ORDER BY ordering. There must be an <code>ORDER BY</code> clause in the window definition to use the <code>GROUPS</code> mode.</p> </li> </ul> </li> <li><p>Start and End Frames</p> <ul> <li><p>PRECEDING: Refers to the rows before the current row in the window frame. For example, <code>3 PRECEDING</code> means three rows before the current row.</p> </li> <li><p>FOLLOWING: Refers to the rows after the current row in the window frame. For example, <code>3 FOLLOWING</code> means three rows after the current row.</p> </li> <li><p>UNBOUNDED: Extends the window frame to the beginning (<code>UNBOUNDED PRECEDING</code>) or the end (<code>UNBOUNDED FOLLOWING</code>) of the partition.</p> </li> <li><p>OFFSET: Specifies a fixed number of rows or range of values before or after the current row. For example, <code>5 PRECEDING</code> or <code>5 FOLLOWING</code>.</p> </li> </ul> </li> <li><p>Frame Exclusions: This option allows rows around the current row to be excluded from the frame, even if they would be included according to the frame start and frame end options.</p> <ul> <li><p>EXCLUDE CURRENT ROW: Excludes the current row from the frame.</p> </li> <li><p>EXCLUDE TIES: Excludes any peers of the current row from the frame, but not the current row itself.</p> </li> <li><p>EXCLUDE GROUP: Excludes the current row and its ordering peers from the frame. This removes all rows that are equivalent to the current row in the ORDER BY ordering. For example, <code>ROWS BETWEEN CURRENT ROW AND 1 FOLLOWING EXCLUDE GROUP</code> excludes the current row and its peers.</p> </li> <li><p>EXCLUDE NO OTHERS: This is the default behavior, which does not exclude any rows from the frame.</p> </li> </ul> </li> </ul>"},{"location":"bitcoin_window_functions/#weekly-volume-comparison","title":"Weekly Volume Comparison\u00b6","text":"<p>We can compute the weekly moving average of the volume of the bitcoin traded in the market. This can be accomplished by a window function:</p>"},{"location":"bitcoin_window_functions/#moving-statistics-close-price","title":"Moving Statistics Close Price\u00b6","text":"<p>Window functions can be used to compute moving statistics:</p>"},{"location":"bitcoin_window_functions/#weighted-average-close-price","title":"Weighted Average Close Price\u00b6","text":"<p>The weighted moving average is computed as follows:</p> <p>$$  \\begin{align*} WMA = \\frac{w_1 \\cdot x_1 + w_2 \\cdot x_2 + \\ldots + w_n \\cdot x_n}{w_1 + w_2 + \\ldots + w_n} \\end{align*} $$</p> <p>where</p> <ul> <li>$x_i$ is the value of the time series at time $i$</li> <li>$w_i$ is the weight at time $i$</li> </ul> <p>The weights, for example, can be defined as follows:</p> Window Weight 1 - 14 Days 0.5 15 - 28 Days 0.3 29 - 60 Days 0.15 61 - 150 Days 0.05"},{"location":"bitcoin_window_functions/#exponential-weighted-average-close-price","title":"Exponential Weighted Average Close Price\u00b6","text":"<p>The exponential moving average at time $t$  is given by:</p> <p>$$ \\begin{align*} EMA_{t} = \\lambda \\cdot x_t + (1 - \\lambda) \\cdot EMA_{t-1} \\end{align*} $$</p> <p>where</p> <ul> <li>$x_t$ is the value of the time series at time $t$</li> <li>$EMA_{t-1}$ is the exponential moving average of the time series at time $t-1$</li> <li>$\\lambda$ is the smoothing factor at time $t$</li> </ul> <p>To find the EMA most similar to a moving average with a window of $n$ days, we can use the formula:</p> <p>$$ \\begin{align*} \\lambda = \\frac{2}{n + 1} \\end{align*} $$</p> <p>The proof for this equation is given in the book Smoothing, Forecasting and Prediction by Robert Goodell Brown.</p>"},{"location":"bitcoin_window_functions/#clean-up","title":"Clean Up\u00b6","text":""},{"location":"clique_bait/","title":"Clique Bait","text":"In\u00a0[87]: Copied! <pre>from IPython.core.interactiveshell import InteractiveShell\n\nInteractiveShell.ast_node_interactivity = \"all\"\n\nimport os\nimport sys\n\nsys.path.append(\"../../../\")\nimport numpy as np\nfrom scikit_posthocs import posthoc_dunn\nfrom scipy.stats import kruskal\n\nfrom src.athena import Athena\nfrom src.utils import create_session\n</pre> from IPython.core.interactiveshell import InteractiveShell  InteractiveShell.ast_node_interactivity = \"all\"  import os import sys  sys.path.append(\"../../../\") import numpy as np from scikit_posthocs import posthoc_dunn from scipy.stats import kruskal  from src.athena import Athena from src.utils import create_session In\u00a0[2]: Copied! <pre>boto3_session = create_session(\n    profile_name=\"dev\",\n    role_arn=os.getenv(\"ATHENA_IAM_ROLE_ARN\"),\n)\n\nwait = True\nctas_approach = False\nstorage_format = \"PARQUET\"\nwrite_compression = \"SNAPPY\"\n\ndatabase = \"clique_bait\"\ntables = [\n    \"users\",\n    \"events\",\n    \"event_identifier\",\n    \"campaign_identifier\",\n    \"page_hierarchy\",\n]\n\nathena = Athena(boto3_session=boto3_session, s3_output=os.getenv(\"ATHENA_S3_OUTPUT\"))\nathena\n</pre> boto3_session = create_session(     profile_name=\"dev\",     role_arn=os.getenv(\"ATHENA_IAM_ROLE_ARN\"), )  wait = True ctas_approach = False storage_format = \"PARQUET\" write_compression = \"SNAPPY\"  database = \"clique_bait\" tables = [     \"users\",     \"events\",     \"event_identifier\",     \"campaign_identifier\",     \"page_hierarchy\", ]  athena = Athena(boto3_session=boto3_session, s3_output=os.getenv(\"ATHENA_S3_OUTPUT\")) athena Out[2]: <pre>Athena(boto3_session=Session(region_name='us-east-1'), s3_output=s3://sql-case-studies/query_results)</pre> In\u00a0[4]: Copied! <pre>for table in tables:\n    athena.query(\n        database=database,\n        query=f\"\"\" \n                SELECT\n                    *\n                FROM\n                    {database}.{table} TABLESAMPLE BERNOULLI(50);\n              \"\"\",\n        ctas_approach=ctas_approach,\n    )\n</pre> for table in tables:     athena.query(         database=database,         query=f\"\"\"                  SELECT                     *                 FROM                     {database}.{table} TABLESAMPLE BERNOULLI(50);               \"\"\",         ctas_approach=ctas_approach,     ) Out[4]: user_id cookie_id start_date 0 2 c81e72 2020-01-18 1 3 eccbc8 2020-02-21 2 4 a87ff6 2020-02-22 3 6 167909 2020-01-25 4 7 8f14e4 2020-02-09 ... ... ... ... 891 392 863bdd 2020-02-15 892 24 f4b5e8 2020-02-10 893 191 e83169 2020-03-21 894 94 59511b 2020-03-22 895 64 87a4ba 2020-03-18 <p>896 rows \u00d7 3 columns</p> Out[4]: visit_id cookie_id page_id event_type sequence_number event_time 0 ccf365 c4ca42 1 1 1 2020-02-04 19:16:09.182 1 ccf365 c4ca42 2 1 2 2020-02-04 19:16:17.358 2 ccf365 c4ca42 6 1 3 2020-02-04 19:16:58.454 3 ccf365 c4ca42 10 1 6 2020-02-04 19:18:11.605 4 ccf365 c4ca42 10 2 7 2020-02-04 19:19:10.570 ... ... ... ... ... ... ... 16212 355a6a 87a4ba 8 2 12 2020-03-18 22:42:33.090 16213 355a6a 87a4ba 9 2 14 2020-03-18 22:43:57.155 16214 355a6a 87a4ba 11 1 16 2020-03-18 22:44:18.900 16215 355a6a 87a4ba 11 2 17 2020-03-18 22:45:12.670 16216 355a6a 87a4ba 13 3 19 2020-03-18 22:45:54.984 <p>16217 rows \u00d7 6 columns</p> Out[4]: event_type event_name 0 2 Add to Cart 1 3 Purchase 2 4 Ad Impression Out[4]: campaign_id products campaign_name start_date end_date 0 1 1-3 BOGOF - Fishing For Compliments 2020-01-01 2020-01-14 1 2 4-5 25% Off - Living The Lux Life 2020-01-15 2020-01-28 2 3 6-8 Half Off - Treat Your Shellf(ish) 2020-02-01 2020-03-31 Out[4]: page_id page_name product_category product_id 0 1 Home Page &lt;NA&gt; &lt;NA&gt; 1 2 All Products &lt;NA&gt; &lt;NA&gt; 2 3 Salmon Fish 1 3 4 Kingfish Fish 2 4 5 Tuna Fish 3 5 6 Russian Caviar Luxury 4 6 7 Black Truffle Luxury 5 7 8 Abalone Shellfish 6 8 9 Lobster Shellfish 7 9 10 Crab Shellfish 8 10 11 Oyster Shellfish 9 In\u00a0[6]: Copied! <pre>query = \"\"\" \nSELECT\n    COUNT(DISTINCT user_id) AS user_count\nFROM\n    clique_bait.users;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  SELECT     COUNT(DISTINCT user_id) AS user_count FROM     clique_bait.users; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[6]: user_count 0 500 In\u00a0[16]: Copied! <pre>query = \"\"\" \nSELECT\n    user_id,\n    COUNT(DISTINCT cookie_id) AS cookie_distinct_count,\n    COUNT(cookie_id) AS cookie_count\nFROM\n    clique_bait.users\nGROUP BY\n    user_id\nHAVING\n    COUNT(DISTINCT cookie_id) &lt;&gt; COUNT(cookie_id)\nORDER BY\n    user_id;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  SELECT     user_id,     COUNT(DISTINCT cookie_id) AS cookie_distinct_count,     COUNT(cookie_id) AS cookie_count FROM     clique_bait.users GROUP BY     user_id HAVING     COUNT(DISTINCT cookie_id) &lt;&gt; COUNT(cookie_id) ORDER BY     user_id; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[16]: user_id cookie_distinct_count cookie_count <p>Check if any unique <code>cookie_id</code> is associated with multiple <code>user_id</code>s using a self join:</p> In\u00a0[19]: Copied! <pre>query = \"\"\" \nSELECT\n    COUNT(DISTINCT user_id) AS user_count_per_cookie_id\nFROM\n    clique_bait.users\nGROUP BY\n    cookie_id\nHAVING\n    COUNT(DISTINCT user_id) &gt; 1\nORDER BY\n    user_count_per_cookie_id;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  SELECT     COUNT(DISTINCT user_id) AS user_count_per_cookie_id FROM     clique_bait.users GROUP BY     cookie_id HAVING     COUNT(DISTINCT user_id) &gt; 1 ORDER BY     user_count_per_cookie_id; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[19]: user_count_per_cookie_id <p>Now, we can calculate the average number of cookies per user:</p> In\u00a0[10]: Copied! <pre>query = \"\"\" \nWITH cookie_counts AS (\n    SELECT\n        COUNT(DISTINCT cookie_id) AS cookie_count\n    FROM\n        clique_bait.users\n    GROUP BY\n        user_id  \n)\nSELECT\n    AVG(cookie_count) AS avg_cookies_per_user\nFROM\n    cookie_counts;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  WITH cookie_counts AS (     SELECT         COUNT(DISTINCT cookie_id) AS cookie_count     FROM         clique_bait.users     GROUP BY         user_id   ) SELECT     AVG(cookie_count) AS avg_cookies_per_user FROM     cookie_counts; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[10]: avg_cookies_per_user 0 3.564 In\u00a0[21]: Copied! <pre>query = \"\"\" \nSELECT\n    MONTH(event_time) AS month,\n    COUNT(DISTINCT visit_id) AS visit_count\nFROM\n    clique_bait.events\nGROUP BY\n    MONTH(event_time)\nORDER BY\n    month;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  SELECT     MONTH(event_time) AS month,     COUNT(DISTINCT visit_id) AS visit_count FROM     clique_bait.events GROUP BY     MONTH(event_time) ORDER BY     month; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[21]: month visit_count 0 1 876 1 2 1488 2 3 916 3 4 248 4 5 36 In\u00a0[23]: Copied! <pre>query = \"\"\" \nSELECT\n    cookie_id,\n    COUNT(DISTINCT visit_id) AS visit_count_per_cookie_id\nFROM\n    clique_bait.events\nGROUP BY\n    cookie_id\nORDER BY\n    visit_count_per_cookie_id DESC;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  SELECT     cookie_id,     COUNT(DISTINCT visit_id) AS visit_count_per_cookie_id FROM     clique_bait.events GROUP BY     cookie_id ORDER BY     visit_count_per_cookie_id DESC; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[23]: cookie_id visit_count_per_cookie_id 0 d3d944 2 1 c81e72 2 2 3416a7 2 3 e36985 2 4 f457c5 2 ... ... ... 1777 af9a20 2 1778 2ecd95 2 1779 2a532c 2 1780 97cf1d 2 1781 7de6a5 2 <p>1782 rows \u00d7 2 columns</p> In\u00a0[24]: Copied! <pre>query = \"\"\" \nSELECT\n    visit_id,\n    COUNT(DISTINCT cookie_id) AS cookie_count_per_visit_id\nFROM\n    clique_bait.events\nGROUP BY\n    visit_id\nORDER BY\n    cookie_count_per_visit_id DESC;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  SELECT     visit_id,     COUNT(DISTINCT cookie_id) AS cookie_count_per_visit_id FROM     clique_bait.events GROUP BY     visit_id ORDER BY     cookie_count_per_visit_id DESC; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[24]: visit_id cookie_count_per_visit_id 0 f0e648 1 1 e6b166 1 2 b3518b 1 3 33395b 1 4 c3019e 1 ... ... ... 3559 aa5d60 1 3560 0d2c3c 1 3561 ae568e 1 3562 ed9cb9 1 3563 ab2955 1 <p>3564 rows \u00d7 2 columns</p> <p>Cookie ID is one-to-many with Visit ID.</p> <p>To calculate the number of events for each event type, we can group the events by <code>event_type</code> and count the number of <code>visit_id</code>s without <code>DISTINCT</code>. This is because if there are multiple events of the same type within a visit, we want to count each event:</p> In\u00a0[37]: Copied! <pre>query = \"\"\" \nSELECT\n    event_id.event_name AS event_name,\n    events.event_type AS event_type,\n    COUNT(events.visit_id) AS visit_count_per_event_type\nFROM\n    clique_bait.events AS events LEFT JOIN clique_bait.event_identifier AS event_id ON events.event_type = event_id.event_type\nGROUP BY\n    event_id.event_name, events.event_type\nORDER BY\n    visit_count_per_event_type DESC;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  SELECT     event_id.event_name AS event_name,     events.event_type AS event_type,     COUNT(events.visit_id) AS visit_count_per_event_type FROM     clique_bait.events AS events LEFT JOIN clique_bait.event_identifier AS event_id ON events.event_type = event_id.event_type GROUP BY     event_id.event_name, events.event_type ORDER BY     visit_count_per_event_type DESC; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[37]: event_name event_type visit_count_per_event_type 0 Page View 1 20928 1 Add to Cart 2 8451 2 Purchase 3 1777 3 Ad Impression 4 876 4 Ad Click 5 702 In\u00a0[68]: Copied! <pre>query = \"\"\" \nSELECT\n    visit_id,\n    MAX(CASE WHEN event_type = 3 THEN 1.0 ELSE 0.0 END) AS has_purchase_event\nFROM \n    clique_bait.events\nGROUP BY \n    visit_id;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  SELECT     visit_id,     MAX(CASE WHEN event_type = 3 THEN 1.0 ELSE 0.0 END) AS has_purchase_event FROM      clique_bait.events GROUP BY      visit_id; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[68]: visit_id has_purchase_event 0 e0ce49 1.0 1 a6c424 0.0 2 5ef346 0.0 3 d69e73 0.0 4 378a75 0.0 ... ... ... 3559 f94f32 0.0 3560 672c30 1.0 3561 8e6c42 0.0 3562 8fb57b 0.0 3563 39fa1a 0.0 <p>3564 rows \u00d7 2 columns</p> <p>To compute the percentage of unique visits with a purchase event, we sum the purchase boolean and divide by the total number of unique visits:</p> In\u00a0[71]: Copied! <pre>query = \"\"\" \nWITH visit_purchase AS (\n    SELECT\n        visit_id,\n        MAX(CASE WHEN event_type = 3 THEN 1.0 ELSE 0.0 END) AS has_purchase_event\n    FROM \n        clique_bait.events\n    GROUP BY \n        visit_id\n)\nSELECT\n    ROUND((SUM(has_purchase_event) / COUNT(*)) * 100, 4) AS purchase_pct\nFROM\n    visit_purchase;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  WITH visit_purchase AS (     SELECT         visit_id,         MAX(CASE WHEN event_type = 3 THEN 1.0 ELSE 0.0 END) AS has_purchase_event     FROM          clique_bait.events     GROUP BY          visit_id ) SELECT     ROUND((SUM(has_purchase_event) / COUNT(*)) * 100, 4) AS purchase_pct FROM     visit_purchase; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[71]: purchase_pct 0 49.8597 In\u00a0[74]: Copied! <pre>query = \"\"\" \nSELECT\n    visit_id,\n    MAX(CASE WHEN event_type = 1 AND page_id = 12 THEN 1.0 ELSE 0.0 END) AS has_checkout_view\n    MAX(CASE WHEN event_type = 3 THEN 1.0 ELSE 0.0 END) AS has_purchase\nFROM\n    clique_bait.events\nGROUP BY\n    visit_id;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  SELECT     visit_id,     MAX(CASE WHEN event_type = 1 AND page_id = 12 THEN 1.0 ELSE 0.0 END) AS has_checkout_view     MAX(CASE WHEN event_type = 3 THEN 1.0 ELSE 0.0 END) AS has_purchase FROM     clique_bait.events GROUP BY     visit_id; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[74]: visit_id has_page_view has_purchase 0 e0ce49 1.0 1.0 1 a6c424 0.0 0.0 2 5ef346 0.0 0.0 3 d69e73 1.0 0.0 4 378a75 0.0 0.0 ... ... ... ... 3559 f94f32 0.0 0.0 3560 672c30 1.0 1.0 3561 8e6c42 0.0 0.0 3562 8fb57b 0.0 0.0 3563 39fa1a 0.0 0.0 <p>3564 rows \u00d7 3 columns</p> <p>We also need to add a where filter to exclude visits that do not have checkout page views from the denominator:</p> In\u00a0[81]: Copied! <pre>query = \"\"\" \nWITH visit_purchase_page_view AS (\n    SELECT\n        visit_id,\n        MAX(CASE WHEN event_type = 1 AND page_id = 12 THEN 1.0 ELSE 0.0 END) AS has_checkout_view,\n        MAX(CASE WHEN event_type = 3 THEN 1.0 ELSE 0.0 END) AS has_purchase\n    FROM\n        clique_bait.events\n    GROUP BY\n        visit_id\n)\nSELECT\n    ROUND((SUM(CASE WHEN has_checkout_view = 1.0 AND has_purchase = 0.0 THEN 1.0 ELSE 0.0 END) / COUNT(*)) * 100, 4) AS checkout_view_no_purchase_pct\nFROM\n    visit_purchase_page_view\nWHERE\n    has_checkout_view = 1.0;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  WITH visit_purchase_page_view AS (     SELECT         visit_id,         MAX(CASE WHEN event_type = 1 AND page_id = 12 THEN 1.0 ELSE 0.0 END) AS has_checkout_view,         MAX(CASE WHEN event_type = 3 THEN 1.0 ELSE 0.0 END) AS has_purchase     FROM         clique_bait.events     GROUP BY         visit_id ) SELECT     ROUND((SUM(CASE WHEN has_checkout_view = 1.0 AND has_purchase = 0.0 THEN 1.0 ELSE 0.0 END) / COUNT(*)) * 100, 4) AS checkout_view_no_purchase_pct FROM     visit_purchase_page_view WHERE     has_checkout_view = 1.0; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[81]: checkout_view_no_purchase_pct 0 15.5017 In\u00a0[105]: Copied! <pre>query = \"\"\" \nSELECT\n    page_id,\n    COUNT(DISTINCT visit_id) AS visit_distinct_count,\n    COUNT(visit_id) AS visit_count\nFROM\n    clique_bait.events\nGROUP BY\n    page_id\nHAVING\n    COUNT(DISTINCT visit_id) &lt;&gt; COUNT(visit_id)\nORDER BY\n    page_id;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  SELECT     page_id,     COUNT(DISTINCT visit_id) AS visit_distinct_count,     COUNT(visit_id) AS visit_count FROM     clique_bait.events GROUP BY     page_id HAVING     COUNT(DISTINCT visit_id) &lt;&gt; COUNT(visit_id) ORDER BY     page_id; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[105]: page_id visit_distinct_count visit_count 0 2 3174 4752 1 3 1559 2497 2 4 1559 2479 3 5 1515 2446 4 6 1563 2509 5 7 1469 2393 6 8 1525 2457 7 9 1547 2515 8 10 1564 2513 9 11 1568 2511 <p>The above are all <code>page_id</code>s that have events that occurred within the same visit, hence the unique count and raw count of <code>visit_id</code>s are different.</p> <p>However, if <code>event_type</code> is considered, then the distinct count of <code>visit_id</code> will be the same as the raw count of <code>visit_id</code>:</p> In\u00a0[107]: Copied! <pre>query = \"\"\" \nSELECT\n    page_id,\n    event_type,\n    COUNT(DISTINCT visit_id) AS visit_distinct_count,\n    COUNT(visit_id) AS visit_count\nFROM\n    clique_bait.events\nGROUP BY\n    page_id, event_type\nHAVING\n    COUNT(DISTINCT visit_id) &lt;&gt; COUNT(visit_id)\nORDER BY\n    page_id;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  SELECT     page_id,     event_type,     COUNT(DISTINCT visit_id) AS visit_distinct_count,     COUNT(visit_id) AS visit_count FROM     clique_bait.events GROUP BY     page_id, event_type HAVING     COUNT(DISTINCT visit_id) &lt;&gt; COUNT(visit_id) ORDER BY     page_id; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[107]: page_id event_type visit_distinct_count visit_count <p>This means that <code>event_type</code>, <code>page_id</code>, and <code>visit_id</code> can be considered as a composite key.</p> In\u00a0[111]: Copied! <pre>query = \"\"\" \nSELECT\n    page_id,\n    visit_id,\n    event_type,\n    COUNT(*) AS count\nFROM\n    clique_bait.events\nGROUP BY\n    page_id, visit_id, event_type\nORDER BY\n    count DESC;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  SELECT     page_id,     visit_id,     event_type,     COUNT(*) AS count FROM     clique_bait.events GROUP BY     page_id, visit_id, event_type ORDER BY     count DESC; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[111]: page_id visit_id event_type count 0 4 b7daee 1 1 1 3 83bfb0 2 1 2 6 b7daee 1 1 3 6 83bfb0 1 1 4 9 b7daee 2 1 ... ... ... ... ... 32729 12 82dac8 1 1 32730 2 e73a2f 1 1 32731 2 aa1346 4 1 32732 2 aa1346 5 1 32733 3 aa1346 2 1 <p>32734 rows \u00d7 4 columns</p> In\u00a0[112]: Copied! <pre>query = \"\"\" \nSELECT\n    visit_id,\n    page_id,\n    event_type,\n    COUNT(*) AS count\nFROM\n    clique_bait.events\nGROUP BY\n    visit_id, page_id, event_type\nORDER BY\n    count DESC;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  SELECT     visit_id,     page_id,     event_type,     COUNT(*) AS count FROM     clique_bait.events GROUP BY     visit_id, page_id, event_type ORDER BY     count DESC; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[112]: visit_id page_id event_type count 0 b7daee 2 1 1 1 f3897a 7 1 1 2 b7daee 4 1 1 3 f3897a 12 1 1 4 b7daee 10 1 1 ... ... ... ... ... 32729 c09cfd 4 2 1 32730 c09cfd 8 2 1 32731 355a6a 6 1 1 32732 355a6a 6 2 1 32733 355a6a 10 1 1 <p>32734 rows \u00d7 4 columns</p> <p>When considering only <code>visit_id</code> and <code>page_id</code>, the relationship is many-to-many since a visit can include multiple page views, and a page can be viewed in multiple visits. However, adding <code>event_type</code> to the combination enforces uniqueness, making the relationship one-to-one within that composite key.</p> <p>In the query below, because of the filter for page view events <code>event_type = 1</code>, we can either use <code>COUNT(DISTINCT visit_id)</code> or <code>COUNT(visitor_id)</code> after group by <code>page_id</code> to calculate the number of views for each page. The result will be the same because controlling for <code>event_type</code> makes the relationship between <code>page_id</code> and <code>visit_id</code> one-to-one.</p> <p>The top three pages by number of views are:</p> In\u00a0[3]: Copied! <pre>query = \"\"\" \nSELECT\n    events.page_id AS page_id,\n    pa.page_name AS page_name,\n    COUNT(DISTINCT visit_id) AS num_views\nFROM\n    clique_bait.events AS events LEFT JOIN clique_bait.page_hierarchy AS pa ON events.page_id = pa.page_id\nWHERE\n    events.event_type = 1\nGROUP BY\n    events.page_id, pa.page_name\nORDER BY\n    num_views DESC\nLIMIT \n    3;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  SELECT     events.page_id AS page_id,     pa.page_name AS page_name,     COUNT(DISTINCT visit_id) AS num_views FROM     clique_bait.events AS events LEFT JOIN clique_bait.page_hierarchy AS pa ON events.page_id = pa.page_id WHERE     events.event_type = 1 GROUP BY     events.page_id, pa.page_name ORDER BY     num_views DESC LIMIT      3; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[3]: page_id page_name num_views 0 2 All Products 3174 1 12 Checkout 2103 2 1 Home Page 1782 In\u00a0[116]: Copied! <pre>query = \"\"\" \nSELECT\n    pa.product_category AS product_cat,\n    SUM(CASE WHEN events.event_type = 1 THEN 1 ELSE 0 END) AS page_views,\n    SUM(CASE WHEN events.event_type = 2 THEN 1 ELSE 0 END) AS cart_adds\nFROM \n    clique_bait.events AS events LEFT JOIN clique_bait.page_hierarchy AS pa ON events.page_id = pa.page_id\nWHERE\n    pa.product_category IS NOT NULL\nGROUP BY\n    pa.product_category\nORDER BY\n    pa.product_category;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  SELECT     pa.product_category AS product_cat,     SUM(CASE WHEN events.event_type = 1 THEN 1 ELSE 0 END) AS page_views,     SUM(CASE WHEN events.event_type = 2 THEN 1 ELSE 0 END) AS cart_adds FROM      clique_bait.events AS events LEFT JOIN clique_bait.page_hierarchy AS pa ON events.page_id = pa.page_id WHERE     pa.product_category IS NOT NULL GROUP BY     pa.product_category ORDER BY     pa.product_category; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[116]: product_cat page_views cart_adds 0 Fish 4633 2789 1 Luxury 3032 1870 2 Shellfish 6204 3792 In\u00a0[134]: Copied! <pre>query = \"\"\" \nSELECT\n    event_type,\n    COUNT(DISTINCT visit_id) AS visit_distinct_count,\n    COUNT(visit_id) AS visit_count\nFROM\n    clique_bait.events\nGROUP BY\n    event_type;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  SELECT     event_type,     COUNT(DISTINCT visit_id) AS visit_distinct_count,     COUNT(visit_id) AS visit_count FROM     clique_bait.events GROUP BY     event_type; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[134]: event_type visit_distinct_count visit_count 0 1 3564 20928 1 2 2510 8451 2 4 876 876 3 5 702 702 4 3 1777 1777 <p>Controlling for <code>event_type = 3</code> (i.e., a purchase event), <code>event_type</code> and <code>visit_id</code> are one-to-one, we can confidently and simply select <code>visit_id</code> in the <code>purchase_visit_ids</code> CTE without needing <code>DISTINCT</code>. This ensures that we efficiently identify all unique visits that resulted in a purchase.</p> <p>In the main query, we use <code>SUM(CASE WHEN events.event_type = 2 THEN 1 ELSE 0 END)</code> to count the number of \"add to cart\" events (<code>event_type = 2</code>) for each product. This aggregation is specifically focused on visits that included both \"add to cart\" and purchase actions.</p> <p>The <code>WHERE EXISTS</code> clause then ensures that we only include \"add to cart\" events in the final count if they are associated with a visit that also resulted in a purchase. This filtering allows us to analyze the relationship between adding items to the cart and completing a purchase, giving insight into user behavior during successful purchase journeys.</p> In\u00a0[143]: Copied! <pre>query = \"\"\" \nWITH purchase_visit_ids AS (\n    SELECT\n        visit_id\n    FROM\n        clique_bait.events\n    WHERE \n        event_type = 3\n)\nSELECT\n    pa.product_id AS product_id,\n    pa.page_name AS product_name,\n    SUM(CASE WHEN events.event_type = 2 THEN 1 ELSE 0 END) AS add_count_followed_by_purchase\nFROM \n    clique_bait.events AS events LEFT JOIN clique_bait.page_hierarchy AS pa ON events.page_id = pa.page_id\nWHERE \n    EXISTS (\n        SELECT \n            NULL\n        FROM \n            purchase_visit_ids AS pvids\n        WHERE \n            events.visit_id = pvids.visit_id\n    )\nAND \n    pa.product_id IS NOT NULL\nGROUP BY \n    pa.product_id, \n    pa.page_name\nORDER BY \n    SUM(CASE WHEN events.event_type = 2 THEN 1 ELSE 0 END) DESC;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  WITH purchase_visit_ids AS (     SELECT         visit_id     FROM         clique_bait.events     WHERE          event_type = 3 ) SELECT     pa.product_id AS product_id,     pa.page_name AS product_name,     SUM(CASE WHEN events.event_type = 2 THEN 1 ELSE 0 END) AS add_count_followed_by_purchase FROM      clique_bait.events AS events LEFT JOIN clique_bait.page_hierarchy AS pa ON events.page_id = pa.page_id WHERE      EXISTS (         SELECT              NULL         FROM              purchase_visit_ids AS pvids         WHERE              events.visit_id = pvids.visit_id     ) AND      pa.product_id IS NOT NULL GROUP BY      pa.product_id,      pa.page_name ORDER BY      SUM(CASE WHEN events.event_type = 2 THEN 1 ELSE 0 END) DESC; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[143]: product_id product_name add_count_followed_by_purchase 0 7 Lobster 754 1 9 Oyster 726 2 8 Crab 719 3 1 Salmon 711 4 5 Black Truffle 707 5 2 Kingfish 707 6 6 Abalone 699 7 3 Tuna 697 8 4 Russian Caviar 697 In\u00a0[68]: Copied! <pre>query = \"\"\" \nSELECT\n    events.visit_id AS visit_id,\n    pa.product_id AS product_id,\n    pa.page_name AS product_name,\n    pa.product_category AS product_cat,\n    SUM(CASE WHEN event_type = 1 THEN 1 ELSE 0 END) AS page_view,\n    SUM(CASE WHEN event_type = 2 THEN 1 ELSE 0 END) AS cart_add\nFROM\n    clique_bait.events AS events LEFT JOIN clique_bait.page_hierarchy AS pa ON events.page_id = pa.page_id\nWHERE\n    pa.product_id IS NOT NULL\nGROUP BY\n    events.visit_id, pa.product_id, pa.page_name, pa.product_category;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  SELECT     events.visit_id AS visit_id,     pa.product_id AS product_id,     pa.page_name AS product_name,     pa.product_category AS product_cat,     SUM(CASE WHEN event_type = 1 THEN 1 ELSE 0 END) AS page_view,     SUM(CASE WHEN event_type = 2 THEN 1 ELSE 0 END) AS cart_add FROM     clique_bait.events AS events LEFT JOIN clique_bait.page_hierarchy AS pa ON events.page_id = pa.page_id WHERE     pa.product_id IS NOT NULL GROUP BY     events.visit_id, pa.product_id, pa.page_name, pa.product_category; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[68]: visit_id product_id product_name product_cat page_view cart_add 0 12e324 1 Salmon Fish 1 0 1 b920d5 7 Lobster Shellfish 1 1 2 7ba9e0 4 Russian Caviar Luxury 1 1 3 7ba9e0 6 Abalone Shellfish 1 1 4 7ba9e0 7 Lobster Shellfish 1 0 ... ... ... ... ... ... ... 13864 c09cfd 7 Lobster Shellfish 1 1 13865 355a6a 4 Russian Caviar Luxury 1 1 13866 355a6a 5 Black Truffle Luxury 1 1 13867 355a6a 6 Abalone Shellfish 1 1 13868 355a6a 8 Crab Shellfish 1 0 <p>13869 rows \u00d7 6 columns</p> In\u00a0[31]: Copied! <pre>query = \"\"\" \nSELECT\n    visit_id\nFROM\n    clique_bait.events\nWHERE \n    event_type = 3;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  SELECT     visit_id FROM     clique_bait.events WHERE      event_type = 3; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[31]: visit_id 0 ccf365 1 9a2f24 2 f61ed7 3 e0ce49 4 8479c1 ... ... 1772 b40bf5 1773 96b841 1774 672c30 1775 c09cfd 1776 355a6a <p>1777 rows \u00d7 1 columns</p> In\u00a0[72]: Copied! <pre>query = \"\"\" \nWITH page_view_cart_add_cte AS (\n    SELECT\n        events.visit_id AS visit_id,\n        pa.product_id AS product_id,\n        pa.page_name AS product_name,\n        pa.product_category AS product_cat,\n        SUM(CASE WHEN event_type = 1 THEN 1 ELSE 0 END) AS page_view,\n        SUM(CASE WHEN event_type = 2 THEN 1 ELSE 0 END) AS cart_add\n    FROM\n        clique_bait.events AS events LEFT JOIN clique_bait.page_hierarchy AS pa ON events.page_id = pa.page_id\n    WHERE\n        pa.product_id IS NOT NULL\n    GROUP BY\n        events.visit_id, \n        pa.product_id, \n        pa.page_name, \n        pa.product_category\n),\npurchase_cte AS (\n    SELECT\n        visit_id\n    FROM\n        clique_bait.events\n    WHERE \n        event_type = 3\n)\nSELECT  \n    page_view_cart_add_cte.visit_id AS visit_id_page_view_cart_add_cte,\n    purchase_cte.visit_id AS visit_id_purchase_cte,\n    page_view_cart_add_cte.product_id AS product_id,\n    page_view_cart_add_cte.product_name AS product_name,\n    page_view_cart_add_cte.product_cat AS product_cat,\n    page_view_cart_add_cte.page_view AS page_view,\n    page_view_cart_add_cte.cart_add AS cart_add,\n    CASE WHEN purchase_cte.visit_id IS NULL THEN 1 ELSE 0 END AS abandoned_purchase\nFROM\n    page_view_cart_add_cte LEFT JOIN purchase_cte ON page_view_cart_add_cte.visit_id = purchase_cte.visit_id;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  WITH page_view_cart_add_cte AS (     SELECT         events.visit_id AS visit_id,         pa.product_id AS product_id,         pa.page_name AS product_name,         pa.product_category AS product_cat,         SUM(CASE WHEN event_type = 1 THEN 1 ELSE 0 END) AS page_view,         SUM(CASE WHEN event_type = 2 THEN 1 ELSE 0 END) AS cart_add     FROM         clique_bait.events AS events LEFT JOIN clique_bait.page_hierarchy AS pa ON events.page_id = pa.page_id     WHERE         pa.product_id IS NOT NULL     GROUP BY         events.visit_id,          pa.product_id,          pa.page_name,          pa.product_category ), purchase_cte AS (     SELECT         visit_id     FROM         clique_bait.events     WHERE          event_type = 3 ) SELECT       page_view_cart_add_cte.visit_id AS visit_id_page_view_cart_add_cte,     purchase_cte.visit_id AS visit_id_purchase_cte,     page_view_cart_add_cte.product_id AS product_id,     page_view_cart_add_cte.product_name AS product_name,     page_view_cart_add_cte.product_cat AS product_cat,     page_view_cart_add_cte.page_view AS page_view,     page_view_cart_add_cte.cart_add AS cart_add,     CASE WHEN purchase_cte.visit_id IS NULL THEN 1 ELSE 0 END AS abandoned_purchase FROM     page_view_cart_add_cte LEFT JOIN purchase_cte ON page_view_cart_add_cte.visit_id = purchase_cte.visit_id; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[72]: visit_id_page_view_cart_add_cte visit_id_purchase_cte product_id product_name product_cat page_view cart_add abandoned_purchase 0 ccf365 ccf365 9 Oyster Shellfish 1 1 0 1 d58cbd &lt;NA&gt; 3 Tuna Fish 1 1 1 2 9a2f24 9a2f24 9 Oyster Shellfish 1 0 0 3 7caba5 &lt;NA&gt; 2 Kingfish Fish 1 0 1 4 7caba5 &lt;NA&gt; 3 Tuna Fish 1 1 1 ... ... ... ... ... ... ... ... ... 13864 b40bf5 b40bf5 2 Kingfish Fish 1 1 0 13865 b40bf5 b40bf5 7 Lobster Shellfish 1 1 0 13866 96b841 96b841 2 Kingfish Fish 1 1 0 13867 96b841 96b841 3 Tuna Fish 1 1 0 13868 c09cfd c09cfd 8 Crab Shellfish 1 1 0 <p>13869 rows \u00d7 8 columns</p> In\u00a0[80]: Copied! <pre>query = \"\"\" \nCREATE OR REPLACE VIEW product_funnel AS \nWITH page_view_cart_add_cte AS (\n    SELECT\n        events.visit_id AS visit_id,\n        pa.product_id AS product_id,\n        pa.page_name AS product_name,\n        pa.product_category AS product_cat,\n        SUM(CASE WHEN event_type = 1 THEN 1 ELSE 0 END) AS page_view,\n        SUM(CASE WHEN event_type = 2 THEN 1 ELSE 0 END) AS cart_add\n    FROM\n        clique_bait.events AS events LEFT JOIN clique_bait.page_hierarchy AS pa ON events.page_id = pa.page_id\n    WHERE\n        pa.product_id IS NOT NULL\n    GROUP BY\n        events.visit_id, \n        pa.product_id, \n        pa.page_name, \n        pa.product_category\n),\npurchase_cte AS (\n    SELECT\n        visit_id\n    FROM\n        clique_bait.events\n    WHERE \n        event_type = 3\n),\ncombined_cte AS (\n    SELECT  \n        page_view_cart_add_cte.visit_id AS visit_id_page_view_cart_add_cte,\n        purchase_cte.visit_id AS visit_id_purchase_cte,\n        page_view_cart_add_cte.product_id AS product_id,\n        page_view_cart_add_cte.product_name AS product_name,\n        page_view_cart_add_cte.product_cat AS product_cat,\n        page_view_cart_add_cte.page_view AS page_view,\n        page_view_cart_add_cte.cart_add AS cart_add,\n        CASE WHEN purchase_cte.visit_id IS NULL THEN 1 ELSE 0 END AS abandoned_purchase\n    FROM\n        page_view_cart_add_cte LEFT JOIN purchase_cte ON page_view_cart_add_cte.visit_id = purchase_cte.visit_id\n)\nSELECT\n    product_id,\n    product_name,\n    product_cat,\n    SUM(page_view) AS page_views,\n    SUM(cart_add) AS cart_adds,\n    SUM(CASE WHEN cart_add = 1 AND abandoned_purchase = 1 THEN 1 ELSE 0 END) AS abandoned,\n    SUM(CASE WHEN cart_add = 1 AND abandoned_purchase = 0 THEN 1 ELSE 0 END) AS purchases\nFROM\n    combined_cte\nGROUP BY\n    product_id,\n    product_cat,\n    product_name\nORDER BY\n    product_id;\n\"\"\"\n\nathena.create_view(database=database, query=query, wait=wait)\n</pre> query = \"\"\"  CREATE OR REPLACE VIEW product_funnel AS  WITH page_view_cart_add_cte AS (     SELECT         events.visit_id AS visit_id,         pa.product_id AS product_id,         pa.page_name AS product_name,         pa.product_category AS product_cat,         SUM(CASE WHEN event_type = 1 THEN 1 ELSE 0 END) AS page_view,         SUM(CASE WHEN event_type = 2 THEN 1 ELSE 0 END) AS cart_add     FROM         clique_bait.events AS events LEFT JOIN clique_bait.page_hierarchy AS pa ON events.page_id = pa.page_id     WHERE         pa.product_id IS NOT NULL     GROUP BY         events.visit_id,          pa.product_id,          pa.page_name,          pa.product_category ), purchase_cte AS (     SELECT         visit_id     FROM         clique_bait.events     WHERE          event_type = 3 ), combined_cte AS (     SELECT           page_view_cart_add_cte.visit_id AS visit_id_page_view_cart_add_cte,         purchase_cte.visit_id AS visit_id_purchase_cte,         page_view_cart_add_cte.product_id AS product_id,         page_view_cart_add_cte.product_name AS product_name,         page_view_cart_add_cte.product_cat AS product_cat,         page_view_cart_add_cte.page_view AS page_view,         page_view_cart_add_cte.cart_add AS cart_add,         CASE WHEN purchase_cte.visit_id IS NULL THEN 1 ELSE 0 END AS abandoned_purchase     FROM         page_view_cart_add_cte LEFT JOIN purchase_cte ON page_view_cart_add_cte.visit_id = purchase_cte.visit_id ) SELECT     product_id,     product_name,     product_cat,     SUM(page_view) AS page_views,     SUM(cart_add) AS cart_adds,     SUM(CASE WHEN cart_add = 1 AND abandoned_purchase = 1 THEN 1 ELSE 0 END) AS abandoned,     SUM(CASE WHEN cart_add = 1 AND abandoned_purchase = 0 THEN 1 ELSE 0 END) AS purchases FROM     combined_cte GROUP BY     product_id,     product_cat,     product_name ORDER BY     product_id; \"\"\"  athena.create_view(database=database, query=query, wait=wait) <pre>Query executed successfully\n</pre> In\u00a0[81]: Copied! <pre>query = \"\"\" \nSELECT\n    *\nFROM\n    clique_bait.product_funnel;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  SELECT     * FROM     clique_bait.product_funnel; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[81]: product_id product_name product_cat page_views cart_adds abandoned purchases 0 1 Salmon Fish 1559 938 227 711 1 2 Kingfish Fish 1559 920 213 707 2 3 Tuna Fish 1515 931 234 697 3 4 Russian Caviar Luxury 1563 946 249 697 4 5 Black Truffle Luxury 1469 924 217 707 5 6 Abalone Shellfish 1525 932 233 699 6 7 Lobster Shellfish 1547 968 214 754 7 8 Crab Shellfish 1564 949 230 719 8 9 Oyster Shellfish 1568 943 217 726 In\u00a0[84]: Copied! <pre>query = \"\"\" \nCREATE OR REPLACE VIEW product_cat_funnel AS \nSELECT\n    product_cat,\n    SUM(page_views) AS page_views,\n    SUM(cart_adds) AS cart_adds,\n    SUM(abandoned) AS abandoned,\n    SUM(purchases) AS purchases\nFROM\n    clique_bait.product_funnel\nGROUP BY\n    product_cat\nORDER BY\n    product_cat;\n\"\"\"\n\nathena.create_view(database=database, query=query, wait=wait)\n</pre> query = \"\"\"  CREATE OR REPLACE VIEW product_cat_funnel AS  SELECT     product_cat,     SUM(page_views) AS page_views,     SUM(cart_adds) AS cart_adds,     SUM(abandoned) AS abandoned,     SUM(purchases) AS purchases FROM     clique_bait.product_funnel GROUP BY     product_cat ORDER BY     product_cat; \"\"\"  athena.create_view(database=database, query=query, wait=wait) <pre>Query executed successfully\n</pre> In\u00a0[86]: Copied! <pre>query = \"\"\" \nSELECT\n    *\nFROM\n    clique_bait.product_cat_funnel;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  SELECT     * FROM     clique_bait.product_cat_funnel; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[86]: product_cat page_views cart_adds abandoned purchases 0 Fish 4633 2789 674 2115 1 Luxury 3032 1870 466 1404 2 Shellfish 6204 3792 894 2898 In\u00a0[92]: Copied! <pre>query = \"\"\" \nWITH ranked_stats AS (\n    SELECT\n        product_id,\n        product_name,\n        page_views,\n        cart_adds,\n        purchases,\n        RANK() OVER (ORDER BY page_views DESC NULLS LAST) AS page_views_rank,\n        RANK() OVER (ORDER BY cart_adds DESC NULLS LAST) AS cart_adds_rank,\n        RANK() OVER (ORDER BY purchases DESC NULLS LAST) AS purchases_rank\n    FROM\n        clique_bait.product_funnel\n)\nSELECT\n    product_id,\n    product_name,\n    page_views,\n    cart_adds,\n    purchases,\n    page_views_rank,\n    cart_adds_rank,\n    purchases_rank\nFROM\n    ranked_stats\nWHERE\n    page_views_rank = 1 \n    OR cart_adds_rank = 1\n    OR purchases_rank = 1;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  WITH ranked_stats AS (     SELECT         product_id,         product_name,         page_views,         cart_adds,         purchases,         RANK() OVER (ORDER BY page_views DESC NULLS LAST) AS page_views_rank,         RANK() OVER (ORDER BY cart_adds DESC NULLS LAST) AS cart_adds_rank,         RANK() OVER (ORDER BY purchases DESC NULLS LAST) AS purchases_rank     FROM         clique_bait.product_funnel ) SELECT     product_id,     product_name,     page_views,     cart_adds,     purchases,     page_views_rank,     cart_adds_rank,     purchases_rank FROM     ranked_stats WHERE     page_views_rank = 1      OR cart_adds_rank = 1     OR purchases_rank = 1; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[92]: product_id product_name page_views cart_adds purchases page_views_rank cart_adds_rank purchases_rank 0 7 Lobster 1547 968 754 6 1 1 1 9 Oyster 1568 943 726 1 4 2 In\u00a0[98]: Copied! <pre>query = \"\"\" \nSELECT\n    product_name,\n    ROUND(TRY_CAST(abandoned AS DOUBLE) / TRY_CAST(cart_adds AS DOUBLE), 4) AS fallout_rate\nFROM\n    clique_bait.product_funnel\nORDER BY\n    fallout_rate DESC;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  SELECT     product_name,     ROUND(TRY_CAST(abandoned AS DOUBLE) / TRY_CAST(cart_adds AS DOUBLE), 4) AS fallout_rate FROM     clique_bait.product_funnel ORDER BY     fallout_rate DESC; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[98]: product_name fallout_rate 0 Russian Caviar 0.2632 1 Tuna 0.2513 2 Abalone 0.2500 3 Crab 0.2424 4 Salmon 0.2420 5 Black Truffle 0.2348 6 Kingfish 0.2315 7 Oyster 0.2301 8 Lobster 0.2211 In\u00a0[99]: Copied! <pre>query = \"\"\" \nSELECT\n    product_name,\n    ROUND(TRY_CAST(purchases AS DOUBLE) / TRY_CAST(page_views AS DOUBLE), 4) AS view_to_purchase_rate\nFROM\n    clique_bait.product_funnel\nORDER BY\n    view_to_purchase_rate DESC;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  SELECT     product_name,     ROUND(TRY_CAST(purchases AS DOUBLE) / TRY_CAST(page_views AS DOUBLE), 4) AS view_to_purchase_rate FROM     clique_bait.product_funnel ORDER BY     view_to_purchase_rate DESC; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[99]: product_name view_to_purchase_rate 0 Lobster 0.4874 1 Black Truffle 0.4813 2 Oyster 0.4630 3 Tuna 0.4601 4 Crab 0.4597 5 Abalone 0.4584 6 Salmon 0.4561 7 Kingfish 0.4535 8 Russian Caviar 0.4459 In\u00a0[106]: Copied! <pre>query = \"\"\" \nSELECT\n    ROUND(TRY_CAST(SUM(cart_adds) AS DOUBLE) / TRY_CAST(SUM(page_views) AS DOUBLE), 4) AS view_to_cart_add_rate\nFROM\n    clique_bait.product_funnel;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  SELECT     ROUND(TRY_CAST(SUM(cart_adds) AS DOUBLE) / TRY_CAST(SUM(page_views) AS DOUBLE), 4) AS view_to_cart_add_rate FROM     clique_bait.product_funnel; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[106]: view_to_cart_add_rate 0 0.6093 In\u00a0[4]: Copied! <pre>query = \"\"\" \nSELECT\n    ROUND(TRY_CAST(SUM(purchases) AS DOUBLE) / TRY_CAST(SUM(cart_adds) AS DOUBLE), 4) AS cart_add_to_purchase_rate\nFROM\n    clique_bait.product_funnel;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  SELECT     ROUND(TRY_CAST(SUM(purchases) AS DOUBLE) / TRY_CAST(SUM(cart_adds) AS DOUBLE), 4) AS cart_add_to_purchase_rate FROM     clique_bait.product_funnel; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[4]: cart_add_to_purchase_rate 0 0.7593 In\u00a0[3]: Copied! <pre>query = \"\"\" \nSELECT \n    DISTINCT sequence_number\nFROM\n    clique_bait.events\nORDER BY\n    sequence_number;\n\"\"\"\n\ndistinct_seq_nums = athena.query(\n    database=database, query=query, ctas_approach=ctas_approach\n)[\"sequence_number\"].values\n\ndistinct_seq_nums\n</pre> query = \"\"\"  SELECT      DISTINCT sequence_number FROM     clique_bait.events ORDER BY     sequence_number; \"\"\"  distinct_seq_nums = athena.query(     database=database, query=query, ctas_approach=ctas_approach )[\"sequence_number\"].values  distinct_seq_nums Out[3]: <pre>&lt;IntegerArray&gt;\n[ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,\n 20, 21, 22, 23]\nLength: 23, dtype: Int32</pre> <p>Next, create <code>MAX(CASE WHEN ...)</code> statements for the entire range of <code>sequence_number</code> values; these are used to reshape the long format of products to a wide format where each product is in its own column.</p> <p>In addition, we also generate a <code>products_cols</code> string containing comma separated column names for the products. This will be used in the final query to select the columns dynamically.</p> In\u00a0[83]: Copied! <pre>select_clauses = []\nfor seq_num in distinct_seq_nums:\n    select_clauses.append(\n        f\"MAX(CASE WHEN seq_num = {seq_num} THEN product_name END) AS product_added_{seq_num}\"\n    )\n\nselect_clause = \",\\n    \".join(select_clauses)\n\nprint(select_clause)\n</pre> select_clauses = [] for seq_num in distinct_seq_nums:     select_clauses.append(         f\"MAX(CASE WHEN seq_num = {seq_num} THEN product_name END) AS product_added_{seq_num}\"     )  select_clause = \",\\n    \".join(select_clauses)  print(select_clause) <pre>MAX(CASE WHEN seq_num = 1 THEN product_name END) AS product_added_1,\n    MAX(CASE WHEN seq_num = 2 THEN product_name END) AS product_added_2,\n    MAX(CASE WHEN seq_num = 3 THEN product_name END) AS product_added_3,\n    MAX(CASE WHEN seq_num = 4 THEN product_name END) AS product_added_4,\n    MAX(CASE WHEN seq_num = 5 THEN product_name END) AS product_added_5,\n    MAX(CASE WHEN seq_num = 6 THEN product_name END) AS product_added_6,\n    MAX(CASE WHEN seq_num = 7 THEN product_name END) AS product_added_7,\n    MAX(CASE WHEN seq_num = 8 THEN product_name END) AS product_added_8,\n    MAX(CASE WHEN seq_num = 9 THEN product_name END) AS product_added_9,\n    MAX(CASE WHEN seq_num = 10 THEN product_name END) AS product_added_10,\n    MAX(CASE WHEN seq_num = 11 THEN product_name END) AS product_added_11,\n    MAX(CASE WHEN seq_num = 12 THEN product_name END) AS product_added_12,\n    MAX(CASE WHEN seq_num = 13 THEN product_name END) AS product_added_13,\n    MAX(CASE WHEN seq_num = 14 THEN product_name END) AS product_added_14,\n    MAX(CASE WHEN seq_num = 15 THEN product_name END) AS product_added_15,\n    MAX(CASE WHEN seq_num = 16 THEN product_name END) AS product_added_16,\n    MAX(CASE WHEN seq_num = 17 THEN product_name END) AS product_added_17,\n    MAX(CASE WHEN seq_num = 18 THEN product_name END) AS product_added_18,\n    MAX(CASE WHEN seq_num = 19 THEN product_name END) AS product_added_19,\n    MAX(CASE WHEN seq_num = 20 THEN product_name END) AS product_added_20,\n    MAX(CASE WHEN seq_num = 21 THEN product_name END) AS product_added_21,\n    MAX(CASE WHEN seq_num = 22 THEN product_name END) AS product_added_22,\n    MAX(CASE WHEN seq_num = 23 THEN product_name END) AS product_added_23\n</pre> In\u00a0[86]: Copied! <pre>product_cols = \", \".join(\n    [f\"product_added_{i}\" for i in range(1, len(distinct_seq_nums) + 1)]\n)\n\nprint(product_cols)\n</pre> product_cols = \", \".join(     [f\"product_added_{i}\" for i in range(1, len(distinct_seq_nums) + 1)] )  print(product_cols) <pre>product_added_1, product_added_2, product_added_3, product_added_4, product_added_5, product_added_6, product_added_7, product_added_8, product_added_9, product_added_10, product_added_11, product_added_12, product_added_13, product_added_14, product_added_15, product_added_16, product_added_17, product_added_18, product_added_19, product_added_20, product_added_21, product_added_22, product_added_23\n</pre> <p>The final steps are as follows:</p> <ol> <li><p>The first CTE selects the distinct <code>visit_id</code> and filters for cart add events, sorting by <code>visit_id</code> and then by <code>sequence_number</code> to ensure that the products ordered based on when the user added them to the cart per visit.</p> </li> <li><p>The second CTE reshapes the long format of products into a wide format, creating a separate column for each product based on the <code>sequence_number</code> value. The <code>MAX(CASE WHEN ...)</code> statements are used to pivot the data. If a product exists at a specific <code>sequence_number</code>, it is placed in the corresponding column, otherwise, it is left as <code>NULL</code>.</p> </li> </ol> visit_id seq_num product_name 1 3 A 1 4 B 2 2 C 2 6 D 2 7 Z visit_id product_1 product_2 product_3 product_4 product_5 product_6 product_7 1 NULL NULL A B NULL NULL NULL 2 NULL C NULL NULL NULL D Z <ol> <li>The final query selects the <code>visit_id</code> and the reshaped product columns, concatenating all product columns into a single string field <code>cart_products</code> that contains the products added to the cart in the order they were added. The <code>CONCAT_WS</code> function is used to concatenate the product columns with a comma separator, which skips <code>NULL</code> values.</li> </ol> In\u00a0[63]: Copied! <pre>query = f\"\"\" \nWITH sorted_product AS (\n    SELECT\n        DISTINCT events.visit_id AS visit_id,\n        events.sequence_number AS seq_num,\n        pa.page_name AS product_name\n    FROM\n        clique_bait.events AS events LEFT JOIN clique_bait.page_hierarchy AS pa ON events.page_id = pa.page_id\n    WHERE\n        events.event_type = 2\n    ORDER BY\n        events.visit_id, events.sequence_number ASC\n),\nproducts_added_wide AS (\n    SELECT\n        visit_id,\n        {select_clause}\n    FROM \n        sorted_product\n    GROUP BY\n        visit_id\n),\nproducts_added AS (\n    SELECT\n        visit_id,\n        CONCAT_WS(', ', {product_cols}) AS cart_products\n    FROM\n        products_added_wide\n)\nSELECT\n    * \nFROM\n    products_added;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = f\"\"\"  WITH sorted_product AS (     SELECT         DISTINCT events.visit_id AS visit_id,         events.sequence_number AS seq_num,         pa.page_name AS product_name     FROM         clique_bait.events AS events LEFT JOIN clique_bait.page_hierarchy AS pa ON events.page_id = pa.page_id     WHERE         events.event_type = 2     ORDER BY         events.visit_id, events.sequence_number ASC ), products_added_wide AS (     SELECT         visit_id,         {select_clause}     FROM          sorted_product     GROUP BY         visit_id ), products_added AS (     SELECT         visit_id,         CONCAT_WS(', ', {product_cols}) AS cart_products     FROM         products_added_wide ) SELECT     *  FROM     products_added; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[63]: visit_id cart_products 0 004aaf Tuna, Lobster 1 01573b Black Truffle, Lobster 2 019f19 Russian Caviar, Abalone 3 01cb19 Tuna 4 01db81 Salmon, Crab ... ... ... 2505 fe5061 Tuna, Abalone 2506 fe6c7e Salmon 2507 fe9c1a Lobster 2508 feb47c Salmon, Russian Caviar, Lobster, Crab 2509 ffd39f Tuna, Abalone, Oyster <p>2510 rows \u00d7 2 columns</p> <p>The above data can then be left join onto the base <code>events</code> table downstream.</p> In\u00a0[15]: Copied! <pre>query = f\"\"\" \nWITH sorted_product AS (\n    SELECT\n        DISTINCT events.visit_id AS visit_id,\n        events.sequence_number AS seq_num,\n        pa.page_name AS product_name\n    FROM\n        clique_bait.events AS events LEFT JOIN clique_bait.page_hierarchy AS pa ON events.page_id = pa.page_id\n    WHERE\n        events.event_type = 2\n    ORDER BY\n        events.visit_id, events.sequence_number ASC\n),\nproducts_added_wide AS (\n    SELECT\n        visit_id,\n        {select_clause}\n    FROM \n        sorted_product\n    GROUP BY\n        visit_id\n),\nproducts_added AS (\n    SELECT\n        visit_id,\n        CONCAT_WS(', ', {product_cols}) AS cart_products\n    FROM\n        products_added_wide\n)\nSELECT\n    users.user_id AS user_id,\n    events.visit_id AS visit_id,\n    camp_id.campaign_name,\n    MIN(events.event_time) AS visit_start_time,\n    MAX(CASE WHEN event_type = 3 THEN 1 ELSE 0 END) AS purchases,\n    SUM(CASE WHEN event_type = 2 THEN 1 ELSE 0 END) AS cart_adds,\n    SUM(CASE WHEN event_type = 1 THEN 1 ELSE 0 END) AS page_views,\n    SUM(CASE WHEN event_type = 4 THEN 1 ELSE 0 END) AS ad_impressions,\n    SUM(CASE WHEN event_type = 5 THEN 1 ELSE 0 END) AS ad_clicks,\n    MAX(products_added.cart_products) AS cart_products\nFROM\n    clique_bait.events AS events \n        LEFT JOIN clique_bait.users AS users ON events.cookie_id = users.cookie_id\n        LEFT JOIN products_added ON events.visit_id = products_added.visit_id\n        LEFT JOIN clique_bait.campaign_identifier AS camp_id ON events.event_time BETWEEN camp_id.start_date AND camp_id.end_date\nGROUP BY\n    users.user_id,\n    events.visit_id,\n    camp_id.campaign_name\nORDER BY\n    users.user_id,\n    events.visit_id,\n    visit_start_time ASC;\n\"\"\"\n\nathena.create_ctas_table(\n    database=database,\n    query=query,\n    ctas_table=\"campaign_metrics\",\n    s3_output=\"s3://sql-case-studies/clique_bait/campaign_metrics/\",\n    storage_format=storage_format,\n    write_compression=write_compression,\n    wait=wait,\n)\n</pre> query = f\"\"\"  WITH sorted_product AS (     SELECT         DISTINCT events.visit_id AS visit_id,         events.sequence_number AS seq_num,         pa.page_name AS product_name     FROM         clique_bait.events AS events LEFT JOIN clique_bait.page_hierarchy AS pa ON events.page_id = pa.page_id     WHERE         events.event_type = 2     ORDER BY         events.visit_id, events.sequence_number ASC ), products_added_wide AS (     SELECT         visit_id,         {select_clause}     FROM          sorted_product     GROUP BY         visit_id ), products_added AS (     SELECT         visit_id,         CONCAT_WS(', ', {product_cols}) AS cart_products     FROM         products_added_wide ) SELECT     users.user_id AS user_id,     events.visit_id AS visit_id,     camp_id.campaign_name,     MIN(events.event_time) AS visit_start_time,     MAX(CASE WHEN event_type = 3 THEN 1 ELSE 0 END) AS purchases,     SUM(CASE WHEN event_type = 2 THEN 1 ELSE 0 END) AS cart_adds,     SUM(CASE WHEN event_type = 1 THEN 1 ELSE 0 END) AS page_views,     SUM(CASE WHEN event_type = 4 THEN 1 ELSE 0 END) AS ad_impressions,     SUM(CASE WHEN event_type = 5 THEN 1 ELSE 0 END) AS ad_clicks,     MAX(products_added.cart_products) AS cart_products FROM     clique_bait.events AS events          LEFT JOIN clique_bait.users AS users ON events.cookie_id = users.cookie_id         LEFT JOIN products_added ON events.visit_id = products_added.visit_id         LEFT JOIN clique_bait.campaign_identifier AS camp_id ON events.event_time BETWEEN camp_id.start_date AND camp_id.end_date GROUP BY     users.user_id,     events.visit_id,     camp_id.campaign_name ORDER BY     users.user_id,     events.visit_id,     visit_start_time ASC; \"\"\"  athena.create_ctas_table(     database=database,     query=query,     ctas_table=\"campaign_metrics\",     s3_output=\"s3://sql-case-studies/clique_bait/campaign_metrics/\",     storage_format=storage_format,     write_compression=write_compression,     wait=wait, ) Out[15]: <pre>{'ctas_database': 'clique_bait',\n 'ctas_table': 'campaign_metrics',\n 'ctas_query_metadata': _QueryMetadata(execution_id='84ec31d6-3c27-4943-a0eb-3ff7cb828ef1', dtype={'rows': 'Int64'}, parse_timestamps=[], parse_dates=[], parse_geometry=[], converters={}, binaries=[], output_location='s3://sql-case-studies/clique_bait/campaign_metrics/tables/84ec31d6-3c27-4943-a0eb-3ff7cb828ef1', manifest_location='s3://sql-case-studies/clique_bait/campaign_metrics/tables/84ec31d6-3c27-4943-a0eb-3ff7cb828ef1-manifest.csv', raw_payload={'QueryExecutionId': '84ec31d6-3c27-4943-a0eb-3ff7cb828ef1', 'Query': 'CREATE TABLE \"clique_bait\".\"campaign_metrics\"\\nWITH(\\n    external_location = \\'s3://sql-case-studies/clique_bait/campaign_metrics/campaign_metrics\\',\\n    write_compression = \\'SNAPPY\\',\\n    format = \\'PARQUET\\')\\nAS  \\nWITH sorted_product AS (\\n    SELECT\\n        DISTINCT events.visit_id AS visit_id,\\n        events.sequence_number AS seq_num,\\n        pa.page_name AS product_name\\n    FROM\\n        clique_bait.events AS events LEFT JOIN clique_bait.page_hierarchy AS pa ON events.page_id = pa.page_id\\n    WHERE\\n        events.event_type = 2\\n    ORDER BY\\n        events.visit_id, events.sequence_number ASC\\n),\\nproducts_added_wide AS (\\n    SELECT\\n        visit_id,\\n        MAX(CASE WHEN seq_num = 1 THEN product_name END) AS product_added_1,\\n    MAX(CASE WHEN seq_num = 2 THEN product_name END) AS product_added_2,\\n    MAX(CASE WHEN seq_num = 3 THEN product_name END) AS product_added_3,\\n    MAX(CASE WHEN seq_num = 4 THEN product_name END) AS product_added_4,\\n    MAX(CASE WHEN seq_num = 5 THEN product_name END) AS product_added_5,\\n    MAX(CASE WHEN seq_num = 6 THEN product_name END) AS product_added_6,\\n    MAX(CASE WHEN seq_num = 7 THEN product_name END) AS product_added_7,\\n    MAX(CASE WHEN seq_num = 8 THEN product_name END) AS product_added_8,\\n    MAX(CASE WHEN seq_num = 9 THEN product_name END) AS product_added_9,\\n    MAX(CASE WHEN seq_num = 10 THEN product_name END) AS product_added_10,\\n    MAX(CASE WHEN seq_num = 11 THEN product_name END) AS product_added_11,\\n    MAX(CASE WHEN seq_num = 12 THEN product_name END) AS product_added_12,\\n    MAX(CASE WHEN seq_num = 13 THEN product_name END) AS product_added_13,\\n    MAX(CASE WHEN seq_num = 14 THEN product_name END) AS product_added_14,\\n    MAX(CASE WHEN seq_num = 15 THEN product_name END) AS product_added_15,\\n    MAX(CASE WHEN seq_num = 16 THEN product_name END) AS product_added_16,\\n    MAX(CASE WHEN seq_num = 17 THEN product_name END) AS product_added_17,\\n    MAX(CASE WHEN seq_num = 18 THEN product_name END) AS product_added_18,\\n    MAX(CASE WHEN seq_num = 19 THEN product_name END) AS product_added_19,\\n    MAX(CASE WHEN seq_num = 20 THEN product_name END) AS product_added_20,\\n    MAX(CASE WHEN seq_num = 21 THEN product_name END) AS product_added_21,\\n    MAX(CASE WHEN seq_num = 22 THEN product_name END) AS product_added_22,\\n    MAX(CASE WHEN seq_num = 23 THEN product_name END) AS product_added_23\\n    FROM \\n        sorted_product\\n    GROUP BY\\n        visit_id\\n),\\nproducts_added AS (\\n    SELECT\\n        visit_id,\\n        CONCAT_WS(\\', \\', product_added_1,\\n    product_added_2,\\n    product_added_3,\\n    product_added_4,\\n    product_added_5,\\n    product_added_6,\\n    product_added_7,\\n    product_added_8,\\n    product_added_9,\\n    product_added_10,\\n    product_added_11,\\n    product_added_12,\\n    product_added_13,\\n    product_added_14,\\n    product_added_15,\\n    product_added_16,\\n    product_added_17,\\n    product_added_18,\\n    product_added_19,\\n    product_added_20,\\n    product_added_21,\\n    product_added_22) AS cart_products\\n    FROM\\n        products_added_wide\\n)\\nSELECT\\n    users.user_id AS user_id,\\n    events.visit_id AS visit_id,\\n    camp_id.campaign_name,\\n    MIN(events.event_time) AS visit_start_time,\\n    MAX(CASE WHEN event_type = 3 THEN 1 ELSE 0 END) AS purchases,\\n    SUM(CASE WHEN event_type = 2 THEN 1 ELSE 0 END) AS cart_adds,\\n    SUM(CASE WHEN event_type = 1 THEN 1 ELSE 0 END) AS page_views,\\n    SUM(CASE WHEN event_type = 4 THEN 1 ELSE 0 END) AS ad_impressions,\\n    SUM(CASE WHEN event_type = 5 THEN 1 ELSE 0 END) AS ad_clicks,\\n    MAX(products_added.cart_products) AS cart_products\\nFROM\\n    clique_bait.events AS events \\n        LEFT JOIN clique_bait.users AS users ON events.cookie_id = users.cookie_id\\n        LEFT JOIN products_added ON events.visit_id = products_added.visit_id\\n        LEFT JOIN clique_bait.campaign_identifier AS camp_id ON events.event_time BETWEEN camp_id.start_date AND camp_id.end_date\\nGROUP BY\\n    users.user_id,\\n    events.visit_id,\\n    camp_id.campaign_name\\nORDER BY\\n    users.user_id,\\n    events.visit_id,\\n    visit_start_time', 'StatementType': 'DDL', 'ResultConfiguration': {'OutputLocation': 's3://sql-case-studies/clique_bait/campaign_metrics/tables/84ec31d6-3c27-4943-a0eb-3ff7cb828ef1'}, 'ResultReuseConfiguration': {'ResultReuseByAgeConfiguration': {'Enabled': False}}, 'QueryExecutionContext': {'Database': 'clique_bait'}, 'Status': {'State': 'SUCCEEDED', 'SubmissionDateTime': datetime.datetime(2024, 8, 9, 23, 32, 9, 118000, tzinfo=tzlocal()), 'CompletionDateTime': datetime.datetime(2024, 8, 9, 23, 32, 11, 718000, tzinfo=tzlocal())}, 'Statistics': {'EngineExecutionTimeInMillis': 2328, 'DataScannedInBytes': 489137, 'DataManifestLocation': 's3://sql-case-studies/clique_bait/campaign_metrics/tables/84ec31d6-3c27-4943-a0eb-3ff7cb828ef1-manifest.csv', 'TotalExecutionTimeInMillis': 2600, 'QueryQueueTimeInMillis': 78, 'ServicePreProcessingTimeInMillis': 169, 'QueryPlanningTimeInMillis': 493, 'ServiceProcessingTimeInMillis': 25, 'ResultReuseInformation': {'ReusedPreviousResult': False}}, 'WorkGroup': 'primary', 'EngineVersion': {'SelectedEngineVersion': 'AUTO', 'EffectiveEngineVersion': 'Athena engine version 3'}, 'SubstatementType': 'CREATE_TABLE_AS_SELECT'})}</pre> In\u00a0[27]: Copied! <pre>query = \"\"\" \nSELECT\n    *\nFROM\n    clique_bait.campaign_metrics;\n\"\"\"\n\ncampaign_metrics = athena.query(\n    database=database, query=query, ctas_approach=ctas_approach\n)\n\ncampaign_metrics\n</pre> query = \"\"\"  SELECT     * FROM     clique_bait.campaign_metrics; \"\"\"  campaign_metrics = athena.query(     database=database, query=query, ctas_approach=ctas_approach )  campaign_metrics Out[27]: user_id visit_id campaign_name visit_start_time purchases cart_adds page_views ad_impressions ad_clicks cart_products 0 1 02a5d5 Half Off - Treat Your Shellf(ish) 2020-02-26 16:57:26.260 0 0 4 0 0 &lt;NA&gt; 1 1 0826dc Half Off - Treat Your Shellf(ish) 2020-02-26 05:58:37.918 0 0 1 0 0 &lt;NA&gt; 2 1 0fc437 Half Off - Treat Your Shellf(ish) 2020-02-04 17:49:49.602 1 6 10 1 1 Tuna, Russian Caviar, Black Truffle, Abalone, ... 3 1 30b94d Half Off - Treat Your Shellf(ish) 2020-03-15 13:12:54.023 1 7 9 1 1 Salmon, Kingfish, Tuna, Russian Caviar, Abalon... 4 1 41355d Half Off - Treat Your Shellf(ish) 2020-03-25 00:11:17.860 0 1 6 0 0 Lobster ... ... ... ... ... ... ... ... ... ... ... 3559 499 e6794c &lt;NA&gt; 2020-01-29 04:55:55.745 1 4 8 0 0 Russian Caviar, Abalone, Lobster, Crab 3560 500 13668d Half Off - Treat Your Shellf(ish) 2020-02-28 02:16:14.963 1 5 8 1 1 Salmon, Tuna, Russian Caviar, Abalone, Crab 3561 500 29e5f8 Half Off - Treat Your Shellf(ish) 2020-02-28 22:06:31.450 1 3 8 0 0 Kingfish, Russian Caviar, Abalone 3562 500 4cdfb7 Half Off - Treat Your Shellf(ish) 2020-02-28 00:31:14.862 0 0 1 0 0 &lt;NA&gt; 3563 500 d2888f Half Off - Treat Your Shellf(ish) 2020-02-28 21:23:41.818 0 0 7 0 0 &lt;NA&gt; <p>3564 rows \u00d7 10 columns</p> In\u00a0[24]: Copied! <pre>metrics = [\"purchases\", \"cart_adds\", \"page_views\", \"ad_impressions\", \"ad_clicks\"]\n\ncampaign_metrics.loc[:, metrics].describe(percentiles=[0.25, 0.5, 0.75, 0.95])\n</pre> metrics = [\"purchases\", \"cart_adds\", \"page_views\", \"ad_impressions\", \"ad_clicks\"]  campaign_metrics.loc[:, metrics].describe(percentiles=[0.25, 0.5, 0.75, 0.95]) Out[24]: purchases cart_adds page_views ad_impressions ad_clicks count 3564.0 3564.0 3564.0 3564.0 3564.0 mean 0.498597 2.371212 5.872054 0.245791 0.19697 std 0.500068 2.233612 3.25593 0.430616 0.397765 min 0.0 0.0 1.0 0.0 0.0 25% 0.0 0.0 1.0 0.0 0.0 50% 0.0 2.0 7.0 0.0 0.0 75% 1.0 4.0 8.0 0.0 0.0 95% 1.0 7.0 10.0 1.0 1.0 max 1.0 9.0 12.0 1.0 1.0 In\u00a0[44]: Copied! <pre>query = \"\"\" \nWITH ad_metrics AS (\n    SELECT \n        user_id,\n        purchases,\n        CASE \n            WHEN ad_impressions = 0 AND ad_clicks = 0 THEN 'no_impression_no_click' \n            WHEN ad_impressions &gt; 0 AND ad_clicks = 0 THEN 'yes_impression_no_click' \n            WHEN ad_impressions &gt; 0 AND ad_clicks &gt; 0 THEN 'yes_impression_yes_click'\n            ELSE 'other'\n        END AS ad_groups\n    FROM \n        clique_bait.campaign_metrics\n)\nSELECT\n    ad_groups,\n    SUM(purchases) AS purchases,\n    COUNT(DISTINCT user_id) AS users,\n    ROUND(TRY_CAST(SUM(purchases) AS double) / TRY_CAST(COUNT(DISTINCT user_id) AS double), 2) AS purchase_rate\nFROM\n    ad_metrics\nGROUP BY\n    ad_groups\nORDER BY\n    purchase_rate DESC;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  WITH ad_metrics AS (     SELECT          user_id,         purchases,         CASE              WHEN ad_impressions = 0 AND ad_clicks = 0 THEN 'no_impression_no_click'              WHEN ad_impressions &gt; 0 AND ad_clicks = 0 THEN 'yes_impression_no_click'              WHEN ad_impressions &gt; 0 AND ad_clicks &gt; 0 THEN 'yes_impression_yes_click'             ELSE 'other'         END AS ad_groups     FROM          clique_bait.campaign_metrics ) SELECT     ad_groups,     SUM(purchases) AS purchases,     COUNT(DISTINCT user_id) AS users,     ROUND(TRY_CAST(SUM(purchases) AS double) / TRY_CAST(COUNT(DISTINCT user_id) AS double), 2) AS purchase_rate FROM     ad_metrics GROUP BY     ad_groups ORDER BY     purchase_rate DESC; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[44]: ad_groups purchases users purchase_rate 0 no_impression_no_click 1040 500 2.08 1 yes_impression_yes_click 624 397 1.57 2 yes_impression_no_click 113 148 0.76 <p>Holding impression to be non-zero, we see that the purchase rate is indeeded higher among user visits that involved an ad click.</p> <p>Interestingly, the group with zero impressions and zero clicks has the highest purchase rate relative to the other two groups--- non-zero impressions and zero clicks, and non-zero impressions and non-zero clicks. This may indicate that there are other factors at play that are not captured in the data, such as user intent or the quality of the ad content.</p> In\u00a0[61]: Copied! <pre>campaign_metrics[\"ad_groups\"] = np.select(\n    condlist=[\n        (campaign_metrics[\"ad_impressions\"] == 0)\n        &amp; (campaign_metrics[\"ad_clicks\"] == 0),\n        (campaign_metrics[\"ad_impressions\"] &gt; 0) &amp; (campaign_metrics[\"ad_clicks\"] == 0),\n        (campaign_metrics[\"ad_impressions\"] &gt; 0) &amp; (campaign_metrics[\"ad_clicks\"] &gt; 0),\n    ],\n    choicelist=[\n        \"no_impression_no_click\",\n        \"yes_impression_no_click\",\n        \"yes_impression_yes_click\",\n    ],\n    default=\"other\",\n)\n\ncampaign_metrics[\"ad_groups\"].value_counts()\n</pre> campaign_metrics[\"ad_groups\"] = np.select(     condlist=[         (campaign_metrics[\"ad_impressions\"] == 0)         &amp; (campaign_metrics[\"ad_clicks\"] == 0),         (campaign_metrics[\"ad_impressions\"] &gt; 0) &amp; (campaign_metrics[\"ad_clicks\"] == 0),         (campaign_metrics[\"ad_impressions\"] &gt; 0) &amp; (campaign_metrics[\"ad_clicks\"] &gt; 0),     ],     choicelist=[         \"no_impression_no_click\",         \"yes_impression_no_click\",         \"yes_impression_yes_click\",     ],     default=\"other\", )  campaign_metrics[\"ad_groups\"].value_counts() Out[61]: <pre>ad_groups\nno_impression_no_click      2688\nyes_impression_yes_click     702\nyes_impression_no_click      174\nName: count, dtype: int64</pre> In\u00a0[60]: Copied! <pre>kruskal(\n    campaign_metrics.loc[\n        campaign_metrics[\"ad_groups\"] == \"no_impression_no_click\", \"purchases\"\n    ],\n    campaign_metrics.loc[\n        campaign_metrics[\"ad_groups\"] == \"yes_impression_no_click\", \"purchases\"\n    ],\n    campaign_metrics.loc[\n        campaign_metrics[\"ad_groups\"] == \"yes_impression_yes_click\", \"purchases\"\n    ],\n)\n</pre> kruskal(     campaign_metrics.loc[         campaign_metrics[\"ad_groups\"] == \"no_impression_no_click\", \"purchases\"     ],     campaign_metrics.loc[         campaign_metrics[\"ad_groups\"] == \"yes_impression_no_click\", \"purchases\"     ],     campaign_metrics.loc[         campaign_metrics[\"ad_groups\"] == \"yes_impression_yes_click\", \"purchases\"     ], ) Out[60]: <pre>KruskalResult(statistic=577.5451006662854, pvalue=3.86967832761235e-126)</pre> <p>Controlling Type-I error rate at any rate, we reject the null hypothesis that the median number of purchases is the same across all groups.</p> In\u00a0[67]: Copied! <pre>alpha = 0.01\n\nposthoc_dunn(\n    a=campaign_metrics, val_col=\"purchases\", group_col=\"ad_groups\", sort=\"bonferroni\"\n) &lt; alpha\n</pre> alpha = 0.01  posthoc_dunn(     a=campaign_metrics, val_col=\"purchases\", group_col=\"ad_groups\", sort=\"bonferroni\" ) &lt; alpha Out[67]: no_impression_no_click yes_impression_no_click yes_impression_yes_click no_impression_no_click False True True yes_impression_no_click True False True yes_impression_yes_click True True False In\u00a0[81]: Copied! <pre>query = \"\"\" \nWITH calculations AS (\n    SELECT\n        campaign_name,\n        SUM(ad_impressions) AS total_impressions,\n        SUM(ad_clicks) AS total_clicks,\n        SUM(purchases) AS total_purchases,\n        COUNT(visit_id) AS total_visits,\n        COUNT(DISTINCT user_id) AS total_users,\n        SUM(page_views) AS total_page_views,\n        SUM(cart_adds) AS total_cart_adds\n    FROM\n        clique_bait.campaign_metrics\n    GROUP BY\n        campaign_name\n)\nSELECT\n    COALESCE(campaign_name, 'No Campaign') AS campaign_name,\n    COALESCE(TRY_CAST(total_clicks AS double) / TRY_CAST(total_impressions AS double), 0) * 100 AS click_through_rate,\n    COALESCE(TRY_CAST(total_purchases AS double) / TRY_CAST(total_clicks AS double), 0) * 100 AS conversion_rate, \n    COALESCE(TRY_CAST(total_purchases AS double) / TRY_CAST(total_visits AS double), 0) * 100 AS purchase_rate,\n    total_impressions,\n    total_clicks,\n    total_purchases,\n    total_visits,\n    total_users,\n    total_page_views,\n    total_cart_adds\nFROM\n    calculations\nORDER BY\n    campaign_name;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  WITH calculations AS (     SELECT         campaign_name,         SUM(ad_impressions) AS total_impressions,         SUM(ad_clicks) AS total_clicks,         SUM(purchases) AS total_purchases,         COUNT(visit_id) AS total_visits,         COUNT(DISTINCT user_id) AS total_users,         SUM(page_views) AS total_page_views,         SUM(cart_adds) AS total_cart_adds     FROM         clique_bait.campaign_metrics     GROUP BY         campaign_name ) SELECT     COALESCE(campaign_name, 'No Campaign') AS campaign_name,     COALESCE(TRY_CAST(total_clicks AS double) / TRY_CAST(total_impressions AS double), 0) * 100 AS click_through_rate,     COALESCE(TRY_CAST(total_purchases AS double) / TRY_CAST(total_clicks AS double), 0) * 100 AS conversion_rate,      COALESCE(TRY_CAST(total_purchases AS double) / TRY_CAST(total_visits AS double), 0) * 100 AS purchase_rate,     total_impressions,     total_clicks,     total_purchases,     total_visits,     total_users,     total_page_views,     total_cart_adds FROM     calculations ORDER BY     campaign_name; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[81]: campaign_name click_through_rate conversion_rate purchase_rate total_impressions total_clicks total_purchases total_visits total_users total_page_views total_cart_adds 0 25% Off - Living The Lux Life 77.884615 249.382716 50.000000 104 81 202 404 160 2434 991 1 BOGOF - Fishing For Compliments 84.615385 230.909091 48.846154 65 55 127 260 103 1536 625 2 Half Off - Treat Your Shellf(ish) 80.103806 254.859611 49.413735 578 463 1180 2388 449 13897 5592 3 No Campaign 79.844961 260.194175 52.343750 129 103 268 512 183 3061 1243 In\u00a0[88]: Copied! <pre>athena.drop_view(database=database, view=\"product_cat_funnel\", wait=wait)\n\nathena.drop_view(database=database, view=\"product_funnel\", wait=wait)\n</pre> athena.drop_view(database=database, view=\"product_cat_funnel\", wait=wait)  athena.drop_view(database=database, view=\"product_funnel\", wait=wait) <pre>Query executed successfully\n</pre> In\u00a0[89]: Copied! <pre>athena.drop_table(database=database, table=\"campaign_metrics\", wait=wait)\n</pre> athena.drop_table(database=database, table=\"campaign_metrics\", wait=wait) <pre>Query executed successfully\n</pre>"},{"location":"clique_bait/#global","title":"Global\u00b6","text":""},{"location":"clique_bait/#problem-statement","title":"Problem Statement\u00b6","text":"<p>Danny, the founder and CEO of Clique Bait, seeks to combine his expertise in digital data analytics with the seafood industry by creating an innovative online seafood store. To align with this vision, the analysis of available data is crucial to develop creative solutions for calculating funnel fallout rates for the Clique Bait online store. Gaining insights into where potential customers drop off in the sales funnel will be key to optimizing the user experience and enhancing conversion rates.</p>"},{"location":"clique_bait/#entity-relationship-diagram","title":"Entity Relationship Diagram\u00b6","text":""},{"location":"clique_bait/#users","title":"Users\u00b6","text":"<p>This table logs the users who visit the Clique Bait website. Each user is identified by a <code>user_id</code> and is tagged via a <code>cookie_id</code>. The <code>start_date</code> indicates when the user first interacted with the website.</p>"},{"location":"clique_bait/#events-table","title":"Events Table\u00b6","text":"<p>This table captures detailed logs of customer visits at the <code>cookie_id</code> level. Each event is recorded with a <code>visit_id</code>, <code>page_id</code>, <code>event_type</code>, and a <code>sequence_number</code> that orders the events within a visit. The <code>event_time</code> marks the timestamp of each interaction.</p>"},{"location":"clique_bait/#event-identifier-table","title":"Event Identifier Table\u00b6","text":"<p>The event identifier table provides the names of the different types of events captured in the <code>Events</code> table. Each <code>event_type</code> corresponds to a specific <code>event_name</code>, such as Page View, Add to Cart, Purchase, etc.</p>"},{"location":"clique_bait/#campaign-identifier-table","title":"Campaign Identifier Table\u00b6","text":"<p>This table contains information about the marketing campaigns run by Clique Bait in 2020. Each campaign is associated with a <code>campaign_id</code>, a list of <code>products</code>, a <code>campaign_name</code>, and the <code>start_date</code> and <code>end_date</code> of the campaign.</p>"},{"location":"clique_bait/#page-hierarchy-table","title":"Page Hierarchy Table\u00b6","text":"<p>The page hierarchy table provides details on the website's structure. Each <code>page_id</code> is associated with a <code>page_name</code>, and in some cases, a <code>product_category</code> and <code>product_id</code>. This table helps in identifying the specific pages visited during user sessions.</p>"},{"location":"clique_bait/#tables","title":"Tables\u00b6","text":""},{"location":"clique_bait/#digital-analysis","title":"Digital Analysis\u00b6","text":""},{"location":"clique_bait/#q1","title":"Q1\u00b6","text":"<p>How many users are there?</p>"},{"location":"clique_bait/#q2","title":"Q2\u00b6","text":"<p>How many cookies does each user have on average?</p>"},{"location":"clique_bait/#verify-cookie-id-uniqueness","title":"Verify Cookie ID Uniqueness\u00b6","text":"<p>First, verify that each <code>cookie_id</code> is non-repeating for each <code>user_id</code>:</p>"},{"location":"clique_bait/#q3","title":"Q3\u00b6","text":"<p>What is the unique number of visits by all users per month?</p>"},{"location":"clique_bait/#q4","title":"Q4\u00b6","text":"<p>What is the number of events for each event type?</p>"},{"location":"clique_bait/#cookie-id-visit-id","title":"Cookie ID &amp; Visit ID\u00b6","text":"<p>Examine the relationship between <code>cookie_id</code> and <code>visit_id</code>:</p>"},{"location":"clique_bait/#q5","title":"Q5\u00b6","text":"<p>What is the percentage of visits which have a purchase event?</p> <p>Group by each <code>visit_id</code> and use <code>MAX</code> to return 1 if there was ever a purchase event in a visit. The <code>CASE</code> when returns 1 for all rows within a visit that are purchases, and <code>MAX</code> simply returns a 1 if there is at least one purchase event in a visit. For examples if a visit has had 2 add to carts and 2 purchases, then <code>[2, 2, 3, 3]</code> will result in <code>[0, 0, 1, 1]</code> and <code>MAX</code> will return 1.</p> <p>Note: we use $1.0$ and $0.0$ instead of $1$ and $0$ to ensure that the division returns a double to avoid integer division downstream.</p>"},{"location":"clique_bait/#q6","title":"Q6\u00b6","text":"<p>What is the percentage of visits which view the checkout page but do not have a purchase event?</p> <p>In the <code>page_hierarchy</code> table, the <code>page_id</code> for the checkout page is <code>12</code> and the <code>event_type</code> for viewing a page is <code>1</code> and <code>3</code> for a purchase event.</p>"},{"location":"clique_bait/#q7","title":"Q7\u00b6","text":"<p>What are the top 3 pages by number of views?</p>"},{"location":"clique_bait/#page-id-visit-id","title":"Page ID &amp; Visit ID\u00b6","text":"<p>Examine the relationship between <code>page_id</code> and <code>visit_id</code>:</p>"},{"location":"clique_bait/#q8","title":"Q8\u00b6","text":"<p>What is the number of views and cart adds for each product category?</p>"},{"location":"clique_bait/#q9","title":"Q9\u00b6","text":"<p>What are the top 3 products by add to cart followed by purchase events?</p>"},{"location":"clique_bait/#product-funnel-analysis","title":"Product Funnel Analysis\u00b6","text":""},{"location":"clique_bait/#product-funnel-view","title":"Product Funnel View\u00b6","text":"<p>Create a view that contains the following information:</p> <ul> <li><p>How many times was each product viewed?</p> </li> <li><p>How many times was each product added to cart?</p> </li> <li><p>How many times was each product added to a cart but not purchased (abandoned)?</p> </li> <li><p>How many times was each product purchased?</p> </li> </ul>"},{"location":"clique_bait/#page-views-cart-adds-cte","title":"Page Views &amp; Cart Adds CTE\u00b6","text":"<p>The first CTE groups by <code>visit_id</code>, <code>product_id</code>/<code>product_name</code>, and <code>product_category</code> to count the number of page views and cart adds for each combination of these dimensions.</p> <p>All events that are associated with non-product pages are filtered out, focusing only on interactions related to specific products where <code>product_id</code> is not null.</p>"},{"location":"clique_bait/#product-purchase-cte","title":"Product Purchase CTE\u00b6","text":"<p>The second CTE identifies visits that resulted in a purchase event, grouping by <code>visit_id</code>, <code>product_id</code>/<code>product_name</code>, and <code>product_category</code> to count the number of purchases for each product.</p> <p>Again, we do not need to use <code>DISTINCT</code> here since <code>visit_id</code> and <code>event_type</code> are one-to-one within the context of a purchase event.</p>"},{"location":"clique_bait/#combined-cte","title":"Combined CTE\u00b6","text":"<p>The final CTE combines the results from the previous two CTEs, left joining the purchase events data onto the page views and cart adds data.</p> <p>In the combined table, if a visit in the <code>page_view_cart_add_cte</code> CTE did not result in a purchase, the <code>page_view_cart_add_cte.visit_id</code> would be non-null while the <code>purchase_cte.visit_id</code> column woud be null, which provides a way for us to determine which visits did and did no lead to a purchase.</p> <p>We use a <code>CASE</code> statment to create a binary flag indicating whether a visit ultimately resulted in a purchase.</p>"},{"location":"clique_bait/#final-view","title":"Final View\u00b6","text":"<p>The final product funnel view takes the combined CTE and aggregates the data, group by <code>product_id</code>/<code>product_name</code> and <code>product_category</code>.</p>"},{"location":"clique_bait/#product-category-funnel","title":"Product Category Funnel\u00b6","text":"<p>To analyze the product funnel by category, we can further group the results by <code>product_category</code> in the final view.</p>"},{"location":"clique_bait/#q1","title":"Q1\u00b6","text":"<p>Which product had the most views, cart adds and purchases?</p>"},{"location":"clique_bait/#q2","title":"Q2\u00b6","text":"<p>Which product was most likely to be abandoned?</p> <p>Because <code>purchases</code> and <code>abandoned</code> are both subsets of <code>cart_adds</code>, we can calculate the abandonment rate as the ratio of abandoned cart adds to total cart adds:</p>"},{"location":"clique_bait/#q3","title":"Q3\u00b6","text":"<p>Which product had the highest view to purchase percentage?</p>"},{"location":"clique_bait/#q4","title":"Q4\u00b6","text":"<p>What is the average conversion rate from view to cart add?</p> <p>We will sum the total page views and car adds and then apply the division to calculate a single metric that proxies for the average conversion rate from view to cart add. This is because we should not average ratios that have different denominators (i.e., page views) even if the they are similarly sized across products.</p>"},{"location":"clique_bait/#q5","title":"Q5\u00b6","text":"<p>What is the average conversion rate from cart add to purchase?</p>"},{"location":"clique_bait/#campaigns-analysis","title":"Campaigns Analysis\u00b6","text":""},{"location":"clique_bait/#cart-products","title":"Cart Products\u00b6","text":"<p>This field is a comma separated text field with products added to the cart sorted by the order they were added to the cart.</p> <p>First, we need to dynamically figure out the maximum range for <code>sequence_number</code> among all visits. This will allow us to generate a sequence of numbers from 1 to the maximum range, which we can then use to reshape <code>page_name</code> (i.e., <code>product_name</code>) into separate columns.</p>"},{"location":"clique_bait/#campaigns-metrics-table","title":"Campaigns Metrics Table\u00b6","text":"<p>We employ a CTAS (Create Table As Select) statement to store the results of the campaigns analysis as a new table:</p> Column Name Description <code>user_id</code> Unique identifier for the user. <code>visit_id</code> Unique identifier for each visit. <code>campaign_name</code> The name of the marketing campaign mapped to the visit if the <code>visit_start_time</code> falls within the campaign's start and end dates. <code>visit_start_time</code> The earliest event time recorded for each visit. <code>purchases</code> A flag indicating whether a purchase was made during the visit (1 for yes, 0 for no). <code>cart_adds</code> Count of products added to the cart during the visit. <code>page_views</code> Count of page views during the visit. <code>ad_impression</code> Count of ad impressions during the visit. <code>ad_click</code> Count of ad clicks during the visit. <code>cart_products</code> A comma-separated list of products added to the cart, sorted by the order they were added."},{"location":"clique_bait/#summary-statistics","title":"Summary Statistics\u00b6","text":""},{"location":"clique_bait/#impression-purchase-rate","title":"Impression &amp; Purchase Rate\u00b6","text":"<p>Does clicking on an impression lead to higher purchase rates?</p>"},{"location":"clique_bait/#kruskal-wallis-test","title":"Kruskal-Wallis Test\u00b6","text":"<p>We can use the Kruskal-Wallis test to determine if there is a statistically significant difference median number of purchases among the three groups. The assumption of independent groups may not hold, but we can verify using visualizations and tests.</p>"},{"location":"clique_bait/#dunns-test-for-multiple-comparisons","title":"Dunn's Test for Multiple Comparisons\u00b6","text":""},{"location":"clique_bait/#campaign-success-metrics","title":"Campaign Success Metrics\u00b6","text":""},{"location":"clique_bait/#clean-up","title":"Clean Up\u00b6","text":""},{"location":"dannys_diner/","title":"Danny's Diner","text":"In\u00a0[1]: Copied! <pre>from IPython.core.interactiveshell import InteractiveShell\n\nInteractiveShell.ast_node_interactivity = \"all\"\n\nimport os\nimport sys\n\nsys.path.append(\"../../../\")\nfrom src.athena import Athena\nfrom src.utils import create_session\n</pre> from IPython.core.interactiveshell import InteractiveShell  InteractiveShell.ast_node_interactivity = \"all\"  import os import sys  sys.path.append(\"../../../\") from src.athena import Athena from src.utils import create_session In\u00a0[2]: Copied! <pre>boto3_session = create_session(\n    profile_name=\"dev\",\n    role_arn=os.getenv(\"ATHENA_IAM_ROLE_ARN\"),\n)\n\nwait = True\nctas_approach = False\n\ndatabase = \"dannys_diner\"\ntables = [\"menu\", \"sales\", \"members\"]\n\nathena = Athena(boto3_session=boto3_session, s3_output=os.getenv(\"ATHENA_S3_OUTPUT\"))\nathena\n</pre> boto3_session = create_session(     profile_name=\"dev\",     role_arn=os.getenv(\"ATHENA_IAM_ROLE_ARN\"), )  wait = True ctas_approach = False  database = \"dannys_diner\" tables = [\"menu\", \"sales\", \"members\"]  athena = Athena(boto3_session=boto3_session, s3_output=os.getenv(\"ATHENA_S3_OUTPUT\")) athena Out[2]: <pre>Athena(boto3_session=Session(region_name='us-east-1'), s3_output=s3://sql-case-studies/query_results)</pre> In\u00a0[5]: Copied! <pre>for table in tables:\n    athena.query(\n        database=database,\n        query=f\"\"\" \n                SELECT\n                    *\n                FROM\n                    {database}.{table} TABLESAMPLE BERNOULLI(50);\n              \"\"\",\n        ctas_approach=ctas_approach,\n    )\n</pre> for table in tables:     athena.query(         database=database,         query=f\"\"\"                  SELECT                     *                 FROM                     {database}.{table} TABLESAMPLE BERNOULLI(50);               \"\"\",         ctas_approach=ctas_approach,     ) Out[5]: customer_id order_date product_id 0 A 2021-01-01 1 1 A 2021-01-01 2 2 A 2021-01-07 2 3 A 2021-01-10 3 4 A 2021-01-11 3 5 A 2021-01-11 3 6 B 2021-01-11 1 7 B 2021-01-16 3 8 C 2021-01-01 3 9 C 2021-01-01 3 10 C 2021-01-07 3 Out[5]: product_id product_name price 0 2 curry 15.0 1 3 ramen 12.0 Out[5]: customer_id join_date 0 A 2021-01-07 In\u00a0[6]: Copied! <pre>q1_query = \"\"\"\nSELECT\n  s.customer_id AS customer_id,\n  SUM(m.price) AS total_spent\nFROM\n  dannys_diner.sales AS s\n  LEFT JOIN dannys_diner.menu AS m ON s.product_id = m.product_id\nGROUP BY\n  s.customer_id\nORDER BY \n  total_spent DESC;\n\"\"\"\n\nathena.query(database=database, query=q1_query, ctas_approach=ctas_approach)\n</pre> q1_query = \"\"\" SELECT   s.customer_id AS customer_id,   SUM(m.price) AS total_spent FROM   dannys_diner.sales AS s   LEFT JOIN dannys_diner.menu AS m ON s.product_id = m.product_id GROUP BY   s.customer_id ORDER BY    total_spent DESC; \"\"\"  athena.query(database=database, query=q1_query, ctas_approach=ctas_approach) Out[6]: customer_id total_spent 0 A 76.0 1 B 74.0 2 C 36.0 In\u00a0[7]: Copied! <pre>q2_query = \"\"\"\nSELECT\n  customer_id,\n  COUNT(DISTINCT order_date) AS num_visits\nFROM\n  dannys_diner.sales\nGROUP BY\n  customer_id\nORDER BY\n  num_visits DESC;\n\"\"\"\n\nathena.query(database=database, query=q2_query, ctas_approach=ctas_approach)\n</pre> q2_query = \"\"\" SELECT   customer_id,   COUNT(DISTINCT order_date) AS num_visits FROM   dannys_diner.sales GROUP BY   customer_id ORDER BY   num_visits DESC; \"\"\"  athena.query(database=database, query=q2_query, ctas_approach=ctas_approach) Out[7]: customer_id num_visits 0 B 6 1 A 4 2 C 2 In\u00a0[8]: Copied! <pre>q3_cte_query = \"\"\"\n  SELECT\n    s.customer_id AS customer_id,\n    RANK() OVER (\n      PARTITION BY s.customer_id\n      ORDER BY s.order_date\n    ) AS rank_number,\n    s.order_date AS order_date,\n    m.product_name AS product_name\n  FROM\n    dannys_diner.sales AS s\n    LEFT JOIN dannys_diner.menu AS m ON s.product_id = m.product_id;\n\"\"\"\n\nathena.query(database=database, query=q3_cte_query, ctas_approach=ctas_approach)\n</pre> q3_cte_query = \"\"\"   SELECT     s.customer_id AS customer_id,     RANK() OVER (       PARTITION BY s.customer_id       ORDER BY s.order_date     ) AS rank_number,     s.order_date AS order_date,     m.product_name AS product_name   FROM     dannys_diner.sales AS s     LEFT JOIN dannys_diner.menu AS m ON s.product_id = m.product_id; \"\"\"  athena.query(database=database, query=q3_cte_query, ctas_approach=ctas_approach) Out[8]: customer_id rank_number order_date product_name 0 C 1 2021-01-01 ramen 1 C 1 2021-01-01 ramen 2 C 3 2021-01-07 ramen 3 A 1 2021-01-01 sushi 4 A 1 2021-01-01 curry 5 A 3 2021-01-07 curry 6 A 4 2021-01-10 ramen 7 A 5 2021-01-11 ramen 8 A 5 2021-01-11 ramen 9 B 1 2021-01-01 curry 10 B 2 2021-01-02 curry 11 B 3 2021-01-04 sushi 12 B 4 2021-01-11 sushi 13 B 5 2021-01-16 ramen 14 B 6 2021-02-01 ramen In\u00a0[9]: Copied! <pre>q3_query = \"\"\"\nWITH sales_data AS (\n  SELECT\n    s.customer_id AS customer_id,\n    RANK() OVER (\n      PARTITION BY s.customer_id\n      ORDER BY s.order_date\n    ) AS rank_number,\n    s.order_date AS order_date,\n    m.product_name AS product_name\n  FROM\n    dannys_diner.sales AS s\n    LEFT JOIN dannys_diner.menu AS m ON s.product_id = m.product_id\n)\nSELECT\n  customer_id,\n  order_date,\n  product_name\nFROM\n  sales_data\nWHERE\n  rank_number = 1\nORDER BY\n  customer_id;\n\"\"\"\n\nathena.query(database=database, query=q3_query, ctas_approach=ctas_approach)\n</pre> q3_query = \"\"\" WITH sales_data AS (   SELECT     s.customer_id AS customer_id,     RANK() OVER (       PARTITION BY s.customer_id       ORDER BY s.order_date     ) AS rank_number,     s.order_date AS order_date,     m.product_name AS product_name   FROM     dannys_diner.sales AS s     LEFT JOIN dannys_diner.menu AS m ON s.product_id = m.product_id ) SELECT   customer_id,   order_date,   product_name FROM   sales_data WHERE   rank_number = 1 ORDER BY   customer_id; \"\"\"  athena.query(database=database, query=q3_query, ctas_approach=ctas_approach) Out[9]: customer_id order_date product_name 0 A 2021-01-01 sushi 1 A 2021-01-01 curry 2 B 2021-01-01 curry 3 C 2021-01-01 ramen 4 C 2021-01-01 ramen In\u00a0[10]: Copied! <pre>q4_query = \"\"\"\nSELECT\n  m.product_name AS product_name,\n  COUNT(m.product_name) AS times_purchased\nFROM\n  dannys_diner.sales AS s\n  LEFT JOIN dannys_diner.menu AS m ON s.product_id = m.product_id\nGROUP BY\n  m.product_name\nORDER BY\n  times_purchased DESC\nLIMIT 1;\n\"\"\"\n\nathena.query(database=database, query=q4_query, ctas_approach=ctas_approach)\n</pre> q4_query = \"\"\" SELECT   m.product_name AS product_name,   COUNT(m.product_name) AS times_purchased FROM   dannys_diner.sales AS s   LEFT JOIN dannys_diner.menu AS m ON s.product_id = m.product_id GROUP BY   m.product_name ORDER BY   times_purchased DESC LIMIT 1; \"\"\"  athena.query(database=database, query=q4_query, ctas_approach=ctas_approach) Out[10]: product_name times_purchased 0 ramen 8 In\u00a0[11]: Copied! <pre>q5_mlp_query = \"\"\"\nSELECT\n    s.customer_id AS customer_id,\n\n    COUNT(s.product_id) OVER (\n        PARTITION BY s.customer_id, s.product_id\n        ORDER BY m.product_name\n    ) AS times_purchased_by_customer_and_product,\n\n    COUNT(s.product_id) OVER (\n        PARTITION BY s.product_id\n    ) AS times_purchased_total_by_product,\n\n    m.product_name AS product_name\nFROM\n    dannys_diner.sales AS s\n    LEFT JOIN dannys_diner.menu AS m ON s.product_id = m.product_id\nORDER BY\n  customer_id, times_purchased_by_customer_and_product DESC;\n\"\"\"\n\nathena.query(database=database, query=q5_mlp_query, ctas_approach=ctas_approach)\n</pre> q5_mlp_query = \"\"\" SELECT     s.customer_id AS customer_id,      COUNT(s.product_id) OVER (         PARTITION BY s.customer_id, s.product_id         ORDER BY m.product_name     ) AS times_purchased_by_customer_and_product,      COUNT(s.product_id) OVER (         PARTITION BY s.product_id     ) AS times_purchased_total_by_product,      m.product_name AS product_name FROM     dannys_diner.sales AS s     LEFT JOIN dannys_diner.menu AS m ON s.product_id = m.product_id ORDER BY   customer_id, times_purchased_by_customer_and_product DESC; \"\"\"  athena.query(database=database, query=q5_mlp_query, ctas_approach=ctas_approach) Out[11]: customer_id times_purchased_by_customer_and_product times_purchased_total_by_product product_name 0 A 3 8 ramen 1 A 3 8 ramen 2 A 3 8 ramen 3 A 2 4 curry 4 A 2 4 curry 5 A 1 3 sushi 6 B 2 4 curry 7 B 2 3 sushi 8 B 2 4 curry 9 B 2 3 sushi 10 B 2 8 ramen 11 B 2 8 ramen 12 C 3 8 ramen 13 C 3 8 ramen 14 C 3 8 ramen In\u00a0[12]: Copied! <pre>q5_cte_query = \"\"\"\nSELECT\n    s.customer_id AS customer_id,\n    m.product_name AS product_name,\n    COUNT(m.product_name) AS times_purchased,\n    RANK() OVER (\n        PARTITION BY s.customer_id\n        ORDER BY COUNT(m.product_name) DESC\n    ) AS rank_number\nFROM\n    dannys_diner.sales AS s\n    LEFT JOIN dannys_diner.menu AS m ON s.product_id = m.product_id\nGROUP BY\n    s.customer_id, m.product_name;\n\"\"\"\n\nathena.query(database=database, query=q5_cte_query, ctas_approach=ctas_approach)\n</pre> q5_cte_query = \"\"\" SELECT     s.customer_id AS customer_id,     m.product_name AS product_name,     COUNT(m.product_name) AS times_purchased,     RANK() OVER (         PARTITION BY s.customer_id         ORDER BY COUNT(m.product_name) DESC     ) AS rank_number FROM     dannys_diner.sales AS s     LEFT JOIN dannys_diner.menu AS m ON s.product_id = m.product_id GROUP BY     s.customer_id, m.product_name; \"\"\"  athena.query(database=database, query=q5_cte_query, ctas_approach=ctas_approach) Out[12]: customer_id product_name times_purchased rank_number 0 C ramen 3 1 1 A ramen 3 1 2 A curry 2 2 3 A sushi 1 3 4 B curry 2 1 5 B sushi 2 1 6 B ramen 2 1 In\u00a0[13]: Copied! <pre>q5_query = \"\"\"\nWITH ranked_sales AS (\n    SELECT\n        s.customer_id AS customer_id,\n        m.product_name AS product_name,\n        COUNT(m.product_name) AS times_purchased,\n        RANK() OVER (\n            PARTITION BY s.customer_id\n            ORDER BY COUNT(m.product_name) DESC\n        ) AS rank_number\n    FROM\n        dannys_diner.sales AS s\n        LEFT JOIN dannys_diner.menu AS m ON s.product_id = m.product_id\n    GROUP BY\n        s.customer_id, m.product_name\n)\nSELECT\n    customer_id,\n    product_name,\n    times_purchased\nFROM\n    ranked_sales\nWHERE\n    rank_number = 1\nORDER BY\n    customer_id;\n\"\"\"\n\nathena.query(database=database, query=q5_query, ctas_approach=ctas_approach)\n</pre> q5_query = \"\"\" WITH ranked_sales AS (     SELECT         s.customer_id AS customer_id,         m.product_name AS product_name,         COUNT(m.product_name) AS times_purchased,         RANK() OVER (             PARTITION BY s.customer_id             ORDER BY COUNT(m.product_name) DESC         ) AS rank_number     FROM         dannys_diner.sales AS s         LEFT JOIN dannys_diner.menu AS m ON s.product_id = m.product_id     GROUP BY         s.customer_id, m.product_name ) SELECT     customer_id,     product_name,     times_purchased FROM     ranked_sales WHERE     rank_number = 1 ORDER BY     customer_id; \"\"\"  athena.query(database=database, query=q5_query, ctas_approach=ctas_approach) Out[13]: customer_id product_name times_purchased 0 A ramen 3 1 B sushi 2 2 B ramen 2 3 B curry 2 4 C ramen 3 In\u00a0[14]: Copied! <pre>q6_cte_query = \"\"\"\n  SELECT\n    s.customer_id AS customer_id,\n    s.order_date AS order_date,\n    menu.product_name AS product_name,\n    RANK() OVER (\n      PARTITION BY s.customer_id\n      ORDER BY order_date ASC\n    ) AS rank_number\n  FROM\n    dannys_diner.sales AS s\n    LEFT JOIN dannys_diner.members AS memb ON s.customer_id = memb.customer_id\n    LEFT JOIN dannys_diner.menu AS menu ON s.product_id = menu.product_id\n  WHERE\n    order_date &gt;= join_date;\n\"\"\"\n\nathena.query(database=database, query=q6_cte_query, ctas_approach=ctas_approach)\n</pre> q6_cte_query = \"\"\"   SELECT     s.customer_id AS customer_id,     s.order_date AS order_date,     menu.product_name AS product_name,     RANK() OVER (       PARTITION BY s.customer_id       ORDER BY order_date ASC     ) AS rank_number   FROM     dannys_diner.sales AS s     LEFT JOIN dannys_diner.members AS memb ON s.customer_id = memb.customer_id     LEFT JOIN dannys_diner.menu AS menu ON s.product_id = menu.product_id   WHERE     order_date &gt;= join_date; \"\"\"  athena.query(database=database, query=q6_cte_query, ctas_approach=ctas_approach) Out[14]: customer_id order_date product_name rank_number 0 A 2021-01-07 curry 1 1 A 2021-01-10 ramen 2 2 A 2021-01-11 ramen 3 3 A 2021-01-11 ramen 3 4 B 2021-01-11 sushi 1 5 B 2021-01-16 ramen 2 6 B 2021-02-01 ramen 3 In\u00a0[15]: Copied! <pre>q6_query = \"\"\"\nWITH ranked_sales AS (\n  SELECT\n    s.customer_id AS customer_id,\n    s.order_date AS order_date,\n    menu.product_name AS product_name,\n    RANK() OVER (\n      PARTITION BY s.customer_id\n      ORDER BY order_date ASC\n    ) AS rank_number\n  FROM\n    dannys_diner.sales AS s\n    LEFT JOIN dannys_diner.members AS memb ON s.customer_id = memb.customer_id\n    LEFT JOIN dannys_diner.menu AS menu ON s.product_id = menu.product_id\n  WHERE\n    order_date &gt;= join_date\n)\nSELECT\n  customer_id,\n  order_date,\n  product_name\nFROM\n  ranked_sales\nWHERE\n  rank_number = 1;\n\"\"\"\n\nathena.query(database=database, query=q6_query, ctas_approach=ctas_approach)\n</pre> q6_query = \"\"\" WITH ranked_sales AS (   SELECT     s.customer_id AS customer_id,     s.order_date AS order_date,     menu.product_name AS product_name,     RANK() OVER (       PARTITION BY s.customer_id       ORDER BY order_date ASC     ) AS rank_number   FROM     dannys_diner.sales AS s     LEFT JOIN dannys_diner.members AS memb ON s.customer_id = memb.customer_id     LEFT JOIN dannys_diner.menu AS menu ON s.product_id = menu.product_id   WHERE     order_date &gt;= join_date ) SELECT   customer_id,   order_date,   product_name FROM   ranked_sales WHERE   rank_number = 1; \"\"\"  athena.query(database=database, query=q6_query, ctas_approach=ctas_approach) Out[15]: customer_id order_date product_name 0 A 2021-01-07 curry 1 B 2021-01-11 sushi <p>The <code>RANK()</code> function is used to assign a rank to each row within a partition of <code>customer_id</code> based on the <code>order_date</code> in ascending order. The <code>rank_number</code> is used to filter the first item purchased by the customer after they became a member.</p> In\u00a0[16]: Copied! <pre>q7_query = \"\"\"\nWITH ranked_sales AS (\n  SELECT\n    s.customer_id AS customer_id,\n    s.order_date AS order_date,\n    menu.product_name AS product_name,\n    RANK() OVER (\n      PARTITION BY s.customer_id\n      ORDER BY order_date DESC\n    ) AS rank_number\n  FROM\n    dannys_diner.sales AS s\n    LEFT JOIN dannys_diner.members AS memb ON s.customer_id = memb.customer_id\n    LEFT JOIN dannys_diner.menu AS menu ON s.product_id = menu.product_id\n  WHERE\n    order_date &lt; join_date\n)\nSELECT\n  customer_id,\n  order_date,\n  product_name\nFROM\n  ranked_sales\nWHERE\n  rank_number = 1;\n\"\"\"\n\nathena.query(database=database, query=q7_query, ctas_approach=ctas_approach)\n</pre> q7_query = \"\"\" WITH ranked_sales AS (   SELECT     s.customer_id AS customer_id,     s.order_date AS order_date,     menu.product_name AS product_name,     RANK() OVER (       PARTITION BY s.customer_id       ORDER BY order_date DESC     ) AS rank_number   FROM     dannys_diner.sales AS s     LEFT JOIN dannys_diner.members AS memb ON s.customer_id = memb.customer_id     LEFT JOIN dannys_diner.menu AS menu ON s.product_id = menu.product_id   WHERE     order_date &lt; join_date ) SELECT   customer_id,   order_date,   product_name FROM   ranked_sales WHERE   rank_number = 1; \"\"\"  athena.query(database=database, query=q7_query, ctas_approach=ctas_approach) Out[16]: customer_id order_date product_name 0 A 2021-01-01 sushi 1 A 2021-01-01 curry 2 B 2021-01-04 sushi In\u00a0[17]: Copied! <pre>q8_query = \"\"\"\nSELECT\n  s.customer_id AS customer_id,\n  COUNT(DISTINCT menu.product_name) AS item_count,\n  SUM(menu.price) AS total_spending\nFROM \n  dannys_diner.sales AS s \n  LEFT JOIN dannys_diner.members AS memb ON s.customer_id = memb.customer_id \n  LEFT JOIN dannys_diner.menu AS menu ON s.product_id = menu.product_id\nWHERE \n  s.order_date &lt; memb.join_date\nGROUP BY\n  s.customer_id\nORDER BY\n  total_spending DESC;\n\"\"\"\n\nathena.query(database=database, query=q8_query, ctas_approach=ctas_approach)\n</pre> q8_query = \"\"\" SELECT   s.customer_id AS customer_id,   COUNT(DISTINCT menu.product_name) AS item_count,   SUM(menu.price) AS total_spending FROM    dannys_diner.sales AS s    LEFT JOIN dannys_diner.members AS memb ON s.customer_id = memb.customer_id    LEFT JOIN dannys_diner.menu AS menu ON s.product_id = menu.product_id WHERE    s.order_date &lt; memb.join_date GROUP BY   s.customer_id ORDER BY   total_spending DESC; \"\"\"  athena.query(database=database, query=q8_query, ctas_approach=ctas_approach) Out[17]: customer_id item_count total_spending 0 B 2 40.0 1 A 2 25.0 In\u00a0[18]: Copied! <pre>q9_query = \"\"\"\nSELECT\n  s.customer_id,\n  SUM(\n    CASE\n      WHEN menu.product_name = 'sushi' THEN menu.price * 10 * 2\n      ELSE menu.price * 10\n    END\n  ) AS total_points\nFROM\n  dannys_diner.sales AS s\n  LEFT JOIN dannys_diner.members AS memb ON s.customer_id = memb.customer_id\n  LEFT JOIN dannys_diner.menu AS menu ON s.product_id = menu.product_id\nGROUP BY\n  s.customer_id\nORDER BY\n  total_points DESC;\n\"\"\"\n\nathena.query(database=database, query=q9_query, ctas_approach=ctas_approach)\n</pre> q9_query = \"\"\" SELECT   s.customer_id,   SUM(     CASE       WHEN menu.product_name = 'sushi' THEN menu.price * 10 * 2       ELSE menu.price * 10     END   ) AS total_points FROM   dannys_diner.sales AS s   LEFT JOIN dannys_diner.members AS memb ON s.customer_id = memb.customer_id   LEFT JOIN dannys_diner.menu AS menu ON s.product_id = menu.product_id GROUP BY   s.customer_id ORDER BY   total_points DESC; \"\"\"  athena.query(database=database, query=q9_query, ctas_approach=ctas_approach) Out[18]: customer_id total_points 0 B 940.0 1 A 860.0 2 C 360.0 In\u00a0[19]: Copied! <pre>q10_query = \"\"\"\nSELECT\n  s.customer_id AS customer_id,\n  SUM(\n    CASE\n      WHEN s.order_date &lt; memb.join_date\n      AND menu.product_name = 'sushi' THEN menu.price * 10 * 2\n      WHEN s.order_date &gt;= memb.join_date\n      AND DATE_DIFF('day', memb.join_date, s.order_date) &lt;= 6 THEN menu.price * 10 * 2\n      ELSE menu.price * 10\n    END\n  ) AS total_points\nFROM\n  dannys_diner.sales AS s\n  LEFT JOIN dannys_diner.members AS memb ON s.customer_id = memb.customer_id\n  LEFT JOIN dannys_diner.menu AS menu ON s.product_id = menu.product_id\nWHERE\n  s.customer_id IN ('A', 'B')\n  AND s.order_date &lt; DATE '2021-02-01'\nGROUP BY\n  s.customer_id\nORDER BY\n  total_points DESC;\n\"\"\"\n\nathena.query(database=database, query=q10_query, ctas_approach=ctas_approach)\n</pre> q10_query = \"\"\" SELECT   s.customer_id AS customer_id,   SUM(     CASE       WHEN s.order_date &lt; memb.join_date       AND menu.product_name = 'sushi' THEN menu.price * 10 * 2       WHEN s.order_date &gt;= memb.join_date       AND DATE_DIFF('day', memb.join_date, s.order_date) &lt;= 6 THEN menu.price * 10 * 2       ELSE menu.price * 10     END   ) AS total_points FROM   dannys_diner.sales AS s   LEFT JOIN dannys_diner.members AS memb ON s.customer_id = memb.customer_id   LEFT JOIN dannys_diner.menu AS menu ON s.product_id = menu.product_id WHERE   s.customer_id IN ('A', 'B')   AND s.order_date &lt; DATE '2021-02-01' GROUP BY   s.customer_id ORDER BY   total_points DESC; \"\"\"  athena.query(database=database, query=q10_query, ctas_approach=ctas_approach) Out[19]: customer_id total_points 0 A 1370.0 1 B 820.0 <p>The <code>CASE</code> statement is used to calculate the total points for each customer based on the following conditions:</p> <ul> <li><p>If the <code>order_date</code> is before the <code>join_date</code> and the <code>product_name</code> is 'sushi', the customer earns $2x$ points in addition to 10 points per dollar spent.</p> </li> <li><p>If the <code>order_date</code> is within the first week after the <code>join_date</code>, the customer earns $2x$ points on all items.</p> </li> <li><p>Otherwise, the customer earns 10 points per dollar spent.</p> </li> </ul> <p>In addition, we need to use <code>DATE_DIFF()</code> function to calculate the difference between two dates in terms of days. In PostgreSQL, we could simple use the <code>join_date - order_date</code> to get the difference in days; however, this is not supported in Athena.</p> In\u00a0[20]: Copied! <pre>q11_query = \"\"\"\nSELECT\n  s.customer_id AS customer_id,\n  s.order_date AS order_date,\n  menu.product_name AS product_name,\n  menu.price AS price,\n  CASE\n    WHEN s.order_date &gt;= memb.join_date THEN 'Y' ELSE 'N'\n  END AS member\nFROM\n  dannys_diner.sales AS s\n  LEFT JOIN dannys_diner.menu AS menu ON s.product_id = menu.product_id\n  LEFT JOIN dannys_diner.members AS memb ON s.customer_id = memb.customer_id\nORDER BY \n  s.customer_id ASC;\n\"\"\"\n\nathena.query(database=database, query=q11_query, ctas_approach=ctas_approach)\n</pre> q11_query = \"\"\" SELECT   s.customer_id AS customer_id,   s.order_date AS order_date,   menu.product_name AS product_name,   menu.price AS price,   CASE     WHEN s.order_date &gt;= memb.join_date THEN 'Y' ELSE 'N'   END AS member FROM   dannys_diner.sales AS s   LEFT JOIN dannys_diner.menu AS menu ON s.product_id = menu.product_id   LEFT JOIN dannys_diner.members AS memb ON s.customer_id = memb.customer_id ORDER BY    s.customer_id ASC; \"\"\"  athena.query(database=database, query=q11_query, ctas_approach=ctas_approach) Out[20]: customer_id order_date product_name price member 0 A 2021-01-01 sushi 10.0 N 1 A 2021-01-01 curry 15.0 N 2 A 2021-01-07 curry 15.0 Y 3 A 2021-01-10 ramen 12.0 Y 4 A 2021-01-11 ramen 12.0 Y 5 A 2021-01-11 ramen 12.0 Y 6 B 2021-01-01 curry 15.0 N 7 B 2021-01-02 curry 15.0 N 8 B 2021-01-04 sushi 10.0 N 9 B 2021-01-11 sushi 10.0 Y 10 B 2021-01-16 ramen 12.0 Y 11 B 2021-02-01 ramen 12.0 Y 12 C 2021-01-01 ramen 12.0 N 13 C 2021-01-01 ramen 12.0 N 14 C 2021-01-07 ramen 12.0 N <p>It is important to set the catch-all condition <code>WHEN s.order_date &gt;= memb.join_date</code> to ensure that customers who are not members are at the time of the order are correctly classified as non-members, including customers who have not joined the program.</p> In\u00a0[21]: Copied! <pre>q12_query = \"\"\"\nWITH ranked_sales AS (\n  SELECT\n    s.customer_id AS customer_id,\n    s.order_date AS order_date,\n    menu.product_name AS product_name,\n    menu.price AS price,\n    CASE\n      WHEN s.order_date &gt;= memb.join_date THEN 'Y' ELSE 'N'\n    END AS member\n  FROM\n    dannys_diner.sales AS s\n    LEFT JOIN dannys_diner.menu AS menu ON s.product_id = menu.product_id\n    LEFT JOIN dannys_diner.members AS memb ON s.customer_id = memb.customer_id\n)\nSELECT\n  customer_id,\n  order_date,\n  product_name,\n  price,\n  member,\n  CASE\n    WHEN member = 'N' THEN NULL\n    ELSE RANK() OVER (\n      PARTITION BY customer_id, member\n      ORDER BY order_date ASC\n    )\n  END AS ranking\nFROM\n  ranked_sales\nORDER BY \n  customer_id ASC, order_date ASC, price DESC;\n\"\"\"\n\nathena.query(database=database, query=q12_query, ctas_approach=ctas_approach)\n</pre> q12_query = \"\"\" WITH ranked_sales AS (   SELECT     s.customer_id AS customer_id,     s.order_date AS order_date,     menu.product_name AS product_name,     menu.price AS price,     CASE       WHEN s.order_date &gt;= memb.join_date THEN 'Y' ELSE 'N'     END AS member   FROM     dannys_diner.sales AS s     LEFT JOIN dannys_diner.menu AS menu ON s.product_id = menu.product_id     LEFT JOIN dannys_diner.members AS memb ON s.customer_id = memb.customer_id ) SELECT   customer_id,   order_date,   product_name,   price,   member,   CASE     WHEN member = 'N' THEN NULL     ELSE RANK() OVER (       PARTITION BY customer_id, member       ORDER BY order_date ASC     )   END AS ranking FROM   ranked_sales ORDER BY    customer_id ASC, order_date ASC, price DESC; \"\"\"  athena.query(database=database, query=q12_query, ctas_approach=ctas_approach) Out[21]: customer_id order_date product_name price member ranking 0 A 2021-01-01 curry 15.0 N &lt;NA&gt; 1 A 2021-01-01 sushi 10.0 N &lt;NA&gt; 2 A 2021-01-07 curry 15.0 Y 1 3 A 2021-01-10 ramen 12.0 Y 2 4 A 2021-01-11 ramen 12.0 Y 3 5 A 2021-01-11 ramen 12.0 Y 3 6 B 2021-01-01 curry 15.0 N &lt;NA&gt; 7 B 2021-01-02 curry 15.0 N &lt;NA&gt; 8 B 2021-01-04 sushi 10.0 N &lt;NA&gt; 9 B 2021-01-11 sushi 10.0 Y 1 10 B 2021-01-16 ramen 12.0 Y 2 11 B 2021-02-01 ramen 12.0 Y 3 12 C 2021-01-01 ramen 12.0 N &lt;NA&gt; 13 C 2021-01-01 ramen 12.0 N &lt;NA&gt; 14 C 2021-01-07 ramen 12.0 N &lt;NA&gt;"},{"location":"dannys_diner/#global","title":"Global\u00b6","text":""},{"location":"dannys_diner/#context","title":"Context\u00b6","text":"<p>Danny is a passionate lover of Japanese cuisine. At the beginning of 2021, he decided to embark on a bold venture by opening a charming restaurant named Danny\u2019s Diner, specializing in his three favorite dishes: sushi, curry, and ramen.</p> <p>Danny\u2019s Diner is currently in need of assistance to ensure its sustainability. While the restaurant has collected some basic data over the past few months, the team lacks the expertise to leverage this data to improve business operations.</p>"},{"location":"dannys_diner/#problem-statement","title":"Problem Statement\u00b6","text":"<p>Danny seeks to utilize the available data to answer key questions about his customers, focusing on their visiting patterns, spending habits, and favorite menu items. Gaining these insights will enable him to foster a deeper connection with his customers and provide a more personalized dining experience for his loyal patrons.</p> <p>Additionally, Danny intends to use these insights to determine whether to expand the existing customer loyalty program. He also requires assistance in generating basic datasets to allow his team to inspect the data easily without needing to use SQL.</p>"},{"location":"dannys_diner/#entity-relationship-diagram","title":"Entity Relationship Diagram\u00b6","text":""},{"location":"dannys_diner/#tables","title":"Tables\u00b6","text":""},{"location":"dannys_diner/#q1","title":"Q1\u00b6","text":"<p>What is the total amount each customer spent at the restaurant?</p>"},{"location":"dannys_diner/#q2","title":"Q2\u00b6","text":"<p>How many days has each customer visited the restaurant?</p>"},{"location":"dannys_diner/#q3","title":"Q3\u00b6","text":"<p>What was the first item(s) from the menu purchased by each customer?</p>"},{"location":"dannys_diner/#rank-function","title":"RANK() Function\u00b6","text":"<p>The <code>RANK()</code> function is a window function that assigns a rank, $1, ..., n$, to each row within a partition of a result set.</p> <p>The rows within a partition that have the same values will receive the same rank. The next row will receive a rank that is incremented by the number of rows that have the same values, and so the rank may not be consecutive.</p> <pre>RANK() OVER (\n    [PARTITION BY partition_expression, ... ]\n    ORDER BY sort_expression [ASC | DESC], ...\n)\n</pre> <ul> <li><p><code>PARTITION BY</code>: The <code>PARTITION BY</code> clause divides the result set into partitions to which the <code>RANK()</code> function is applied. If the <code>PARTITION BY</code> clause is not specified, the <code>RANK()</code> function treats the whole result set as a single partition.</p> </li> <li><p><code>ORDER BY</code>: The <code>ORDER BY</code> clause specifies the column or columns that the <code>RANK()</code> function uses to determine the order of rows within a partition. If the <code>ORDER BY</code> clause is not specified, the <code>RANK()</code> function assigns a unique rank to each row within a partition.</p> </li> </ul>"},{"location":"dannys_diner/#q4","title":"Q4\u00b6","text":"<p>What is the most purchased item on the menu and how many times was it purchased by all customers?</p>"},{"location":"dannys_diner/#q5","title":"Q5\u00b6","text":"<p>Which item was the most popular for each customer?</p>"},{"location":"dannys_diner/#using-multiple-level-partitioning","title":"Using Multiple Level Partitioning\u00b6","text":"<p>The first approach uses multiple level partitioning to calculate the number of times each customer purchased each product and the total number of times each product was purchased.</p> <ul> <li><p>The first <code>COUNT()</code> function calculates the number of times each customer purchased each product.</p> </li> <li><p>The second <code>COUNT()</code> function calculates the total number of times each product was purchased by all customers.</p> </li> </ul>"},{"location":"dannys_diner/#cte-rank-function","title":"CTE &amp; RANK() Function\u00b6","text":"<p>The second approach uses a Common Table Expression (CTE) to rank the number of times each product was purchased by each customer and filter the most popular product for each customer. This ensures that ties are handled correctly, i.e., if two products are equally popular, both products will be included in the result.</p>"},{"location":"dannys_diner/#q6","title":"Q6\u00b6","text":"<p>Which item was purchased first by the customer after they became a member and what date (including the date they joined) was it?</p>"},{"location":"dannys_diner/#q7","title":"Q7\u00b6","text":"<p>Which item was purchased just before the customer became a member and what date was it?</p> <p>This is the reverse of the previous question. The two differences are:</p> <ul> <li><p>Instead of filtering <code>order_date &gt;= join_date</code>, we filter <code>order_date &lt; join_date</code>.</p> </li> <li><p>The <code>ORDER BY</code> clause in the <code>RANK()</code> function is changed to <code>ORDER BY order_date DESC</code> to rank the <code>order_date</code> in descending order. This ensures that the item purchased just before the customer became a member is ranked first.</p> </li> </ul>"},{"location":"dannys_diner/#q8","title":"Q8\u00b6","text":"<p>What is the total (distinct) items and amount spent for each member before they became a member?</p>"},{"location":"dannys_diner/#q9","title":"Q9\u00b6","text":"<p>If each $\\$1$ spent equates to $10$ points and sushi has a $2x$ points multiplier - how many points would each customer have?</p>"},{"location":"dannys_diner/#q10","title":"Q10\u00b6","text":"<p>In the first week after a customer joins the program (including their join date) they earn $2x$ points on all items, not just sushi - how many points do customer A and B have at the end of January?</p>"},{"location":"dannys_diner/#q11","title":"Q11\u00b6","text":"<p>Create a categorical field <code>Y</code> or <code>N</code> based on if a customer is a member.</p>"},{"location":"dannys_diner/#q12","title":"Q12\u00b6","text":"<p>Danny also requires further information about the ranking of customer products, but he purposely does not need the ranking for non-member purchases so he expects null ranking values for the records when customers are not yet part of the loyalty program.</p>"},{"location":"data_bank/","title":"Data Bank","text":"In\u00a0[1]: Copied! <pre>from IPython.core.interactiveshell import InteractiveShell\n\nInteractiveShell.ast_node_interactivity = \"all\"\n\nimport os\nimport sys\n\nsys.path.append(\"../../../\")\nfrom src.athena import Athena\nfrom src.utils import create_session\n</pre> from IPython.core.interactiveshell import InteractiveShell  InteractiveShell.ast_node_interactivity = \"all\"  import os import sys  sys.path.append(\"../../../\") from src.athena import Athena from src.utils import create_session In\u00a0[54]: Copied! <pre>boto3_session = create_session(\n    profile_name=\"dev\",\n    role_arn=os.getenv(\"ATHENA_IAM_ROLE_ARN\"),\n)\n\nwait = True\nctas_approach = False\n\ndatabase = \"data_bank\"\ntables = [\"regions\", \"customer_nodes\", \"customer_transactions\"]\n\nathena = Athena(boto3_session=boto3_session, s3_output=os.getenv(\"ATHENA_S3_OUTPUT\"))\nathena\n</pre> boto3_session = create_session(     profile_name=\"dev\",     role_arn=os.getenv(\"ATHENA_IAM_ROLE_ARN\"), )  wait = True ctas_approach = False  database = \"data_bank\" tables = [\"regions\", \"customer_nodes\", \"customer_transactions\"]  athena = Athena(boto3_session=boto3_session, s3_output=os.getenv(\"ATHENA_S3_OUTPUT\")) athena Out[54]: <pre>Athena(boto3_session=Session(region_name='us-east-1'), s3_output=s3://sql-case-studies/query_results)</pre> In\u00a0[7]: Copied! <pre>for table in tables:\n    athena.query(\n        database=database,\n        query=f\"\"\" \n                SELECT\n                    *\n                FROM\n                    {database}.{table} TABLESAMPLE BERNOULLI(30);\n              \"\"\",\n        ctas_approach=ctas_approach,\n    )\n</pre> for table in tables:     athena.query(         database=database,         query=f\"\"\"                  SELECT                     *                 FROM                     {database}.{table} TABLESAMPLE BERNOULLI(30);               \"\"\",         ctas_approach=ctas_approach,     ) Out[7]: region_id region_name 0 1 Australia Out[7]: customer_id region_id node_id start_date end_date 0 2 3 5 2020-01-03 2020-01-17 1 4 5 4 2020-01-07 2020-01-19 2 5 3 3 2020-01-15 2020-01-23 3 7 2 5 2020-01-20 2020-02-04 4 20 2 4 2020-01-18 2020-02-09 ... ... ... ... ... ... 1002 491 3 5 2020-05-13 2262-04-11 1003 492 1 4 2020-05-06 2262-04-11 1004 493 4 3 2020-03-19 2262-04-11 1005 495 5 4 2020-05-07 2262-04-11 1006 500 2 2 2020-04-15 2262-04-11 <p>1007 rows \u00d7 5 columns</p> Out[7]: customer_id txn_date txn_type txn_amount 0 1 2020-01-02 deposit 312.0 1 1 2020-03-05 purchase 612.0 2 3 2020-02-22 purchase 965.0 3 3 2020-03-19 withdrawal 188.0 4 4 2020-01-21 deposit 390.0 ... ... ... ... ... 1743 499 2020-03-12 deposit 754.0 1744 500 2020-03-07 purchase 452.0 1745 500 2020-03-01 purchase 929.0 1746 500 2020-01-30 deposit 922.0 1747 500 2020-01-16 deposit 227.0 <p>1748 rows \u00d7 4 columns</p> In\u00a0[63]: Copied! <pre>for table in tables:\n    query = f\"\"\" \n    WITH deduped_data AS (\n      SELECT\n        DISTINCT *\n      FROM\n        data_bank.{table}\n    )\n    SELECT\n      COUNT(*) AS unique_count_{table}\n    FROM\n      deduped_data;\n    \"\"\"\n\n    athena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> for table in tables:     query = f\"\"\"      WITH deduped_data AS (       SELECT         DISTINCT *       FROM         data_bank.{table}     )     SELECT       COUNT(*) AS unique_count_{table}     FROM       deduped_data;     \"\"\"      athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[63]: unique_count_regions 0 5 Out[63]: unique_count_customer_nodes 0 3500 Out[63]: unique_count_customer_transactions 0 5868 In\u00a0[67]: Copied! <pre>query = \"\"\" \nWITH grouped_counts AS (\n  SELECT\n    customer_id,\n    region_id,\n    node_id,\n    start_date,\n    end_date,\n    COUNT(*) AS freq\n  FROM\n    data_bank.customer_nodes\n  GROUP BY\n    customer_id,\n    region_id,\n    node_id,\n    start_date,\n    end_date\n)\nSELECT\n  *\nFROM\n  grouped_counts\nWHERE\n  freq &gt; 1\nORDER BY\n  freq DESC;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  WITH grouped_counts AS (   SELECT     customer_id,     region_id,     node_id,     start_date,     end_date,     COUNT(*) AS freq   FROM     data_bank.customer_nodes   GROUP BY     customer_id,     region_id,     node_id,     start_date,     end_date ) SELECT   * FROM   grouped_counts WHERE   freq &gt; 1 ORDER BY   freq DESC; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[67]: customer_id region_id node_id start_date end_date freq In\u00a0[3]: Copied! <pre>query = \"\"\" \nWITH grouped_counts AS (\n  SELECT\n    customer_id,\n    txn_date,\n    txn_type,\n    txn_amount,\n    COUNT(*) AS freq\n  FROM\n    data_bank.customer_transactions\n  GROUP BY\n    customer_id,\n    txn_date,\n    txn_type,\n    txn_amount\n)\nSELECT\n  *\nFROM\n  grouped_counts\nWHERE\n  freq &gt; 1\nORDER BY\n  freq DESC;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  WITH grouped_counts AS (   SELECT     customer_id,     txn_date,     txn_type,     txn_amount,     COUNT(*) AS freq   FROM     data_bank.customer_transactions   GROUP BY     customer_id,     txn_date,     txn_type,     txn_amount ) SELECT   * FROM   grouped_counts WHERE   freq &gt; 1 ORDER BY   freq DESC; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[3]: customer_id txn_date txn_type txn_amount freq <p>No duplicate are identified in either table.</p> In\u00a0[55]: Copied! <pre>query = \"\"\" \nSELECT\n  r.region_name AS region_name,\n  cn.node_id AS node_id,\n  COUNT(*) AS region_node_count\nFROM\n  data_bank.customer_nodes AS cn LEFT JOIN data_bank.regions AS r ON cn.region_id = r.region_id\nGROUP BY\n  r.region_name, \n  cn.node_id\nORDER BY\n  region_node_count DESC;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  SELECT   r.region_name AS region_name,   cn.node_id AS node_id,   COUNT(*) AS region_node_count FROM   data_bank.customer_nodes AS cn LEFT JOIN data_bank.regions AS r ON cn.region_id = r.region_id GROUP BY   r.region_name,    cn.node_id ORDER BY   region_node_count DESC; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[55]: region_name node_id region_node_count 0 America 1 174 1 Australia 4 168 2 Australia 5 157 3 Australia 1 154 4 America 5 154 5 Asia 3 153 6 Africa 4 151 7 Australia 3 146 8 America 3 146 9 Australia 2 145 10 Africa 5 144 11 Africa 3 144 12 Africa 2 142 13 Asia 1 138 14 America 4 136 15 Africa 1 133 16 Europe 1 129 17 Europe 4 129 18 Asia 2 129 19 Europe 5 127 20 Asia 5 125 21 America 2 125 22 Europe 2 121 23 Asia 4 120 24 Europe 3 110 In\u00a0[43]: Copied! <pre>query = \"\"\" \nSELECT\n  cn.region_id AS region_id,\n  r.region_name AS region_name,\n  COUNT(DISTINCT cn.node_id) AS unique_node_count\nFROM\n  data_bank.customer_nodes AS cn LEFT JOIN data_bank.regions AS r ON cn.region_id = r.region_id\nGROUP BY\n  cn.region_id,\n  r.region_name\nORDER BY\n    unique_node_count DESC;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  SELECT   cn.region_id AS region_id,   r.region_name AS region_name,   COUNT(DISTINCT cn.node_id) AS unique_node_count FROM   data_bank.customer_nodes AS cn LEFT JOIN data_bank.regions AS r ON cn.region_id = r.region_id GROUP BY   cn.region_id,   r.region_name ORDER BY     unique_node_count DESC; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[43]: region_id region_name unique_node_count 0 3 Africa 5 1 2 America 5 2 1 Australia 5 3 4 Asia 5 4 5 Europe 5 In\u00a0[54]: Copied! <pre>query = \"\"\" \nSELECT\n  r.region_name AS region_name,\n  COUNT(DISTINCT cn.customer_id) AS unique_customer_count\nFROM\n  data_bank.customer_nodes AS cn LEFT JOIN data_bank.regions AS r ON cn.region_id = r.region_id\nGROUP BY\n  r.region_name\nORDER BY \n  unique_customer_count DESC;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  SELECT   r.region_name AS region_name,   COUNT(DISTINCT cn.customer_id) AS unique_customer_count FROM   data_bank.customer_nodes AS cn LEFT JOIN data_bank.regions AS r ON cn.region_id = r.region_id GROUP BY   r.region_name ORDER BY    unique_customer_count DESC; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[54]: region_name unique_customer_count 0 Australia 110 1 America 105 2 Africa 102 3 Asia 95 4 Europe 88 In\u00a0[88]: Copied! <pre>query = \"\"\" \nWITH sanitized_dates AS (\n  SELECT\n    node_id,\n    start_date,\n    CASE\n      WHEN end_date &gt; CURRENT_DATE THEN CURRENT_DATE\n      ELSE end_date\n    END AS end_date\n  FROM\n    data_bank.customer_nodes\n)\nSELECT\n  node_id,\n  AVG(DATE_DIFF('day', start_date, end_date)) AS avg_day_at_node,\n  AVG(DATE_DIFF('year', start_date, end_date)) AS avg_year_at_node\nFROM\n  sanitized_dates\nGROUP BY\n  node_id\nORDER BY \n  avg_day_at_node DESC;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  WITH sanitized_dates AS (   SELECT     node_id,     start_date,     CASE       WHEN end_date &gt; CURRENT_DATE THEN CURRENT_DATE       ELSE end_date     END AS end_date   FROM     data_bank.customer_nodes ) SELECT   node_id,   AVG(DATE_DIFF('day', start_date, end_date)) AS avg_day_at_node,   AVG(DATE_DIFF('year', start_date, end_date)) AS avg_year_at_node FROM   sanitized_dates GROUP BY   node_id ORDER BY    avg_day_at_node DESC; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[88]: node_id avg_day_at_node avg_year_at_node 0 3 248.261803 0.606581 1 5 239.937765 0.588402 2 4 239.235795 0.585227 3 2 228.217523 0.555891 4 1 215.340659 0.521978 <p>The CTE replaces all instances of <code>pandas.Timestamp.max = 2262-04-11 23:47:16</code> with the current timestamp:</p> In\u00a0[91]: Copied! <pre>query = \"\"\" \nSELECT\n  node_id,\n  start_date,\n  CASE\n    WHEN end_date &gt; CURRENT_DATE THEN CURRENT_DATE\n    ELSE end_date\n  END AS end_date\nFROM\n    data_bank.customer_nodes;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  SELECT   node_id,   start_date,   CASE     WHEN end_date &gt; CURRENT_DATE THEN CURRENT_DATE     ELSE end_date   END AS end_date FROM     data_bank.customer_nodes; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[91]: node_id start_date end_date 0 4 2020-01-02 2020-01-03 1 5 2020-01-03 2020-01-17 2 4 2020-01-27 2020-02-18 3 4 2020-01-07 2020-01-19 4 3 2020-01-15 2020-01-23 ... ... ... ... 3495 4 2020-02-25 2024-07-17 3496 4 2020-05-27 2024-07-17 3497 2 2020-04-05 2024-07-17 3498 1 2020-02-03 2024-07-17 3499 2 2020-04-15 2024-07-17 <p>3500 rows \u00d7 3 columns</p> In\u00a0[92]: Copied! <pre>query = \"\"\" \nWITH sanitized_dates AS (\n  SELECT\n    r.region_name AS region_name,\n    cn.start_date AS start_date,\n    CASE\n      WHEN cn.end_date &gt; CURRENT_DATE THEN CURRENT_DATE\n      ELSE cn.end_date\n    END AS end_date\n  FROM\n    data_bank.customer_nodes AS cn LEFT JOIN data_bank.regions AS r ON cn.region_id = r.region_id\n)\nSELECT\n  region_name,\n  APPROX_PERCENTILE(DATE_DIFF('day', start_date, end_date), 0.8) AS pct_80_days_at_node,\n  APPROX_PERCENTILE(DATE_DIFF('day', start_date, end_date), 0.95) AS pct_95_days_at_node\nFROM\n  sanitized_dates\nGROUP BY\n  region_name\nORDER BY \n  region_name;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  WITH sanitized_dates AS (   SELECT     r.region_name AS region_name,     cn.start_date AS start_date,     CASE       WHEN cn.end_date &gt; CURRENT_DATE THEN CURRENT_DATE       ELSE cn.end_date     END AS end_date   FROM     data_bank.customer_nodes AS cn LEFT JOIN data_bank.regions AS r ON cn.region_id = r.region_id ) SELECT   region_name,   APPROX_PERCENTILE(DATE_DIFF('day', start_date, end_date), 0.8) AS pct_80_days_at_node,   APPROX_PERCENTILE(DATE_DIFF('day', start_date, end_date), 0.95) AS pct_95_days_at_node FROM   sanitized_dates GROUP BY   region_name ORDER BY    region_name; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[92]: region_name pct_80_days_at_node pct_95_days_at_node 0 Africa 27 1561 1 America 27 1560 2 Asia 27 1564 3 Australia 28 1559 4 Europe 28 1553 In\u00a0[9]: Copied! <pre>query = \"\"\" \nSELECT\n    txn_type AS transaction_type,\n    COUNT(txn_type) AS total_count,\n    SUM(txn_amount) AS total_amount\nFROM\n    data_bank.customer_transactions\nGROUP BY\n    txn_type\nORDER BY\n    total_amount DESC;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  SELECT     txn_type AS transaction_type,     COUNT(txn_type) AS total_count,     SUM(txn_amount) AS total_amount FROM     data_bank.customer_transactions GROUP BY     txn_type ORDER BY     total_amount DESC; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[9]: transaction_type total_count total_amount 0 deposit 2671 1359168.0 1 purchase 1617 806537.0 2 withdrawal 1580 793003.0 In\u00a0[57]: Copied! <pre>query = \"\"\" \nWITH counts_data AS (\n    SELECT\n        COUNT(txn_type) AS deposit_count,\n        SUM(txn_amount) AS deposit_amount\n    FROM\n        data_bank.customer_transactions\n    WHERE\n        txn_type = 'deposit'\n    GROUP BY\n        customer_id\n)\nSELECT\n    ROUND(AVG(deposit_count)) AS avg_deposit_count,\n    ROUND(SUM(deposit_amount) / SUM(deposit_count)) AS avg_deposit_amount\nFROM\n    counts_data;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  WITH counts_data AS (     SELECT         COUNT(txn_type) AS deposit_count,         SUM(txn_amount) AS deposit_amount     FROM         data_bank.customer_transactions     WHERE         txn_type = 'deposit'     GROUP BY         customer_id ) SELECT     ROUND(AVG(deposit_count)) AS avg_deposit_count,     ROUND(SUM(deposit_amount) / SUM(deposit_count)) AS avg_deposit_amount FROM     counts_data; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[57]: avg_deposit_count avg_deposit_amount 0 5.0 509.0 In\u00a0[9]: Copied! <pre>query = \"\"\" \nSELECT\n    customer_id,\n    MONTH(txn_date) AS txn_month,\n    SUM(CASE WHEN txn_type = 'deposit' THEN 1 ELSE 0 END) AS deposit_count,\n    SUM(CASE WHEN txn_type = 'purchase' OR txn_type = 'withdrawal' THEN 1 ELSE 0 END) AS purchase_withdrawal_count\nFROM\n    data_bank.customer_transactions\nGROUP BY\n    customer_id, MONTH(txn_date);\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  SELECT     customer_id,     MONTH(txn_date) AS txn_month,     SUM(CASE WHEN txn_type = 'deposit' THEN 1 ELSE 0 END) AS deposit_count,     SUM(CASE WHEN txn_type = 'purchase' OR txn_type = 'withdrawal' THEN 1 ELSE 0 END) AS purchase_withdrawal_count FROM     data_bank.customer_transactions GROUP BY     customer_id, MONTH(txn_date); \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[9]: customer_id txn_month deposit_count purchase_withdrawal_count 0 7 1 1 0 1 8 1 2 1 2 8 4 0 2 3 10 1 2 4 4 12 1 1 1 ... ... ... ... ... 1715 497 4 1 1 1716 498 2 1 0 1717 498 4 1 0 1718 500 3 3 4 1719 500 2 2 1 <p>1720 rows \u00d7 4 columns</p> <p>Group by <code>txn_month</code> and count the number of unique customers that meet the criteria:</p> <ul> <li><p>More than 1 deposit</p> </li> <li><p>Either 1 purchase or 1 withdrawal</p> </li> </ul> <p><code>WHERE</code> is executed before <code>GROUP BY</code>, which comes before <code>SELECT</code> in the order of operations. The data is first filtered to include only the rows where the customer made more than 1 deposit and either 1 purchase or 1 withdrawal; then the data is grouped by <code>txn_month</code> and the number of unique customers for each month is counted.</p> In\u00a0[7]: Copied! <pre>query = \"\"\" \nWITH counts_data AS (\n    SELECT\n        customer_id,\n        MONTH(txn_date) AS txn_month,\n        SUM(CASE WHEN txn_type = 'deposit' THEN 1 ELSE 0 END) AS deposit_count,\n        SUM(CASE WHEN txn_type = 'purchase' OR txn_type = 'withdrawal' THEN 1 ELSE 0 END) AS purchase_withdrawal_count\n    FROM\n        data_bank.customer_transactions\n    GROUP BY\n        MONTH(txn_date), customer_id\n)\nSELECT\n    txn_month AS month,\n    COUNT(DISTINCT customer_id) AS customer_count\nFROM\n    counts_data\nWHERE\n    deposit_count &gt; 1 \n    AND purchase_withdrawal_count &gt; 0\nGROUP BY\n    txn_month\nORDER BY\n    txn_month;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  WITH counts_data AS (     SELECT         customer_id,         MONTH(txn_date) AS txn_month,         SUM(CASE WHEN txn_type = 'deposit' THEN 1 ELSE 0 END) AS deposit_count,         SUM(CASE WHEN txn_type = 'purchase' OR txn_type = 'withdrawal' THEN 1 ELSE 0 END) AS purchase_withdrawal_count     FROM         data_bank.customer_transactions     GROUP BY         MONTH(txn_date), customer_id ) SELECT     txn_month AS month,     COUNT(DISTINCT customer_id) AS customer_count FROM     counts_data WHERE     deposit_count &gt; 1      AND purchase_withdrawal_count &gt; 0 GROUP BY     txn_month ORDER BY     txn_month; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[7]: month customer_count 0 1 168 1 2 181 2 3 192 3 4 70 In\u00a0[21]: Copied! <pre>query = \"\"\" \nSELECT\n    DISTINCT txn_type\nFROM\n    data_bank.customer_transactions;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  SELECT     DISTINCT txn_type FROM     data_bank.customer_transactions; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[21]: txn_type 0 withdrawal 1 deposit 2 purchase <ul> <li><p>Withdrawal should decrease the balance</p> </li> <li><p>Purchase should decrease the balance</p> </li> <li><p>Deposit should increase the balance</p> </li> </ul> In\u00a0[37]: Copied! <pre>query = \"\"\" \nSELECT\n    customer_id,\n    txn_date,\n    MONTH(txn_date) AS txn_month,\n    CASE \n        WHEN txn_type IS NULL THEN NULL\n        WHEN txn_type = 'deposit' THEN txn_amount \n        ELSE - txn_amount\n    END AS txn_amount_net\nFROM\n    data_bank.customer_transactions\nORDER BY\n    customer_id, txn_month;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  SELECT     customer_id,     txn_date,     MONTH(txn_date) AS txn_month,     CASE          WHEN txn_type IS NULL THEN NULL         WHEN txn_type = 'deposit' THEN txn_amount          ELSE - txn_amount     END AS txn_amount_net FROM     data_bank.customer_transactions ORDER BY     customer_id, txn_month; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[37]: customer_id txn_date txn_month txn_amount_net 0 1 2020-01-02 1 312.0 1 1 2020-03-05 3 -612.0 2 1 2020-03-17 3 324.0 3 1 2020-03-19 3 -664.0 4 2 2020-01-03 1 549.0 ... ... ... ... ... 5863 500 2020-03-22 3 -954.0 5864 500 2020-03-17 3 344.0 5865 500 2020-03-07 3 -452.0 5866 500 2020-03-11 3 -426.0 5867 500 2020-03-01 3 -929.0 <p>5868 rows \u00d7 4 columns</p> <p>We can then use a second CTE to further compute the cumulative sum of the net transaction amounts for each customer and month. This ensures that we take into account the order of the transactions and the fact that the balance at the end of each month is cumulative.</p> <p>We use two window functions:</p> <ul> <li><p><code>SUM()</code> partitions by customer ID and orders by transaction date in ascending order to calculate the cumulative sum of the net transaction amounts for each customer and month</p> </li> <li><p><code>ROW_NUMBER()</code> partitions by customer ID and orders by transaction date in descending order to assign a row number to each transaction for each customer; this allows us to filter for the last transaction of each month</p> </li> </ul> In\u00a0[43]: Copied! <pre>query = \"\"\" \nWITH net_amounts AS (\n    SELECT\n        customer_id,\n        txn_date,\n        MONTH(txn_date) AS txn_month,\n        CASE \n            WHEN txn_type IS NULL THEN NULL\n            WHEN txn_type = 'deposit' THEN txn_amount \n            ELSE - txn_amount\n        END AS txn_amount_net\n    FROM\n        data_bank.customer_transactions\n    ORDER BY\n        customer_id, txn_month\n)\nSELECT\n    customer_id,\n    txn_date,\n    txn_month,\n    txn_amount_net,\n    SUM(txn_amount_net) OVER (\n        PARTITION BY customer_id\n        ORDER BY txn_date ASC\n        ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n    ) AS month_end_balance,\n    ROW_NUMBER() OVER (\n        PARTITION BY customer_id, txn_month\n        ORDER BY txn_date DESC\n    ) AS row_num\nFROM\n    net_amounts\nORDER BY\n    customer_id, txn_month, row_num DESC;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  WITH net_amounts AS (     SELECT         customer_id,         txn_date,         MONTH(txn_date) AS txn_month,         CASE              WHEN txn_type IS NULL THEN NULL             WHEN txn_type = 'deposit' THEN txn_amount              ELSE - txn_amount         END AS txn_amount_net     FROM         data_bank.customer_transactions     ORDER BY         customer_id, txn_month ) SELECT     customer_id,     txn_date,     txn_month,     txn_amount_net,     SUM(txn_amount_net) OVER (         PARTITION BY customer_id         ORDER BY txn_date ASC         ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW     ) AS month_end_balance,     ROW_NUMBER() OVER (         PARTITION BY customer_id, txn_month         ORDER BY txn_date DESC     ) AS row_num FROM     net_amounts ORDER BY     customer_id, txn_month, row_num DESC; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[43]: customer_id txn_date txn_month txn_amount_net month_end_balance row_num 0 1 2020-01-02 1 312.0 312.0 1 1 1 2020-03-05 3 -612.0 -300.0 3 2 1 2020-03-17 3 324.0 24.0 2 3 1 2020-03-19 3 -664.0 -640.0 1 4 2 2020-01-03 1 549.0 549.0 1 ... ... ... ... ... ... ... 5863 500 2020-03-07 3 -452.0 2462.0 5 5864 500 2020-03-11 3 -426.0 2036.0 4 5865 500 2020-03-17 3 344.0 2380.0 3 5866 500 2020-03-22 3 -954.0 1426.0 2 5867 500 2020-03-25 3 825.0 2251.0 1 <p>5868 rows \u00d7 6 columns</p> In\u00a0[44]: Copied! <pre>query = \"\"\" \nWITH net_amounts AS (\n    SELECT\n        customer_id,\n        txn_date,\n        MONTH(txn_date) AS txn_month,\n        CASE \n            WHEN txn_type IS NULL THEN NULL\n            WHEN txn_type = 'deposit' THEN txn_amount \n            ELSE - txn_amount\n        END AS txn_amount_net\n    FROM\n        data_bank.customer_transactions\n    ORDER BY\n        customer_id, txn_month\n),\ncumulative_net_amounts AS (\n    SELECT\n        customer_id,\n        txn_date,\n        txn_month,\n        txn_amount_net,\n        SUM(txn_amount_net) OVER (\n            PARTITION BY customer_id\n            ORDER BY txn_date ASC\n            ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n        ) AS month_end_balance,\n        ROW_NUMBER() OVER (\n            PARTITION BY customer_id, txn_month\n            ORDER BY txn_date DESC\n        ) AS row_num\n    FROM\n        net_amounts\n)\nSELECT\n    customer_id,\n    txn_month,\n    month_end_balance\nFROM\n    cumulative_net_amounts\nWHERE\n    row_num = 1\nORDER BY\n    customer_id, txn_month;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  WITH net_amounts AS (     SELECT         customer_id,         txn_date,         MONTH(txn_date) AS txn_month,         CASE              WHEN txn_type IS NULL THEN NULL             WHEN txn_type = 'deposit' THEN txn_amount              ELSE - txn_amount         END AS txn_amount_net     FROM         data_bank.customer_transactions     ORDER BY         customer_id, txn_month ), cumulative_net_amounts AS (     SELECT         customer_id,         txn_date,         txn_month,         txn_amount_net,         SUM(txn_amount_net) OVER (             PARTITION BY customer_id             ORDER BY txn_date ASC             ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW         ) AS month_end_balance,         ROW_NUMBER() OVER (             PARTITION BY customer_id, txn_month             ORDER BY txn_date DESC         ) AS row_num     FROM         net_amounts ) SELECT     customer_id,     txn_month,     month_end_balance FROM     cumulative_net_amounts WHERE     row_num = 1 ORDER BY     customer_id, txn_month; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[44]: customer_id txn_month month_end_balance 0 1 1 312.0 1 1 3 -640.0 2 2 1 549.0 3 2 3 610.0 4 3 1 144.0 ... ... ... ... 1715 499 2 868.0 1716 499 3 599.0 1717 500 1 1594.0 1718 500 2 2981.0 1719 500 3 2251.0 <p>1720 rows \u00d7 3 columns</p> In\u00a0[128]: Copied! <pre>query = \"\"\" \nSELECT\n    customer_id,\n    DATE_TRUNC('month', txn_date) AS txn_month,\n    SUM(\n        CASE \n            WHEN txn_type IS NULL THEN NULL\n            WHEN txn_type = 'deposit' THEN txn_amount \n            ELSE - txn_amount\n        END\n    ) AS txn_amount_net\nFROM \n    data_bank.customer_transactions\nGROUP BY \n    customer_id, DATE_TRUNC('month', txn_date)\nORDER BY \n    customer_id, DATE_TRUNC('month', txn_date);\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  SELECT     customer_id,     DATE_TRUNC('month', txn_date) AS txn_month,     SUM(         CASE              WHEN txn_type IS NULL THEN NULL             WHEN txn_type = 'deposit' THEN txn_amount              ELSE - txn_amount         END     ) AS txn_amount_net FROM      data_bank.customer_transactions GROUP BY      customer_id, DATE_TRUNC('month', txn_date) ORDER BY      customer_id, DATE_TRUNC('month', txn_date); \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[128]: customer_id txn_month txn_amount_net 0 1 2020-01-01 312.0 1 1 2020-03-01 -952.0 2 2 2020-01-01 549.0 3 2 2020-03-01 61.0 4 3 2020-01-01 144.0 ... ... ... ... 1715 499 2020-02-01 1719.0 1716 499 2020-03-01 -816.0 1717 500 2020-01-01 1594.0 1718 500 2020-02-01 1387.0 1719 500 2020-03-01 -730.0 <p>1720 rows \u00d7 3 columns</p> In\u00a0[129]: Copied! <pre>query = \"\"\" \nSELECT \n    customer_id,\n    DATE_TRUNC('month', MIN(txn_date)) AS min_txn_month\nFROM \n    data_bank.customer_transactions\nGROUP BY \n    customer_id;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  SELECT      customer_id,     DATE_TRUNC('month', MIN(txn_date)) AS min_txn_month FROM      data_bank.customer_transactions GROUP BY      customer_id; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[129]: customer_id min_txn_month 0 3 2020-01-01 1 7 2020-01-01 2 14 2020-01-01 3 19 2020-01-01 4 21 2020-01-01 ... ... ... 495 488 2020-01-01 496 491 2020-01-01 497 492 2020-01-01 498 495 2020-01-01 499 496 2020-01-01 <p>500 rows \u00d7 2 columns</p> In\u00a0[130]: Copied! <pre>query = \"\"\" \nWITH min_txn_dates AS (\n    SELECT \n        customer_id,\n        DATE_TRUNC('month', MIN(txn_date)) AS min_txn_month\n    FROM \n        data_bank.customer_transactions\n    GROUP BY \n        customer_id\n)\nSELECT \n    customer_id, \n    month_offset\nFROM \n    min_txn_dates,\n    UNNEST(SEQUENCE(0, 1)) AS t (month_offset);\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  WITH min_txn_dates AS (     SELECT          customer_id,         DATE_TRUNC('month', MIN(txn_date)) AS min_txn_month     FROM          data_bank.customer_transactions     GROUP BY          customer_id ) SELECT      customer_id,      month_offset FROM      min_txn_dates,     UNNEST(SEQUENCE(0, 1)) AS t (month_offset); \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[130]: customer_id month_offset 0 1 0 1 1 1 2 6 0 3 6 1 4 8 0 ... ... ... 995 481 1 996 483 0 997 483 1 998 493 0 999 493 1 <p>1000 rows \u00d7 2 columns</p> <p>The <code>DATE_ADD(unit, value, timestamp)</code> function adds a specified value to a timestamp, e.g. <code>DATE_ADD('month', 1, '2022-01-01')</code> returns <code>2022-02-01</code>:</p> In\u00a0[131]: Copied! <pre>query = \"\"\" \nWITH min_txn_dates AS (\n    SELECT \n        customer_id,\n        DATE_TRUNC('month', MIN(txn_date)) AS min_txn_month\n    FROM \n        data_bank.customer_transactions\n    GROUP BY \n        customer_id\n)\nSELECT \n    customer_id, \n    DATE_ADD('month', month_offset, min_txn_month) AS txn_month, \n    month_offset + 1 AS month_number\nFROM \n    min_txn_dates,\n    UNNEST(SEQUENCE(0, 1)) AS t (month_offset);\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  WITH min_txn_dates AS (     SELECT          customer_id,         DATE_TRUNC('month', MIN(txn_date)) AS min_txn_month     FROM          data_bank.customer_transactions     GROUP BY          customer_id ) SELECT      customer_id,      DATE_ADD('month', month_offset, min_txn_month) AS txn_month,      month_offset + 1 AS month_number FROM      min_txn_dates,     UNNEST(SEQUENCE(0, 1)) AS t (month_offset); \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[131]: customer_id txn_month month_number 0 3 2020-01-01 1 1 3 2020-02-01 2 2 7 2020-01-01 1 3 7 2020-02-01 2 4 14 2020-01-01 1 ... ... ... ... 995 481 2020-02-01 2 996 483 2020-01-01 1 997 483 2020-02-01 2 998 493 2020-01-01 1 999 493 2020-02-01 2 <p>1000 rows \u00d7 3 columns</p> In\u00a0[133]: Copied! <pre>query = \"\"\" \nWITH monthly_balances_cte AS (\n    SELECT\n        customer_id,\n        DATE_TRUNC('month', txn_date) AS txn_month,\n        SUM(\n            CASE \n                WHEN txn_type IS NULL THEN NULL\n                WHEN txn_type = 'deposit' THEN txn_amount \n                ELSE - txn_amount\n            END\n        ) AS txn_amount_net\n    FROM \n        data_bank.customer_transactions\n    GROUP BY \n        customer_id, DATE_TRUNC('month', txn_date)\n    ORDER BY \n        customer_id, DATE_TRUNC('month', txn_date)\n),\n\nfirst_second_months_cte AS (\n  WITH min_txn_dates AS (\n      SELECT \n          customer_id,\n          DATE_TRUNC('month', MIN(txn_date)) AS min_txn_month\n      FROM \n          data_bank.customer_transactions\n      GROUP BY \n          customer_id\n  )\n  SELECT \n      customer_id, \n      DATE_ADD('month', month_offset, min_txn_month) AS txn_month, \n      month_offset + 1 AS month_number\n  FROM \n      min_txn_dates,\n      UNNEST(SEQUENCE(0, 1)) AS t (month_offset)\n)\nSELECT\n    first_second_months_cte.customer_id,\n    first_second_months_cte.txn_month,\n    first_second_months_cte.month_number,\n    COALESCE(monthly_balances_cte.txn_amount_net, -999) AS second_month_txn_amount_net\nFROM \n    first_second_months_cte \n    LEFT JOIN \n        monthly_balances_cte \n    ON \n        first_second_months_cte.txn_month = monthly_balances_cte.txn_month\n        AND first_second_months_cte.customer_id = monthly_balances_cte.customer_id;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  WITH monthly_balances_cte AS (     SELECT         customer_id,         DATE_TRUNC('month', txn_date) AS txn_month,         SUM(             CASE                  WHEN txn_type IS NULL THEN NULL                 WHEN txn_type = 'deposit' THEN txn_amount                  ELSE - txn_amount             END         ) AS txn_amount_net     FROM          data_bank.customer_transactions     GROUP BY          customer_id, DATE_TRUNC('month', txn_date)     ORDER BY          customer_id, DATE_TRUNC('month', txn_date) ),  first_second_months_cte AS (   WITH min_txn_dates AS (       SELECT            customer_id,           DATE_TRUNC('month', MIN(txn_date)) AS min_txn_month       FROM            data_bank.customer_transactions       GROUP BY            customer_id   )   SELECT        customer_id,        DATE_ADD('month', month_offset, min_txn_month) AS txn_month,        month_offset + 1 AS month_number   FROM        min_txn_dates,       UNNEST(SEQUENCE(0, 1)) AS t (month_offset) ) SELECT     first_second_months_cte.customer_id,     first_second_months_cte.txn_month,     first_second_months_cte.month_number,     COALESCE(monthly_balances_cte.txn_amount_net, -999) AS second_month_txn_amount_net FROM      first_second_months_cte      LEFT JOIN          monthly_balances_cte      ON          first_second_months_cte.txn_month = monthly_balances_cte.txn_month         AND first_second_months_cte.customer_id = monthly_balances_cte.customer_id; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[133]: customer_id txn_month month_number second_month_txn_amount_net 0 2 2020-01-01 1 549.0 1 2 2020-02-01 2 -999.0 2 4 2020-01-01 1 848.0 3 4 2020-02-01 2 -999.0 4 5 2020-01-01 1 954.0 ... ... ... ... ... 995 485 2020-02-01 2 1491.0 996 487 2020-01-01 1 -572.0 997 487 2020-02-01 2 884.0 998 490 2020-01-01 1 271.0 999 490 2020-02-01 2 71.0 <p>1000 rows \u00d7 4 columns</p> In\u00a0[138]: Copied! <pre>query = \"\"\" \nWITH monthly_balances_cte AS (\n    SELECT\n        customer_id,\n        DATE_TRUNC('month', txn_date) AS txn_month,\n        SUM(\n            CASE \n                WHEN txn_type IS NULL THEN NULL\n                WHEN txn_type = 'deposit' THEN txn_amount \n                ELSE - txn_amount\n            END\n        ) AS txn_amount_net\n    FROM \n        data_bank.customer_transactions\n    GROUP BY \n        customer_id, DATE_TRUNC('month', txn_date)\n    ORDER BY \n        customer_id, DATE_TRUNC('month', txn_date)\n),\n\nfirst_second_months_cte AS (\n  WITH min_txn_dates AS (\n      SELECT \n          customer_id,\n          DATE_TRUNC('month', MIN(txn_date)) AS min_txn_month\n      FROM \n          data_bank.customer_transactions\n      GROUP BY \n          customer_id\n  )\n  SELECT \n      customer_id, \n      DATE_ADD('month', month_offset, min_txn_month) AS txn_month, \n      month_offset + 1 AS month_number\n  FROM \n      min_txn_dates,\n      UNNEST(SEQUENCE(0, 1)) AS t (month_offset)\n),\n\nmonthly_transactions_cte AS (\n    SELECT\n        first_second_months_cte.customer_id,\n        first_second_months_cte.txn_month,\n        first_second_months_cte.month_number,\n        COALESCE(monthly_balances_cte.txn_amount_net, -999) AS second_month_txn_amount_net\n    FROM \n        first_second_months_cte \n        LEFT JOIN \n            monthly_balances_cte \n        ON \n            first_second_months_cte.txn_month = monthly_balances_cte.txn_month\n            AND first_second_months_cte.customer_id = monthly_balances_cte.customer_id\n)\nSELECT\n    customer_id,\n    month_number,\n    LAG(second_month_txn_amount_net) OVER (\n        PARTITION BY customer_id\n        ORDER BY txn_month\n    ) AS first_month_txn_amount_net,\n    second_month_txn_amount_net\nFROM \n    monthly_transactions_cte;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  WITH monthly_balances_cte AS (     SELECT         customer_id,         DATE_TRUNC('month', txn_date) AS txn_month,         SUM(             CASE                  WHEN txn_type IS NULL THEN NULL                 WHEN txn_type = 'deposit' THEN txn_amount                  ELSE - txn_amount             END         ) AS txn_amount_net     FROM          data_bank.customer_transactions     GROUP BY          customer_id, DATE_TRUNC('month', txn_date)     ORDER BY          customer_id, DATE_TRUNC('month', txn_date) ),  first_second_months_cte AS (   WITH min_txn_dates AS (       SELECT            customer_id,           DATE_TRUNC('month', MIN(txn_date)) AS min_txn_month       FROM            data_bank.customer_transactions       GROUP BY            customer_id   )   SELECT        customer_id,        DATE_ADD('month', month_offset, min_txn_month) AS txn_month,        month_offset + 1 AS month_number   FROM        min_txn_dates,       UNNEST(SEQUENCE(0, 1)) AS t (month_offset) ),  monthly_transactions_cte AS (     SELECT         first_second_months_cte.customer_id,         first_second_months_cte.txn_month,         first_second_months_cte.month_number,         COALESCE(monthly_balances_cte.txn_amount_net, -999) AS second_month_txn_amount_net     FROM          first_second_months_cte          LEFT JOIN              monthly_balances_cte          ON              first_second_months_cte.txn_month = monthly_balances_cte.txn_month             AND first_second_months_cte.customer_id = monthly_balances_cte.customer_id ) SELECT     customer_id,     month_number,     LAG(second_month_txn_amount_net) OVER (         PARTITION BY customer_id         ORDER BY txn_month     ) AS first_month_txn_amount_net,     second_month_txn_amount_net FROM      monthly_transactions_cte; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[138]: customer_id month_number first_month_txn_amount_net second_month_txn_amount_net 0 2 1 NaN 549.0 1 2 2 549.0 -999.0 2 4 1 NaN 848.0 3 4 2 848.0 -999.0 4 5 1 NaN 954.0 ... ... ... ... ... 995 485 2 16.0 1491.0 996 487 1 NaN -572.0 997 487 2 -572.0 884.0 998 490 1 NaN 271.0 999 490 2 271.0 71.0 <p>1000 rows \u00d7 4 columns</p> In\u00a0[141]: Copied! <pre>query = \"\"\" \nWITH monthly_balances_cte AS (\n    SELECT\n        customer_id,\n        DATE_TRUNC('month', txn_date) AS txn_month,\n        SUM(\n            CASE \n                WHEN txn_type IS NULL THEN NULL\n                WHEN txn_type = 'deposit' THEN txn_amount \n                ELSE -txn_amount\n            END\n        ) AS txn_amount_net\n    FROM \n        data_bank.customer_transactions\n    GROUP BY \n        customer_id, DATE_TRUNC('month', txn_date)\n    ORDER BY \n        customer_id, DATE_TRUNC('month', txn_date)\n),\n\nfirst_second_months_cte AS (\n  WITH min_txn_dates AS (\n      SELECT \n          customer_id,\n          DATE_TRUNC('month', MIN(txn_date)) AS min_txn_month\n      FROM \n          data_bank.customer_transactions\n      GROUP BY \n          customer_id\n  )\n  SELECT \n      customer_id, \n      DATE_ADD('month', month_offset, min_txn_month) AS txn_month, \n      month_offset + 1 AS month_number\n  FROM \n      min_txn_dates,\n      UNNEST(SEQUENCE(0, 1)) AS t (month_offset)\n),\n\nmonthly_transactions_cte AS (\n    SELECT\n        first_second_months_cte.customer_id,\n        first_second_months_cte.txn_month,\n        first_second_months_cte.month_number,\n        COALESCE(monthly_balances_cte.txn_amount_net, -999) AS second_month_txn_amount_net\n    FROM \n        first_second_months_cte \n        LEFT JOIN \n            monthly_balances_cte \n        ON \n            first_second_months_cte.txn_month = monthly_balances_cte.txn_month\n            AND first_second_months_cte.customer_id = monthly_balances_cte.customer_id\n),\n\nfirst_second_month_alignment_cte AS (\n    SELECT\n        customer_id,\n        month_number,\n        LAG(second_month_txn_amount_net) OVER (\n            PARTITION BY customer_id\n            ORDER BY txn_month\n        ) AS first_month_txn_amount_net,\n        second_month_txn_amount_net\n    FROM \n        monthly_transactions_cte\n)\nSELECT\n    COUNT(DISTINCT customer_id) AS customer_count,\n    SUM(CASE WHEN first_month_txn_amount_net &gt; 0 THEN 1 ELSE 0 END) AS positive_first_month_count,\n    SUM(CASE WHEN first_month_txn_amount_net &lt; 0 THEN 1 ELSE 0 END) AS negative_first_month_count,\n    SUM(CASE\n            WHEN first_month_txn_amount_net &gt; 0\n                 AND second_month_txn_amount_net &gt; 0\n                 AND second_month_txn_amount_net &gt; 1.05 * first_month_txn_amount_net\n            THEN 1\n            ELSE 0\n        END\n    ) AS second_month_5_pct_higher_count,\n    SUM(\n        CASE\n            WHEN first_month_txn_amount_net &gt; 0\n                 AND second_month_txn_amount_net &lt; 0\n                 AND second_month_txn_amount_net &lt; 0.95 * first_month_txn_amount_net\n            THEN 1\n            ELSE 0\n        END\n    ) AS second_month_5_pct_lower_count,\n    SUM(\n        CASE\n        WHEN first_month_txn_amount_net &gt; 0\n            AND second_month_txn_amount_net &lt; 0\n            AND second_month_txn_amount_net &lt; -first_month_txn_amount_net\n            THEN 1\n        ELSE 0 END\n    ) AS positive_to_negative_count\nFROM \n    first_second_month_alignment_cte\nWHERE \n    first_month_txn_amount_net IS NOT NULL\n    AND second_month_txn_amount_net &lt;&gt; -999;\n\"\"\"\n\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  WITH monthly_balances_cte AS (     SELECT         customer_id,         DATE_TRUNC('month', txn_date) AS txn_month,         SUM(             CASE                  WHEN txn_type IS NULL THEN NULL                 WHEN txn_type = 'deposit' THEN txn_amount                  ELSE -txn_amount             END         ) AS txn_amount_net     FROM          data_bank.customer_transactions     GROUP BY          customer_id, DATE_TRUNC('month', txn_date)     ORDER BY          customer_id, DATE_TRUNC('month', txn_date) ),  first_second_months_cte AS (   WITH min_txn_dates AS (       SELECT            customer_id,           DATE_TRUNC('month', MIN(txn_date)) AS min_txn_month       FROM            data_bank.customer_transactions       GROUP BY            customer_id   )   SELECT        customer_id,        DATE_ADD('month', month_offset, min_txn_month) AS txn_month,        month_offset + 1 AS month_number   FROM        min_txn_dates,       UNNEST(SEQUENCE(0, 1)) AS t (month_offset) ),  monthly_transactions_cte AS (     SELECT         first_second_months_cte.customer_id,         first_second_months_cte.txn_month,         first_second_months_cte.month_number,         COALESCE(monthly_balances_cte.txn_amount_net, -999) AS second_month_txn_amount_net     FROM          first_second_months_cte          LEFT JOIN              monthly_balances_cte          ON              first_second_months_cte.txn_month = monthly_balances_cte.txn_month             AND first_second_months_cte.customer_id = monthly_balances_cte.customer_id ),  first_second_month_alignment_cte AS (     SELECT         customer_id,         month_number,         LAG(second_month_txn_amount_net) OVER (             PARTITION BY customer_id             ORDER BY txn_month         ) AS first_month_txn_amount_net,         second_month_txn_amount_net     FROM          monthly_transactions_cte ) SELECT     COUNT(DISTINCT customer_id) AS customer_count,     SUM(CASE WHEN first_month_txn_amount_net &gt; 0 THEN 1 ELSE 0 END) AS positive_first_month_count,     SUM(CASE WHEN first_month_txn_amount_net &lt; 0 THEN 1 ELSE 0 END) AS negative_first_month_count,     SUM(CASE             WHEN first_month_txn_amount_net &gt; 0                  AND second_month_txn_amount_net &gt; 0                  AND second_month_txn_amount_net &gt; 1.05 * first_month_txn_amount_net             THEN 1             ELSE 0         END     ) AS second_month_5_pct_higher_count,     SUM(         CASE             WHEN first_month_txn_amount_net &gt; 0                  AND second_month_txn_amount_net &lt; 0                  AND second_month_txn_amount_net &lt; 0.95 * first_month_txn_amount_net             THEN 1             ELSE 0         END     ) AS second_month_5_pct_lower_count,     SUM(         CASE         WHEN first_month_txn_amount_net &gt; 0             AND second_month_txn_amount_net &lt; 0             AND second_month_txn_amount_net &lt; -first_month_txn_amount_net             THEN 1         ELSE 0 END     ) AS positive_to_negative_count FROM      first_second_month_alignment_cte WHERE      first_month_txn_amount_net IS NOT NULL     AND second_month_txn_amount_net &lt;&gt; -999; \"\"\"   athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[141]: customer_count positive_first_month_count negative_first_month_count second_month_5_pct_higher_count second_month_5_pct_lower_count positive_to_negative_count 0 455 304 151 63 175 114 In\u00a0[143]: Copied! <pre>query = \"\"\" \nWITH monthly_balances_cte AS (\n    SELECT\n        customer_id,\n        DATE_TRUNC('month', txn_date) AS txn_month,\n        SUM(\n            CASE \n                WHEN txn_type IS NULL THEN NULL\n                WHEN txn_type = 'deposit' THEN txn_amount \n                ELSE -txn_amount\n            END\n        ) AS txn_amount_net\n    FROM \n        data_bank.customer_transactions\n    GROUP BY \n        customer_id, DATE_TRUNC('month', txn_date)\n    ORDER BY \n        customer_id, DATE_TRUNC('month', txn_date)\n),\n\nfirst_second_months_cte AS (\n  WITH min_txn_dates AS (\n      SELECT \n          customer_id,\n          DATE_TRUNC('month', MIN(txn_date)) AS min_txn_month\n      FROM \n          data_bank.customer_transactions\n      GROUP BY \n          customer_id\n  )\n  SELECT \n      customer_id, \n      DATE_ADD('month', month_offset, min_txn_month) AS txn_month, \n      month_offset + 1 AS month_number\n  FROM \n      min_txn_dates,\n      UNNEST(SEQUENCE(0, 1)) AS t (month_offset)\n),\n\nmonthly_transactions_cte AS (\n    SELECT\n        first_second_months_cte.customer_id,\n        first_second_months_cte.txn_month,\n        first_second_months_cte.month_number,\n        COALESCE(monthly_balances_cte.txn_amount_net, -999) AS second_month_txn_amount_net\n    FROM \n        first_second_months_cte \n        LEFT JOIN \n            monthly_balances_cte \n        ON \n            first_second_months_cte.txn_month = monthly_balances_cte.txn_month\n            AND first_second_months_cte.customer_id = monthly_balances_cte.customer_id\n),\n\nfirst_second_month_alignment_cte AS (\n    SELECT\n        customer_id,\n        month_number,\n        LAG(second_month_txn_amount_net) OVER (\n            PARTITION BY customer_id\n            ORDER BY txn_month\n        ) AS first_month_txn_amount_net,\n        second_month_txn_amount_net\n    FROM \n        monthly_transactions_cte\n),\n\ncalculations_cte AS (\n    SELECT\n        COUNT(DISTINCT customer_id) AS customer_count,\n        SUM(CASE WHEN first_month_txn_amount_net &gt; 0 THEN 1 ELSE 0 END) AS positive_first_month_count,\n        SUM(CASE WHEN first_month_txn_amount_net &lt; 0 THEN 1 ELSE 0 END) AS negative_first_month_count,\n        SUM(CASE\n                WHEN first_month_txn_amount_net &gt; 0\n                    AND second_month_txn_amount_net &gt; 0\n                    AND second_month_txn_amount_net &gt; 1.05 * first_month_txn_amount_net\n                THEN 1\n                ELSE 0\n            END\n        ) AS second_month_5_pct_higher_count,\n        SUM(\n            CASE\n                WHEN first_month_txn_amount_net &gt; 0\n                    AND second_month_txn_amount_net &lt; 0\n                    AND second_month_txn_amount_net &lt; 0.95 * first_month_txn_amount_net\n                THEN 1\n                ELSE 0\n            END\n        ) AS second_month_5_pct_lower_count,\n        SUM(\n            CASE\n            WHEN first_month_txn_amount_net &gt; 0\n                AND second_month_txn_amount_net &lt; 0\n                AND second_month_txn_amount_net &lt; -first_month_txn_amount_net\n                THEN 1\n            ELSE 0 END\n        ) AS positive_to_negative_count\n    FROM \n        first_second_month_alignment_cte\n    WHERE \n        first_month_txn_amount_net IS NOT NULL\n        AND second_month_txn_amount_net &lt;&gt; -999\n)\nSELECT\n  ROUND(100 * positive_first_month_count / customer_count, 2) AS positive_pct,\n  ROUND(100 * negative_first_month_count / customer_count, 2) AS negative_pct,\n  ROUND(100 * second_month_5_pct_higher_count / positive_first_month_count, 4) AS second_month_5_pct_higher_pct,\n  ROUND(100 * second_month_5_pct_lower_count / positive_first_month_count, 4) AS second_month_5_pct_lower_pct,\n  ROUND(100 * positive_to_negative_count / positive_first_month_count, 4) AS positive_to_negative_pct\nFROM \n  calculations_cte;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  WITH monthly_balances_cte AS (     SELECT         customer_id,         DATE_TRUNC('month', txn_date) AS txn_month,         SUM(             CASE                  WHEN txn_type IS NULL THEN NULL                 WHEN txn_type = 'deposit' THEN txn_amount                  ELSE -txn_amount             END         ) AS txn_amount_net     FROM          data_bank.customer_transactions     GROUP BY          customer_id, DATE_TRUNC('month', txn_date)     ORDER BY          customer_id, DATE_TRUNC('month', txn_date) ),  first_second_months_cte AS (   WITH min_txn_dates AS (       SELECT            customer_id,           DATE_TRUNC('month', MIN(txn_date)) AS min_txn_month       FROM            data_bank.customer_transactions       GROUP BY            customer_id   )   SELECT        customer_id,        DATE_ADD('month', month_offset, min_txn_month) AS txn_month,        month_offset + 1 AS month_number   FROM        min_txn_dates,       UNNEST(SEQUENCE(0, 1)) AS t (month_offset) ),  monthly_transactions_cte AS (     SELECT         first_second_months_cte.customer_id,         first_second_months_cte.txn_month,         first_second_months_cte.month_number,         COALESCE(monthly_balances_cte.txn_amount_net, -999) AS second_month_txn_amount_net     FROM          first_second_months_cte          LEFT JOIN              monthly_balances_cte          ON              first_second_months_cte.txn_month = monthly_balances_cte.txn_month             AND first_second_months_cte.customer_id = monthly_balances_cte.customer_id ),  first_second_month_alignment_cte AS (     SELECT         customer_id,         month_number,         LAG(second_month_txn_amount_net) OVER (             PARTITION BY customer_id             ORDER BY txn_month         ) AS first_month_txn_amount_net,         second_month_txn_amount_net     FROM          monthly_transactions_cte ),  calculations_cte AS (     SELECT         COUNT(DISTINCT customer_id) AS customer_count,         SUM(CASE WHEN first_month_txn_amount_net &gt; 0 THEN 1 ELSE 0 END) AS positive_first_month_count,         SUM(CASE WHEN first_month_txn_amount_net &lt; 0 THEN 1 ELSE 0 END) AS negative_first_month_count,         SUM(CASE                 WHEN first_month_txn_amount_net &gt; 0                     AND second_month_txn_amount_net &gt; 0                     AND second_month_txn_amount_net &gt; 1.05 * first_month_txn_amount_net                 THEN 1                 ELSE 0             END         ) AS second_month_5_pct_higher_count,         SUM(             CASE                 WHEN first_month_txn_amount_net &gt; 0                     AND second_month_txn_amount_net &lt; 0                     AND second_month_txn_amount_net &lt; 0.95 * first_month_txn_amount_net                 THEN 1                 ELSE 0             END         ) AS second_month_5_pct_lower_count,         SUM(             CASE             WHEN first_month_txn_amount_net &gt; 0                 AND second_month_txn_amount_net &lt; 0                 AND second_month_txn_amount_net &lt; -first_month_txn_amount_net                 THEN 1             ELSE 0 END         ) AS positive_to_negative_count     FROM          first_second_month_alignment_cte     WHERE          first_month_txn_amount_net IS NOT NULL         AND second_month_txn_amount_net &lt;&gt; -999 ) SELECT   ROUND(100 * positive_first_month_count / customer_count, 2) AS positive_pct,   ROUND(100 * negative_first_month_count / customer_count, 2) AS negative_pct,   ROUND(100 * second_month_5_pct_higher_count / positive_first_month_count, 4) AS second_month_5_pct_higher_pct,   ROUND(100 * second_month_5_pct_lower_count / positive_first_month_count, 4) AS second_month_5_pct_lower_pct,   ROUND(100 * positive_to_negative_count / positive_first_month_count, 4) AS positive_to_negative_pct FROM    calculations_cte; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[143]: positive_pct negative_pct second_month_5_pct_higher_pct second_month_5_pct_lower_pct positive_to_negative_pct 0 66 33 20 57 37"},{"location":"data_bank/#global","title":"Global\u00b6","text":""},{"location":"data_bank/#problem-statement","title":"Problem Statement\u00b6","text":"<p>The financial industry is witnessing a transformative innovation with the advent of Neo-Banks: modern, digital-only banks that operate without physical branches.</p> <p>Recognizing a unique opportunity at the intersection of digital banking, cryptocurrency, and data management, Danny has launched an ambitious initiative called Data Bank.</p> <p>Data Bank functions as a conventional digital bank but distinguishes itself by offering the world's most secure distributed data storage platform alongside typical banking activities.</p> <p>In this innovative model, customers are allocated cloud data storage limits directly proportional to their account balances. This novel approach presents several intriguing challenges and opportunities.</p> <p>The management team at Data Bank seeks to expand their customer base while accurately tracking and predicting data storage requirements. They require assistance in navigating these complexities to ensure the initiative's success.</p>"},{"location":"data_bank/#entity-relationship-diagram","title":"Entity Relationship Diagram\u00b6","text":""},{"location":"data_bank/#regions","title":"Regions\u00b6","text":"<p>The regions dimension table contains the regions in which the network nodes are located.</p>"},{"location":"data_bank/#customer-nodes","title":"Customer Nodes\u00b6","text":"<p>Customers are randomly distributed across the nodes within their regions. This is the fact table containing the customer, region, and node ID's as well as start and end dates for each customer's allocation to a node.</p>"},{"location":"data_bank/#customer-transactions","title":"Customer Transactions\u00b6","text":"<p>The customer transactions table contains the transactions for each customer. The transactions are linked to the customer nodes table by the customer ID.</p>"},{"location":"data_bank/#tables","title":"Tables\u00b6","text":""},{"location":"data_bank/#duplicates","title":"Duplicates\u00b6","text":""},{"location":"data_bank/#using-cte","title":"Using CTE\u00b6","text":"<p>Count the number of distinct rows in the tables:</p>"},{"location":"data_bank/#grouping-by-all-columns-counting","title":"Grouping By All Columns &amp; Counting\u00b6","text":""},{"location":"data_bank/#customer-nodes","title":"Customer Nodes\u00b6","text":""},{"location":"data_bank/#customer-transactions","title":"Customer Transactions\u00b6","text":""},{"location":"data_bank/#customer-nodes-exploration","title":"Customer Nodes Exploration\u00b6","text":""},{"location":"data_bank/#q1","title":"Q1\u00b6","text":"<p>How many unique nodes are there on the Data Bank system?</p> <p>Because each region has a unique set of nodes, we can count unique combinations of region ID and node ID to get the total number of unique nodes across all regions.</p>"},{"location":"data_bank/#q2","title":"Q2\u00b6","text":"<p>What is the number of nodes per region?</p>"},{"location":"data_bank/#q3","title":"Q3\u00b6","text":"<p>How many customers are allocated to each region?</p>"},{"location":"data_bank/#q4","title":"Q4\u00b6","text":"<p>How many days on average are customers reallocated to a different node?</p>"},{"location":"data_bank/#q5","title":"Q5\u00b6","text":"<p>What is the median, 80th and 95th percentile for this same reallocation days metric for each region?</p>"},{"location":"data_bank/#customer-transactions","title":"Customer Transactions\u00b6","text":""},{"location":"data_bank/#q1","title":"Q1\u00b6","text":"<p>What is the unique count and total amount for each transaction type?</p>"},{"location":"data_bank/#q2","title":"Q2\u00b6","text":"<p>What is the average total historical deposit counts and amounts for all customers?</p>"},{"location":"data_bank/#q3","title":"Q3\u00b6","text":"<p>For each month - how many Data Bank customers make more than 1 deposit and either 1 purchase or 1 withdrawal within that month?</p> <p>For each customer and month combination, we can count the number of deposits, purchases, and withdrawals using <code>CASE</code> statements:</p>"},{"location":"data_bank/#q4","title":"Q4\u00b6","text":"<p>What is the closing balance for each customer at the end of the month?</p> <p>First, check the unique transaction types:</p>"},{"location":"data_bank/#example-calculation","title":"Example Calculation\u00b6","text":"<p>For customer with ID 1, we can see that the closing balance for each month is calculated correctly.</p> <ul> <li><p>The closing balance for January is $\\$312$</p> </li> <li><p>The closing balance for February is $\\$-640$</p> <ul> <li>$312 - 612 + 324 - 664 = -640$</li> </ul> </li> </ul> <p>The final step is to filter for the last transaction of each month:</p>"},{"location":"data_bank/#q5","title":"Q5\u00b6","text":"<p>Compare the closing balance of a customer\u2019s first month and the closing balance from their second month.</p> <ul> <li><p>Percentages of customers with a negative and positive first month balance</p> </li> <li><p>Percentage of customers who increased their opening month\u2019s positive closing balance by more than $5\\%$ in the following month</p> </li> <li><p>Percentage of customers who reduced their opening month\u2019s positive closing balance by more than $5\\%$ in the following month</p> </li> <li><p>Percentage of customers who move from a positive balance in the first month to a negative balance in the second month</p> </li> </ul>"},{"location":"data_bank/#1-monthly-balance-cte","title":"1. Monthly Balance CTE\u00b6","text":"<p>The first CTE computes the net transaction amount for each customer and month:</p>"},{"location":"data_bank/#2-first-and-second-month-for-each-customer-cte","title":"2. First and Second Month For Each Customer CTE\u00b6","text":""},{"location":"data_bank/#minimum-month-for-each-customer","title":"Minimum Month For Each Customer\u00b6","text":""},{"location":"data_bank/#first-and-second-month-via-cross-join-with-generated-sequence","title":"First and Second Month Via Cross Join With Generated Sequence\u00b6","text":"<p>The <code>CROSS JOIN</code> creates a Cartesian product of each row in <code>min_txn_dates</code>, i.e., the minimum month for each customer, with the generated sequences 0 and 1 <code>UNNEST(SEQUENCE(0, 1))</code></p>"},{"location":"data_bank/#3-monthly-transactions-cte","title":"3. Monthly Transactions CTE\u00b6","text":"<p>Left join the <code>monthly_balance_cte</code> CTE onto the <code>generated_months_cte</code> CTE to get the net transaction amount for each customer for the first and second month.</p> <p>Note that <code>COALESCE()</code> is used to replace <code>NULL</code> values with -999.</p>"},{"location":"data_bank/#4-monthly-aggregates-cte","title":"4. Monthly Aggregates CTE\u00b6","text":"<p>This CTE uses the <code>LAG()</code> window function to create a new column, <code>first_month_txn_amount_net</code>, which contains the net transaction amount of the previous month for each customer. By doing this, it aligns the transaction amounts of consecutive months in the same row across two columns, allowing us to compare the balance of the current month with the balance of the previous month for each customer.</p>"},{"location":"data_bank/#5-final-calculations","title":"5. Final Calculations\u00b6","text":"<p>The following calculations are performed:</p> <ol> <li>Total count of unique customers without missing balance in the second month after the first month, i.e., <code>second_month_txn_amount_net &lt;&gt; -999</code>.</li> </ol> <pre>COUNT(DISTINCT customer_id) AS customer_count\n</pre> <ol> <li>Total count customers with positive and negative balance in the first month:</li> </ol> <pre>SUM(CASE WHEN first_month_txn_amount_net &gt; 0 THEN 1 ELSE 0 END) AS positive_first_month_count\nSUM(CASE WHEN first_month_txn_amount_net &lt; 0 THEN 1 ELSE 0 END) AS negative_first_month_count\n</pre> <ol> <li>Total count of customers with positive balance in the first month and increased or decreased by more than $5\\%$ in the second month:</li> </ol> <pre>SUM(CASE\n        WHEN first_month_txn_amount_net &gt; 0\n                AND second_month_txn_amount_net &gt; 0\n                AND second_month_txn_amount_net &gt; 1.05 * first_month_txn_amount_net\n        THEN 1\n        ELSE 0\n    END\n) AS second_month_5_pct_higher_count\n\nSUM(\n    CASE\n        WHEN first_month_txn_amount_net &gt; 0\n                AND second_month_txn_amount_net &lt; 0\n                AND second_month_txn_amount_net &lt; 0.95 * first_month_txn_amount_net\n        THEN 1\n        ELSE 0\n    END\n) AS second_month_5_pct_lower_count\n</pre> <ol> <li>Total count of customers with positive balance in the first month and negative balance in the second month:</li> </ol> <pre>SUM(\n    CASE\n    WHEN first_month_txn_amount_net &gt; 0\n        AND second_month_txn_amount_net &lt; 0\n        AND second_month_txn_amount_net &lt; -first_month_txn_amount_net\n        THEN 1\n    ELSE 0 END\n) AS positive_to_negative_count\n</pre>"},{"location":"data_bank/#putting-it-all-together","title":"Putting It All Together\u00b6","text":""},{"location":"data_mart/","title":"Data Mart","text":"In\u00a0[296]: Copied! <pre>from IPython.core.interactiveshell import InteractiveShell\n\nInteractiveShell.ast_node_interactivity = \"all\"\n\nimport os\nimport sys\nfrom itertools import product\nfrom typing import List\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nsys.path.append(\"../../../\")\nfrom src.athena import Athena\nfrom src.utils import create_session\n</pre> from IPython.core.interactiveshell import InteractiveShell  InteractiveShell.ast_node_interactivity = \"all\"  import os import sys from itertools import product from typing import List  import matplotlib.pyplot as plt import pandas as pd  sys.path.append(\"../../../\") from src.athena import Athena from src.utils import create_session In\u00a0[297]: Copied! <pre>boto3_session = create_session(\n    profile_name=\"dev\",\n    role_arn=os.getenv(\"ATHENA_IAM_ROLE_ARN\"),\n)\n\nwait = True\nctas_approach = False\nstorage_format = \"PARQUET\"\nwrite_compression = \"SNAPPY\"\n\ndatabase = \"data_mart\"\ntables = [\"weekly_sales\"]\nsql_path = \"../sql/\"\n\nathena = Athena(boto3_session=boto3_session, s3_output=os.getenv(\"ATHENA_S3_OUTPUT\"))\nathena\n</pre> boto3_session = create_session(     profile_name=\"dev\",     role_arn=os.getenv(\"ATHENA_IAM_ROLE_ARN\"), )  wait = True ctas_approach = False storage_format = \"PARQUET\" write_compression = \"SNAPPY\"  database = \"data_mart\" tables = [\"weekly_sales\"] sql_path = \"../sql/\"  athena = Athena(boto3_session=boto3_session, s3_output=os.getenv(\"ATHENA_S3_OUTPUT\")) athena Out[297]: <pre>Athena(boto3_session=Session(region_name='us-east-1'), s3_output=s3://sql-case-studies/query_results)</pre> In\u00a0[3]: Copied! <pre>for table in tables:\n    athena.query(\n        database=database,\n        query=f\"\"\" \n                SELECT\n                    *\n                FROM\n                    {database}.{table} TABLESAMPLE BERNOULLI(30);\n              \"\"\",\n        ctas_approach=ctas_approach,\n    )\n</pre> for table in tables:     athena.query(         database=database,         query=f\"\"\"                  SELECT                     *                 FROM                     {database}.{table} TABLESAMPLE BERNOULLI(30);               \"\"\",         ctas_approach=ctas_approach,     ) Out[3]: week_date region platform segment customer_type transactions sales 0 2020-08-31 ASIA Retail C3 New 120631 3656163.0 1 2020-08-31 ASIA Retail F1 New 31574 996575.0 2 2020-08-31 USA Retail &lt;NA&gt; Guest 529151 16509610.0 3 2020-08-31 USA Shopify F1 Existing 1398 260773.0 4 2020-08-31 OCEANIA Shopify C2 Existing 4661 882690.0 ... ... ... ... ... ... ... ... 5060 2018-03-26 CANADA Retail &lt;NA&gt; Guest 456449 12133134.0 5061 2018-03-26 CANADA Shopify F1 Existing 371 68811.0 5062 2018-03-26 AFRICA Retail C2 Existing 82495 3886146.0 5063 2018-03-26 EUROPE Shopify F3 New 2 300.0 5064 2018-03-26 USA Shopify C4 New 16 2784.0 <p>5065 rows \u00d7 7 columns</p> In\u00a0[4]: Copied! <pre>for table in tables:\n    query = f\"\"\"\n    WITH deduped_data AS (\n        SELECT\n            DISTINCT *\n        FROM\n            {database}.{table}\n    )\n    SELECT\n        COUNT(*) AS unique_count_{table}\n    FROM\n        deduped_data;\n    \"\"\"\n    athena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> for table in tables:     query = f\"\"\"     WITH deduped_data AS (         SELECT             DISTINCT *         FROM             {database}.{table}     )     SELECT         COUNT(*) AS unique_count_{table}     FROM         deduped_data;     \"\"\"     athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[4]: unique_count_weekly_sales 0 17117 In\u00a0[5]: Copied! <pre>query = \"\"\" \nWITH grouped_counts AS (\n    SELECT\n        week_date,\n        region,\n        platform,\n        segment,\n        customer_type,\n        transactions,\n        sales,\n        COUNT(*) AS freq\n    FROM\n        data_mart.weekly_sales\n    GROUP BY\n        week_date,\n        region,\n        platform,\n        segment,\n        customer_type,\n        transactions,\n        sales\n)\nSELECT\n    *\nFROM\n    grouped_counts\nWHERE\n    freq &gt; 1\nORDER BY\n    freq DESC;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  WITH grouped_counts AS (     SELECT         week_date,         region,         platform,         segment,         customer_type,         transactions,         sales,         COUNT(*) AS freq     FROM         data_mart.weekly_sales     GROUP BY         week_date,         region,         platform,         segment,         customer_type,         transactions,         sales ) SELECT     * FROM     grouped_counts WHERE     freq &gt; 1 ORDER BY     freq DESC; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[5]: week_date region platform segment customer_type transactions sales freq In\u00a0[10]: Copied! <pre>query = \"\"\"\n    SELECT \n        column_name \n    FROM \n        information_schema.columns \n    WHERE \n        1 = 1\n        AND table_name = 'weekly_sales' \n        AND table_schema = 'data_mart';\n\"\"\"\n\ncolumn_names = athena.query(\n    database=database, query=query, ctas_approach=ctas_approach\n).values.flatten()\ncolumn_names\n</pre> query = \"\"\"     SELECT          column_name      FROM          information_schema.columns      WHERE          1 = 1         AND table_name = 'weekly_sales'          AND table_schema = 'data_mart'; \"\"\"  column_names = athena.query(     database=database, query=query, ctas_approach=ctas_approach ).values.flatten() column_names Out[10]: <pre>array(['week_date', 'region', 'platform', 'segment', 'customer_type',\n       'transactions', 'sales'], dtype=object)</pre> In\u00a0[12]: Copied! <pre>select_clauses = []\nfor col in column_names:\n    select_clauses.append(\n        f\"'{col}' AS column_name, \"\n        f\"SUM(CASE WHEN {col} IS NULL THEN 1 ELSE 0 END) AS null_count, \"\n        f\"ROUND(100.0 * SUM(CASE WHEN {col} IS NULL THEN 1 ELSE 0 END) / COUNT(*), 2) AS null_percentage\"\n    )\n\nselect_query = \" UNION ALL \".join(\n    [f\"SELECT {clause} FROM data_mart.weekly_sales\" for clause in select_clauses]\n)\n\nquery = f\"\"\"{select_query};\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> select_clauses = [] for col in column_names:     select_clauses.append(         f\"'{col}' AS column_name, \"         f\"SUM(CASE WHEN {col} IS NULL THEN 1 ELSE 0 END) AS null_count, \"         f\"ROUND(100.0 * SUM(CASE WHEN {col} IS NULL THEN 1 ELSE 0 END) / COUNT(*), 2) AS null_percentage\"     )  select_query = \" UNION ALL \".join(     [f\"SELECT {clause} FROM data_mart.weekly_sales\" for clause in select_clauses] )  query = f\"\"\"{select_query};\"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[12]: column_name null_count null_percentage 0 transactions 0 0.00 1 region 0 0.00 2 week_date 0 0.00 3 platform 0 0.00 4 segment 3024 17.67 5 customer_type 0 0.00 6 sales 0 0.00 In\u00a0[13]: Copied! <pre>categorical_cols = [\"region\", \"platform\", \"segment\", \"customer_type\"]\nfor col in categorical_cols:\n    query = f\"\"\" \n    SELECT\n        DISTINCT {col}  \n    FROM\n        data_mart.weekly_sales;\n    \"\"\"\n    athena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> categorical_cols = [\"region\", \"platform\", \"segment\", \"customer_type\"] for col in categorical_cols:     query = f\"\"\"      SELECT         DISTINCT {col}       FROM         data_mart.weekly_sales;     \"\"\"     athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[13]: region 0 USA 1 EUROPE 2 AFRICA 3 SOUTH AMERICA 4 CANADA 5 ASIA 6 OCEANIA Out[13]: platform 0 Retail 1 Shopify Out[13]: segment 0 F3 1 C3 2 F2 3 C4 4 C2 5 F1 6 &lt;NA&gt; 7 C1 Out[13]: customer_type 0 New 1 Existing 2 Guest In\u00a0[18]: Copied! <pre>query = \"\"\"\nSELECT \n    sales \nFROM \n    data_mart.weekly_sales\nWHERE \n    1 = 1\n    AND sales &lt;= (\n        SELECT\n            APPROX_PERCENTILE(sales , 0.90)\n        FROM\n            data_mart.weekly_sales\n    );\n\"\"\"\nsales = athena.query(database=database, query=query, ctas_approach=ctas_approach)\n\nplt.figure(figsize=(8, 6))\nplt.violinplot(sales, showmeans=True, showextrema=True, showmedians=True)\nplt.title(\"Sales\")\nplt.ylabel(\"Dollars\")\nplt.show();\n</pre> query = \"\"\" SELECT      sales  FROM      data_mart.weekly_sales WHERE      1 = 1     AND sales &lt;= (         SELECT             APPROX_PERCENTILE(sales , 0.90)         FROM             data_mart.weekly_sales     ); \"\"\" sales = athena.query(database=database, query=query, ctas_approach=ctas_approach)  plt.figure(figsize=(8, 6)) plt.violinplot(sales, showmeans=True, showextrema=True, showmedians=True) plt.title(\"Sales\") plt.ylabel(\"Dollars\") plt.show(); In\u00a0[23]: Copied! <pre>query = \"\"\"\nSELECT \n    AVG(sales) / 1000000.0 AS mean_sales_millions,\n    STDDEV(sales) / 1000000.0 AS std_sales_millions,\n    APPROX_PERCENTILE(sales, 0.25) / 1000000.0 AS pct_25_millions,\n    APPROX_PERCENTILE(sales, 0.50) / 1000000.0 AS median_millions,\n    APPROX_PERCENTILE(sales, 0.75) / 1000000.0 AS pct_75_millions\nFROM \n    data_mart.weekly_sales;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\" SELECT      AVG(sales) / 1000000.0 AS mean_sales_millions,     STDDEV(sales) / 1000000.0 AS std_sales_millions,     APPROX_PERCENTILE(sales, 0.25) / 1000000.0 AS pct_25_millions,     APPROX_PERCENTILE(sales, 0.50) / 1000000.0 AS median_millions,     APPROX_PERCENTILE(sales, 0.75) / 1000000.0 AS pct_75_millions FROM      data_mart.weekly_sales; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[23]: mean_sales_millions std_sales_millions pct_25_millions median_millions pct_75_millions 0 2.380302 6.973521 0.02535 0.220297 1.656853 In\u00a0[19]: Copied! <pre>query = \"\"\"\nSELECT \n    transactions \nFROM \n    data_mart.weekly_sales\nWHERE \n    1 = 1\n    AND sales &lt;= (\n        SELECT\n            APPROX_PERCENTILE(transactions, 0.95)\n        FROM\n            data_mart.weekly_sales\n    );\n\"\"\"\nsales = athena.query(database=database, query=query, ctas_approach=ctas_approach)\n\nplt.figure(figsize=(8, 6))\nplt.violinplot(sales, showmeans=True, showextrema=True, showmedians=True)\nplt.title(\"Transactions\")\nplt.ylabel(\"Counts\")\nplt.show();\n</pre> query = \"\"\" SELECT      transactions  FROM      data_mart.weekly_sales WHERE      1 = 1     AND sales &lt;= (         SELECT             APPROX_PERCENTILE(transactions, 0.95)         FROM             data_mart.weekly_sales     ); \"\"\" sales = athena.query(database=database, query=query, ctas_approach=ctas_approach)  plt.figure(figsize=(8, 6)) plt.violinplot(sales, showmeans=True, showextrema=True, showmedians=True) plt.title(\"Transactions\") plt.ylabel(\"Counts\") plt.show(); In\u00a0[25]: Copied! <pre>query = \"\"\"\nSELECT \n    AVG(transactions) AS mean_transactions,\n    STDDEV(transactions) AS std_transactions,\n    APPROX_PERCENTILE(transactions, 0.25) AS pct_25,\n    APPROX_PERCENTILE(transactions, 0.50) AS median,\n    APPROX_PERCENTILE(transactions, 0.75) AS pct_75\nFROM \n    data_mart.weekly_sales;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\" SELECT      AVG(transactions) AS mean_transactions,     STDDEV(transactions) AS std_transactions,     APPROX_PERCENTILE(transactions, 0.25) AS pct_25,     APPROX_PERCENTILE(transactions, 0.50) AS median,     APPROX_PERCENTILE(transactions, 0.75) AS pct_75 FROM      data_mart.weekly_sales; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[25]: mean_transactions std_transactions pct_25 median pct_75 0 63554.325875 237625.690744 181 1855 40903 In\u00a0[28]: Copied! <pre>query = \"\"\" \nSELECT\n    MAX(week_date) AS max_date,\n    MIN(week_date) AS min_date,\n    DATE_DIFF('DAY', MIN(week_date), MAX(week_date)) AS days_difference,\n    DATE_DIFF('MONTH', MIN(week_date), MAX(week_date)) AS months_difference,\n    DATE_DIFF('YEAR', MIN(week_date), MAX(week_date)) AS years_difference\nFROM\n    data_mart.weekly_sales;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  SELECT     MAX(week_date) AS max_date,     MIN(week_date) AS min_date,     DATE_DIFF('DAY', MIN(week_date), MAX(week_date)) AS days_difference,     DATE_DIFF('MONTH', MIN(week_date), MAX(week_date)) AS months_difference,     DATE_DIFF('YEAR', MIN(week_date), MAX(week_date)) AS years_difference FROM     data_mart.weekly_sales; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[28]: max_date min_date days_difference months_difference years_difference 0 2020-08-31 2018-03-26 889 29 2 In\u00a0[81]: Copied! <pre>query = \"\"\" \nSELECT\n    week_date,\n    CAST(CEIL(DAY_OF_YEAR(week_date) / 7.0) AS INTEGER) AS week_number,\n    EXTRACT(MONTH FROM week_date) AS month_number,\n    EXTRACT(YEAR FROM week_date) AS calendar_year,\n\n    COALESCE(segment, 'Unknown') AS segment,\n    CASE \n        WHEN SUBSTRING(segment, -1, 1) = '1' THEN 'Young Adults'\n        WHEN SUBSTRING(segment, -1, 1) = '2' THEN 'Middle Aged'\n        WHEN SUBSTRING(segment, -1, 1) IN ('3', '4') THEN 'Retirees'\n        ELSE 'Unknown'\n    END AS age_band,\n    CASE \n        WHEN SUBSTRING(segment, 1, 1) = 'C' THEN 'Couples'\n        WHEN SUBSTRING(segment, 1, 1) = 'F' THEN 'Families'\n        ELSE 'Unknown'\n    END AS demographics,\n\n    region,\n    platform,\n    customer_type,\n\n    transactions,\n    ROUND(sales / transactions, 2) AS avg_transaction,\n    sales\nFROM\n    data_mart.weekly_sales;\n\"\"\"\n\nathena.create_ctas_table(\n    database=database,\n    query=query,\n    ctas_table=\"sales\",\n    s3_output=\"s3://sql-case-studies/data_mart/sales/\",\n    storage_format=storage_format,\n    write_compression=write_compression,\n    wait=wait,\n)\n</pre> query = \"\"\"  SELECT     week_date,     CAST(CEIL(DAY_OF_YEAR(week_date) / 7.0) AS INTEGER) AS week_number,     EXTRACT(MONTH FROM week_date) AS month_number,     EXTRACT(YEAR FROM week_date) AS calendar_year,      COALESCE(segment, 'Unknown') AS segment,     CASE          WHEN SUBSTRING(segment, -1, 1) = '1' THEN 'Young Adults'         WHEN SUBSTRING(segment, -1, 1) = '2' THEN 'Middle Aged'         WHEN SUBSTRING(segment, -1, 1) IN ('3', '4') THEN 'Retirees'         ELSE 'Unknown'     END AS age_band,     CASE          WHEN SUBSTRING(segment, 1, 1) = 'C' THEN 'Couples'         WHEN SUBSTRING(segment, 1, 1) = 'F' THEN 'Families'         ELSE 'Unknown'     END AS demographics,      region,     platform,     customer_type,      transactions,     ROUND(sales / transactions, 2) AS avg_transaction,     sales FROM     data_mart.weekly_sales; \"\"\"  athena.create_ctas_table(     database=database,     query=query,     ctas_table=\"sales\",     s3_output=\"s3://sql-case-studies/data_mart/sales/\",     storage_format=storage_format,     write_compression=write_compression,     wait=wait, ) Out[81]: <pre>{'ctas_database': 'data_mart',\n 'ctas_table': 'sales',\n 'ctas_query_metadata': _QueryMetadata(execution_id='c8df1006-7397-4bbd-9384-e66616336ec2', dtype={'rows': 'Int64'}, parse_timestamps=[], parse_dates=[], parse_geometry=[], converters={}, binaries=[], output_location='s3://sql-case-studies/data_mart/sales/tables/c8df1006-7397-4bbd-9384-e66616336ec2', manifest_location='s3://sql-case-studies/data_mart/sales/tables/c8df1006-7397-4bbd-9384-e66616336ec2-manifest.csv', raw_payload={'QueryExecutionId': 'c8df1006-7397-4bbd-9384-e66616336ec2', 'Query': 'CREATE TABLE \"data_mart\".\"sales\"\\nWITH(\\n    external_location = \\'s3://sql-case-studies/data_mart/sales/sales\\',\\n    write_compression = \\'SNAPPY\\',\\n    format = \\'PARQUET\\')\\nAS  \\nSELECT\\n    week_date,\\n    CAST(CEIL(DAY_OF_YEAR(week_date) / 7.0) AS INTEGER) AS week_number,\\n    EXTRACT(MONTH FROM week_date) AS month_number,\\n    EXTRACT(YEAR FROM week_date) AS calendar_year,\\n\\n    COALESCE(segment, \\'Unknown\\') AS segment,\\n    CASE \\n        WHEN SUBSTRING(segment, -1, 1) = \\'1\\' THEN \\'Young Adults\\'\\n        WHEN SUBSTRING(segment, -1, 1) = \\'2\\' THEN \\'Middle Aged\\'\\n        WHEN SUBSTRING(segment, -1, 1) IN (\\'3\\', \\'4\\') THEN \\'Retirees\\'\\n        ELSE \\'Unknown\\'\\n    END AS age_band,\\n    CASE \\n        WHEN SUBSTRING(segment, 1, 1) = \\'C\\' THEN \\'Couples\\'\\n        WHEN SUBSTRING(segment, 1, 1) = \\'F\\' THEN \\'Families\\'\\n        ELSE \\'Unknown\\'\\n    END AS demographics,\\n\\n    region,\\n    platform,\\n    customer_type,\\n\\n    transactions,\\n    ROUND(sales / transactions, 2) AS avg_transaction,\\n    sales\\nFROM\\n    data_mart.weekly_sales', 'StatementType': 'DDL', 'ResultConfiguration': {'OutputLocation': 's3://sql-case-studies/data_mart/sales/tables/c8df1006-7397-4bbd-9384-e66616336ec2'}, 'ResultReuseConfiguration': {'ResultReuseByAgeConfiguration': {'Enabled': False}}, 'QueryExecutionContext': {'Database': 'data_mart'}, 'Status': {'State': 'SUCCEEDED', 'SubmissionDateTime': datetime.datetime(2025, 1, 22, 0, 39, 4, 872000, tzinfo=tzlocal()), 'CompletionDateTime': datetime.datetime(2025, 1, 22, 0, 39, 6, 259000, tzinfo=tzlocal())}, 'Statistics': {'EngineExecutionTimeInMillis': 1189, 'DataScannedInBytes': 188004, 'DataManifestLocation': 's3://sql-case-studies/data_mart/sales/tables/c8df1006-7397-4bbd-9384-e66616336ec2-manifest.csv', 'TotalExecutionTimeInMillis': 1387, 'QueryQueueTimeInMillis': 57, 'ServicePreProcessingTimeInMillis': 112, 'QueryPlanningTimeInMillis': 177, 'ServiceProcessingTimeInMillis': 29, 'ResultReuseInformation': {'ReusedPreviousResult': False}}, 'WorkGroup': 'primary', 'EngineVersion': {'SelectedEngineVersion': 'AUTO', 'EffectiveEngineVersion': 'Athena engine version 3'}, 'SubstatementType': 'CREATE_TABLE_AS_SELECT'})}</pre> In\u00a0[83]: Copied! <pre>athena.query(\n    database=database,\n    query=\"SELECT * FROM data_mart.sales TABLESAMPLE BERNOULLI(30);\",\n    ctas_approach=ctas_approach,\n)\n</pre> athena.query(     database=database,     query=\"SELECT * FROM data_mart.sales TABLESAMPLE BERNOULLI(30);\",     ctas_approach=ctas_approach, ) Out[83]: week_date week_number month_number calendar_year segment age_band demographics region platform customer_type transactions avg_transaction sales 0 2020-08-31 35 8 2020 C3 Retirees Couples ASIA Retail New 120631 30.31 3656163.0 1 2020-08-31 35 8 2020 C2 Middle Aged Couples AFRICA Retail New 58046 30.29 1758388.0 2 2020-08-31 35 8 2020 F2 Middle Aged Families CANADA Shopify Existing 1336 182.54 243878.0 3 2020-08-31 35 8 2020 F3 Retirees Families AFRICA Shopify Existing 2514 206.64 519502.0 4 2020-08-31 35 8 2020 F2 Middle Aged Families AFRICA Shopify New 318 155.84 49557.0 ... ... ... ... ... ... ... ... ... ... ... ... ... ... 5129 2018-03-26 13 3 2018 C2 Middle Aged Couples ASIA Retail New 72058 27.87 2008197.0 5130 2018-03-26 13 3 2018 F1 Young Adults Families USA Shopify Existing 980 203.76 199685.0 5131 2018-03-26 13 3 2018 F1 Young Adults Families SOUTH AMERICA Shopify New 3 225.67 677.0 5132 2018-03-26 13 3 2018 F3 Retirees Families EUROPE Shopify New 2 150.00 300.0 5133 2018-03-26 13 3 2018 F2 Middle Aged Families USA Retail New 25665 41.46 1064172.0 <p>5134 rows \u00d7 13 columns</p> In\u00a0[84]: Copied! <pre>query = \"\"\" \nSELECT\n    DISTINCT DAY_OF_WEEK(week_date) AS day_of_week\nFROM\n    data_mart.sales;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  SELECT     DISTINCT DAY_OF_WEEK(week_date) AS day_of_week FROM     data_mart.sales; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[84]: day_of_week 0 1 In\u00a0[87]: Copied! <pre>query = \"\"\" \nSELECT\n    DISTINCT week_number\nFROM\n    data_mart.sales\nORDER BY\n    week_number DESC;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  SELECT     DISTINCT week_number FROM     data_mart.sales ORDER BY     week_number DESC; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[87]: week_number 0 36 1 35 2 34 3 33 4 32 5 31 6 30 7 29 8 28 9 27 10 26 11 25 12 24 13 23 14 22 15 21 16 20 17 19 18 18 19 17 20 16 21 15 22 14 23 13 24 12 <p>Weeks 1 to 11 are missing in the dataset.</p> In\u00a0[91]: Copied! <pre>query = \"\"\" \nSELECT\n    calendar_year,\n    SUM(transactions) / 1000000.0 AS total_transactions_millions\nFROM\n    data_mart.sales\nGROUP BY\n    calendar_year\nORDER BY\n    total_transactions_millions DESC;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  SELECT     calendar_year,     SUM(transactions) / 1000000.0 AS total_transactions_millions FROM     data_mart.sales GROUP BY     calendar_year ORDER BY     total_transactions_millions DESC; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[91]: calendar_year total_transactions_millions 0 2020 375.813651 1 2019 365.639285 2 2018 346.406460 In\u00a0[137]: Copied! <pre>query = \"\"\" \nSELECT\n    region,\n    DATE_TRUNC('MONTH', week_date) AS month_year,\n    SUM(sales) / 1000000.0 AS total_sales_millions\nFROM\n    data_mart.sales\nGROUP BY\n    region,\n    DATE_TRUNC('MONTH', week_date)\nORDER BY\n    region ASC,\n    DATE_TRUNC('MONTH', week_date) ASC;\n\"\"\"\n\nsales_by_region_month = athena.query(\n    database=database, query=query, ctas_approach=ctas_approach\n)\nsales_by_region_month.head(10)\n</pre> query = \"\"\"  SELECT     region,     DATE_TRUNC('MONTH', week_date) AS month_year,     SUM(sales) / 1000000.0 AS total_sales_millions FROM     data_mart.sales GROUP BY     region,     DATE_TRUNC('MONTH', week_date) ORDER BY     region ASC,     DATE_TRUNC('MONTH', week_date) ASC; \"\"\"  sales_by_region_month = athena.query(     database=database, query=query, ctas_approach=ctas_approach ) sales_by_region_month.head(10) Out[137]: region month_year total_sales_millions 0 AFRICA 2018-03-01 130.542211 1 AFRICA 2018-04-01 650.194752 2 AFRICA 2018-05-01 522.814996 3 AFRICA 2018-06-01 519.127097 4 AFRICA 2018-07-01 674.135864 5 AFRICA 2018-08-01 539.077368 6 AFRICA 2018-09-01 135.084533 7 AFRICA 2019-03-01 141.619350 8 AFRICA 2019-04-01 700.447296 9 AFRICA 2019-05-01 553.828220 In\u00a0[138]: Copied! <pre>plt.figure(figsize=(12, 6))\n\n# Plot data for each region\nfor region, group in sales_by_region_month.groupby(\"region\"):\n    plt.plot(\n        group[\"month_year\"], group[\"total_sales_millions\"], label=region, marker=\"o\"\n    )\n\nplt.title(\"Monthly Total Sales by Region\", fontsize=16)\nplt.xlabel(\"Month Year\", fontsize=14)\nplt.ylabel(\"Total Sales (Millions)\", fontsize=14)\nplt.yticks(fontsize=12)\nplt.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5, alpha=0.7)\nplt.legend(\n    title=\"Region\",\n    fontsize=12,\n    title_fontsize=14,\n    loc=\"upper left\",\n    bbox_to_anchor=(1.05, 1.0),  # Move legend outside the plot\n)\nplt.tight_layout()\nplt.show();\n</pre> plt.figure(figsize=(12, 6))  # Plot data for each region for region, group in sales_by_region_month.groupby(\"region\"):     plt.plot(         group[\"month_year\"], group[\"total_sales_millions\"], label=region, marker=\"o\"     )  plt.title(\"Monthly Total Sales by Region\", fontsize=16) plt.xlabel(\"Month Year\", fontsize=14) plt.ylabel(\"Total Sales (Millions)\", fontsize=14) plt.yticks(fontsize=12) plt.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5, alpha=0.7) plt.legend(     title=\"Region\",     fontsize=12,     title_fontsize=14,     loc=\"upper left\",     bbox_to_anchor=(1.05, 1.0),  # Move legend outside the plot ) plt.tight_layout() plt.show(); In\u00a0[100]: Copied! <pre>query = \"\"\" \nSELECT\n    platform,\n    SUM(transactions) / 100000.0 AS total_transactions_millions\nFROM\n    data_mart.sales\nGROUP BY\n    platform\nORDER BY\n    total_transactions_millions DESC;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  SELECT     platform,     SUM(transactions) / 100000.0 AS total_transactions_millions FROM     data_mart.sales GROUP BY     platform ORDER BY     total_transactions_millions DESC; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[100]: platform total_transactions_millions 0 Retail 10819.34227 1 Shopify 59.25169 In\u00a0[147]: Copied! <pre>query = \"\"\" \nWITH sales_pct_by_platform AS (\n    SELECT\n        DATE_TRUNC('MONTH', week_date) AS month_year,\n        platform,\n        SUM(sales) * 100.0 / SUM(SUM(sales)) OVER(PARTITION BY DATE_TRUNC('MONTH', week_date)) AS sales_pct\n    FROM\n        data_mart.sales\n    GROUP BY\n        DATE_TRUNC('MONTH', week_date),\n        platform\n    ORDER BY\n        DATE_TRUNC('MONTH', week_date),\n        platform\n)\nSELECT\n    month_year,\n    MAX(CASE WHEN platform = 'Retail' THEN sales_pct ELSE NULL END) AS retail,\n    MAX(CASE WHEN platform = 'Shopify' THEN sales_pct ELSE NULL END) AS shopify\nFROM\n    sales_pct_by_platform\nGROUP BY\n    month_year\nORDER BY\n    month_year ASC;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  WITH sales_pct_by_platform AS (     SELECT         DATE_TRUNC('MONTH', week_date) AS month_year,         platform,         SUM(sales) * 100.0 / SUM(SUM(sales)) OVER(PARTITION BY DATE_TRUNC('MONTH', week_date)) AS sales_pct     FROM         data_mart.sales     GROUP BY         DATE_TRUNC('MONTH', week_date),         platform     ORDER BY         DATE_TRUNC('MONTH', week_date),         platform ) SELECT     month_year,     MAX(CASE WHEN platform = 'Retail' THEN sales_pct ELSE NULL END) AS retail,     MAX(CASE WHEN platform = 'Shopify' THEN sales_pct ELSE NULL END) AS shopify FROM     sales_pct_by_platform GROUP BY     month_year ORDER BY     month_year ASC; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[147]: month_year retail shopify 0 2018-03-01 97.918532 2.081468 1 2018-04-01 97.925940 2.074060 2 2018-05-01 97.727866 2.272134 3 2018-06-01 97.755527 2.244473 4 2018-07-01 97.753021 2.246979 5 2018-08-01 97.706277 2.293723 6 2018-09-01 97.678574 2.321426 7 2019-03-01 97.706553 2.293447 8 2019-04-01 97.800180 2.199820 9 2019-05-01 97.524904 2.475096 10 2019-06-01 97.421580 2.578420 11 2019-07-01 97.352333 2.647667 12 2019-08-01 97.210798 2.789202 13 2019-09-01 97.087077 2.912923 14 2020-03-01 97.298375 2.701625 15 2020-04-01 96.962685 3.037315 16 2020-05-01 96.711033 3.288967 17 2020-06-01 96.803401 3.196599 18 2020-07-01 96.672693 3.327307 19 2020-08-01 96.511318 3.488682 <p>Another approach is to use a CTE:</p> In\u00a0[146]: Copied! <pre>query = \"\"\" \nWITH monthly_totals AS (\n    SELECT\n        DATE_TRUNC('MONTH', week_date) AS month_year,\n        SUM(sales) AS total_sales\n    FROM\n        data_mart.sales\n    GROUP BY\n        DATE_TRUNC('MONTH', week_date)\n),\nsales_pct_by_platform AS (\n    SELECT\n        DATE_TRUNC('MONTH', s.week_date) AS month_year,\n        s.platform,\n        SUM(s.sales) * 100.0 / t.total_sales AS sales_pct\n    FROM\n        data_mart.sales AS s \n            INNER JOIN monthly_totals AS t ON DATE_TRUNC('MONTH', s.week_date) = t.month_year\n    GROUP BY\n        DATE_TRUNC('MONTH', s.week_date), \n        s.platform, \n        t.total_sales\n    ORDER BY\n        DATE_TRUNC('MONTH', s.week_date), \n        s.platform\n)\nSELECT\n    month_year,\n    MAX(CASE WHEN platform = 'Retail' THEN sales_pct ELSE NULL END) AS retail,\n    MAX(CASE WHEN platform = 'Shopify' THEN sales_pct ELSE NULL END) AS shopify\nFROM\n    sales_pct_by_platform\nGROUP BY\n    month_year\nORDER BY\n    month_year ASC;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  WITH monthly_totals AS (     SELECT         DATE_TRUNC('MONTH', week_date) AS month_year,         SUM(sales) AS total_sales     FROM         data_mart.sales     GROUP BY         DATE_TRUNC('MONTH', week_date) ), sales_pct_by_platform AS (     SELECT         DATE_TRUNC('MONTH', s.week_date) AS month_year,         s.platform,         SUM(s.sales) * 100.0 / t.total_sales AS sales_pct     FROM         data_mart.sales AS s              INNER JOIN monthly_totals AS t ON DATE_TRUNC('MONTH', s.week_date) = t.month_year     GROUP BY         DATE_TRUNC('MONTH', s.week_date),          s.platform,          t.total_sales     ORDER BY         DATE_TRUNC('MONTH', s.week_date),          s.platform ) SELECT     month_year,     MAX(CASE WHEN platform = 'Retail' THEN sales_pct ELSE NULL END) AS retail,     MAX(CASE WHEN platform = 'Shopify' THEN sales_pct ELSE NULL END) AS shopify FROM     sales_pct_by_platform GROUP BY     month_year ORDER BY     month_year ASC; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[146]: month_year retail shopify 0 2018-03-01 97.918532 2.081468 1 2018-04-01 97.925940 2.074060 2 2018-05-01 97.727866 2.272134 3 2018-06-01 97.755527 2.244473 4 2018-07-01 97.753021 2.246979 5 2018-08-01 97.706277 2.293723 6 2018-09-01 97.678574 2.321426 7 2019-03-01 97.706553 2.293447 8 2019-04-01 97.800180 2.199820 9 2019-05-01 97.524904 2.475096 10 2019-06-01 97.421580 2.578420 11 2019-07-01 97.352333 2.647667 12 2019-08-01 97.210798 2.789202 13 2019-09-01 97.087077 2.912923 14 2020-03-01 97.298375 2.701625 15 2020-04-01 96.962685 3.037315 16 2020-05-01 96.711033 3.288967 17 2020-06-01 96.803401 3.196599 18 2020-07-01 96.672693 3.327307 19 2020-08-01 96.511318 3.488682 In\u00a0[113]: Copied! <pre>query = \"\"\" \nSELECT\n    calendar_year,\n    demographics,\n    SUM(sales) * 100.0 / SUM(SUM(sales)) OVER(PARTITION BY calendar_year) AS sales_pct\nFROM\n    data_mart.sales\nGROUP BY\n    calendar_year,\n    demographics\nORDER BY\n    calendar_year,\n    demographics;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  SELECT     calendar_year,     demographics,     SUM(sales) * 100.0 / SUM(SUM(sales)) OVER(PARTITION BY calendar_year) AS sales_pct FROM     data_mart.sales GROUP BY     calendar_year,     demographics ORDER BY     calendar_year,     demographics; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[113]: calendar_year demographics sales_pct 0 2018 Couples 26.380462 1 2018 Families 31.987565 2 2018 Unknown 41.631973 3 2019 Couples 27.275157 4 2019 Families 32.474231 5 2019 Unknown 40.250612 6 2020 Couples 28.719883 7 2020 Families 32.725289 8 2020 Unknown 38.554828 In\u00a0[153]: Copied! <pre>query = \"\"\" \nSELECT\n    age_band,\n    demographics,\n    SUM(SUM(sales)) OVER(PARTITION BY age_band) / 1000000.0 AS total_sales_millions_by_age_band,\n    SUM(SUM(sales)) OVER(PARTITION BY demographics) / 1000000.0 AS total_sales_millions_by_demographics,\n    SUM(sales) / 1000000.0 AS total_sales_millions_by_age_demo\nFROM\n    data_mart.sales\nWHERE\n    1 = 1\n    AND platform IN ('Retail')\nGROUP BY\n    age_band,\n    demographics\nORDER BY\n    age_band,\n    demographics;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  SELECT     age_band,     demographics,     SUM(SUM(sales)) OVER(PARTITION BY age_band) / 1000000.0 AS total_sales_millions_by_age_band,     SUM(SUM(sales)) OVER(PARTITION BY demographics) / 1000000.0 AS total_sales_millions_by_demographics,     SUM(sales) / 1000000.0 AS total_sales_millions_by_age_demo FROM     data_mart.sales WHERE     1 = 1     AND platform IN ('Retail') GROUP BY     age_band,     demographics ORDER BY     age_band,     demographics; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[153]: age_band demographics total_sales_millions_by_age_band total_sales_millions_by_demographics total_sales_millions_by_age_demo 0 Middle Aged Couples 6208.251885 10827.663141 1854.160330 1 Middle Aged Families 6208.251885 12759.667756 4354.091555 2 Retirees Couples 13005.266922 10827.663141 6370.580014 3 Retirees Families 13005.266922 12759.667756 6634.686908 4 Unknown Unknown 16067.285525 16067.285525 16067.285525 5 Young Adults Couples 4373.812090 10827.663141 2602.922797 6 Young Adults Families 4373.812090 12759.667756 1770.889293 In\u00a0[150]: Copied! <pre>query = \"\"\" \nSELECT\n    age_band,\n    SUM(sales) / 1000000.0 AS total_sales_millions,\n    SUM(sales) * 100.0 / SUM(SUM(sales)) OVER() AS sales_pct\nFROM\n    data_mart.sales\nWHERE\n    1 = 1\n    AND platform IN ('Retail')\nGROUP BY\n    age_band\nORDER BY\n    total_sales_millions DESC;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  SELECT     age_band,     SUM(sales) / 1000000.0 AS total_sales_millions,     SUM(sales) * 100.0 / SUM(SUM(sales)) OVER() AS sales_pct FROM     data_mart.sales WHERE     1 = 1     AND platform IN ('Retail') GROUP BY     age_band ORDER BY     total_sales_millions DESC; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[150]: age_band total_sales_millions sales_pct 0 Unknown 16067.285525 40.518071 1 Retirees 13005.266922 32.796350 2 Middle Aged 6208.251885 15.655811 3 Young Adults 4373.812090 11.029768 In\u00a0[152]: Copied! <pre>query = \"\"\" \nSELECT\n    demographics,\n    SUM(sales) / 1000000.0 AS total_sales_millions,\n    SUM(sales) * 100.0 / SUM(SUM(sales)) OVER() AS sales_pct\nFROM\n    data_mart.sales\nWHERE\n    1 = 1\n    AND platform IN ('Retail')\nGROUP BY\n    demographics\nORDER BY\n    total_sales_millions DESC;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  SELECT     demographics,     SUM(sales) / 1000000.0 AS total_sales_millions,     SUM(sales) * 100.0 / SUM(SUM(sales)) OVER() AS sales_pct FROM     data_mart.sales WHERE     1 = 1     AND platform IN ('Retail') GROUP BY     demographics ORDER BY     total_sales_millions DESC; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[152]: demographics total_sales_millions sales_pct 0 Unknown 16067.285525 40.518071 1 Families 12759.667756 32.177005 2 Couples 10827.663141 27.304925 In\u00a0[129]: Copied! <pre>query = \"\"\" \nSELECT\n    calendar_year,\n    platform,\n    ROUND(SUM(sales) / SUM(transactions), 2) AS avg_transaction\nFROM\n    data_mart.sales\nGROUP BY\n    calendar_year,\n    platform\nORDER BY\n    calendar_year,\n    platform;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  SELECT     calendar_year,     platform,     ROUND(SUM(sales) / SUM(transactions), 2) AS avg_transaction FROM     data_mart.sales GROUP BY     calendar_year,     platform ORDER BY     calendar_year,     platform; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[129]: calendar_year platform avg_transaction 0 2018 Retail 36.56 1 2018 Shopify 192.48 2 2019 Retail 36.83 3 2019 Shopify 183.36 4 2020 Retail 36.56 5 2020 Shopify 179.03 In\u00a0[244]: Copied! <pre>query = \"\"\"\nSELECT\n    week_date,\n    CASE \n        WHEN week_date &lt; DATE '2020-06-15' THEN '1 before' \n        WHEN week_date &gt;= DATE '2020-06-15' THEN '2 after' \n    END AS intervention,\n    sales\nFROM\n    data_mart.sales\nWHERE\n    1 = 1\n    AND week_date &gt;= DATE_ADD('WEEK', -4, DATE '2020-06-15') \n    AND week_date &lt; DATE_ADD('WEEK', 4, DATE '2020-06-15');\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\" SELECT     week_date,     CASE          WHEN week_date &lt; DATE '2020-06-15' THEN '1 before'          WHEN week_date &gt;= DATE '2020-06-15' THEN '2 after'      END AS intervention,     sales FROM     data_mart.sales WHERE     1 = 1     AND week_date &gt;= DATE_ADD('WEEK', -4, DATE '2020-06-15')      AND week_date &lt; DATE_ADD('WEEK', 4, DATE '2020-06-15'); \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[244]: week_date intervention sales 0 2020-07-06 2 after 275749.0 1 2020-07-06 2 after 7059529.0 2 2020-07-06 2 after 12375.0 3 2020-07-06 2 after 359403.0 4 2020-07-06 2 after 48780232.0 ... ... ... ... 1899 2020-05-18 1 before 2146858.0 1900 2020-05-18 1 before 36469.0 1901 2020-05-18 1 before 480498.0 1902 2020-05-18 1 before 258708.0 1903 2020-05-18 1 before 993855.0 <p>1904 rows \u00d7 3 columns</p> In\u00a0[245]: Copied! <pre>query = \"\"\" \nWITH before_after AS (\n    SELECT\n        CASE \n            WHEN week_date &lt; DATE '2020-06-15' THEN '1 before' \n            WHEN week_date &gt;= DATE '2020-06-15' THEN '2 after' \n        END AS intervention,\n        sales\n    FROM\n        data_mart.sales\n    WHERE\n        1 = 1\n        AND week_date &gt;= DATE_ADD('WEEK', -4, DATE '2020-06-15') \n        AND week_date &lt; DATE_ADD('WEEK', 4, DATE '2020-06-15')\n)\nSELECT\n    intervention,\n    SUM(sales) / 1000000.0 AS total_sales_millions,\n    SUM(sales) * 100.0 / SUM(SUM(sales)) OVER() AS sales_pct\nFROM\n    before_after\nGROUP BY\n    intervention;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  WITH before_after AS (     SELECT         CASE              WHEN week_date &lt; DATE '2020-06-15' THEN '1 before'              WHEN week_date &gt;= DATE '2020-06-15' THEN '2 after'          END AS intervention,         sales     FROM         data_mart.sales     WHERE         1 = 1         AND week_date &gt;= DATE_ADD('WEEK', -4, DATE '2020-06-15')          AND week_date &lt; DATE_ADD('WEEK', 4, DATE '2020-06-15') ) SELECT     intervention,     SUM(sales) / 1000000.0 AS total_sales_millions,     SUM(sales) * 100.0 / SUM(SUM(sales)) OVER() AS sales_pct FROM     before_after GROUP BY     intervention; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[245]: intervention total_sales_millions sales_pct 0 2 after 2318.994168 49.711844 1 1 before 2345.878357 50.288156 In\u00a0[249]: Copied! <pre>query = \"\"\" \nWITH before_after AS (\n    SELECT\n        CASE \n            WHEN week_date &lt; DATE '2020-06-15' THEN '1 before' \n            WHEN week_date &gt;= DATE '2020-06-15' THEN '2 after' \n        END AS intervention,\n        sales\n    FROM\n        data_mart.sales\n    WHERE\n        1 = 1\n        AND week_date &gt;= DATE_ADD('WEEK', -4, DATE '2020-06-15') \n        AND week_date &lt; DATE_ADD('WEEK', 4, DATE '2020-06-15')\n),\n\ntotal_sales AS (\n    SELECT\n        intervention,\n        SUM(sales) / 1000000.0 AS total_sales_millions,\n        SUM(sales) * 100.0 / SUM(SUM(sales)) OVER() AS sales_pct\n    FROM\n        before_after\n    GROUP BY\n        intervention\n)\nSELECT\n    intervention,\n    total_sales_millions AS total_sales_before,\n    sales_pct,\n    LAG(total_sales_millions, 1) OVER(ORDER BY intervention) AS total_sales_after,\n    100.0 * ((total_sales_millions / LAG(total_sales_millions, 1) OVER(ORDER BY intervention)) - 1) AS sales_pct_change,\n    (total_sales_millions - LAG(total_sales_millions, 1) OVER(ORDER BY intervention)) AS sales_difference_millions\nFROM\n    total_sales;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  WITH before_after AS (     SELECT         CASE              WHEN week_date &lt; DATE '2020-06-15' THEN '1 before'              WHEN week_date &gt;= DATE '2020-06-15' THEN '2 after'          END AS intervention,         sales     FROM         data_mart.sales     WHERE         1 = 1         AND week_date &gt;= DATE_ADD('WEEK', -4, DATE '2020-06-15')          AND week_date &lt; DATE_ADD('WEEK', 4, DATE '2020-06-15') ),  total_sales AS (     SELECT         intervention,         SUM(sales) / 1000000.0 AS total_sales_millions,         SUM(sales) * 100.0 / SUM(SUM(sales)) OVER() AS sales_pct     FROM         before_after     GROUP BY         intervention ) SELECT     intervention,     total_sales_millions AS total_sales_before,     sales_pct,     LAG(total_sales_millions, 1) OVER(ORDER BY intervention) AS total_sales_after,     100.0 * ((total_sales_millions / LAG(total_sales_millions, 1) OVER(ORDER BY intervention)) - 1) AS sales_pct_change,     (total_sales_millions - LAG(total_sales_millions, 1) OVER(ORDER BY intervention)) AS sales_difference_millions FROM     total_sales; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[249]: intervention total_sales_before sales_pct total_sales_after sales_pct_change sales_difference_millions 0 1 before 2345.878357 50.288156 NaN NaN NaN 1 2 after 2318.994168 49.711844 2345.878357 -1.146018 -26.884189 In\u00a0[253]: Copied! <pre>def sales_metrics_query(year: int, weeks: int) -&gt; str:\n    return f\"\"\" \n            WITH before_after AS (\n                SELECT\n                    CASE \n                        WHEN week_date &lt; DATE '{year}-06-15' THEN '1 before' \n                        WHEN week_date &gt;= DATE '{year}-06-15' THEN '2 after' \n                    END AS intervention,\n                    sales\n                FROM\n                    data_mart.sales\n                WHERE\n                    1 = 1\n                    AND week_date &gt;= DATE_ADD('WEEK', -{weeks}, DATE '{year}-06-15') \n                    AND week_date &lt; DATE_ADD('WEEK', {weeks}, DATE '{year}-06-15')\n            ),\n\n            total_sales AS (\n                SELECT\n                    intervention,\n                    SUM(sales) / 1000000.0 AS total_sales_millions,\n                    SUM(sales) * 100.0 / SUM(SUM(sales)) OVER() AS sales_pct\n                FROM\n                    before_after\n                GROUP BY\n                    intervention\n            ),\n\n            sales_metrics AS (\n                SELECT\n                    intervention,\n                    total_sales_millions,\n                    sales_pct,\n                    100.0 * ((total_sales_millions / LAG(total_sales_millions, 1) OVER(ORDER BY intervention)) - 1) AS sales_pct_change,\n                    (total_sales_millions - LAG(total_sales_millions, 1) OVER(ORDER BY intervention)) AS sales_difference_millions\n                FROM\n                    total_sales\n            )\n            SELECT\n                MAX(CASE WHEN intervention = '1 before' THEN total_sales_millions ELSE NULL END) AS total_sales_milllions_before,\n                MAX(CASE WHEN intervention = '2 after' THEN total_sales_millions ELSE NULL END) AS total_sales_milllions_after,\n\n                MAX(CASE WHEN intervention = '1 before' THEN sales_pct ELSE NULL END) AS sales_pct_before,\n                MAX(CASE WHEN intervention = '2 after' THEN sales_pct ELSE NULL END) AS sales_pct_after,\n\n                MAX(CASE WHEN intervention = '2 after' THEN sales_pct_change ELSE NULL END) AS sales_pct_change,\n                MAX(CASE WHEN intervention = '2 after' THEN sales_difference_millions ELSE NULL END) AS sales_difference_millions\n            FROM\n                sales_metrics;\n            \"\"\"\n</pre> def sales_metrics_query(year: int, weeks: int) -&gt; str:     return f\"\"\"              WITH before_after AS (                 SELECT                     CASE                          WHEN week_date &lt; DATE '{year}-06-15' THEN '1 before'                          WHEN week_date &gt;= DATE '{year}-06-15' THEN '2 after'                      END AS intervention,                     sales                 FROM                     data_mart.sales                 WHERE                     1 = 1                     AND week_date &gt;= DATE_ADD('WEEK', -{weeks}, DATE '{year}-06-15')                      AND week_date &lt; DATE_ADD('WEEK', {weeks}, DATE '{year}-06-15')             ),              total_sales AS (                 SELECT                     intervention,                     SUM(sales) / 1000000.0 AS total_sales_millions,                     SUM(sales) * 100.0 / SUM(SUM(sales)) OVER() AS sales_pct                 FROM                     before_after                 GROUP BY                     intervention             ),              sales_metrics AS (                 SELECT                     intervention,                     total_sales_millions,                     sales_pct,                     100.0 * ((total_sales_millions / LAG(total_sales_millions, 1) OVER(ORDER BY intervention)) - 1) AS sales_pct_change,                     (total_sales_millions - LAG(total_sales_millions, 1) OVER(ORDER BY intervention)) AS sales_difference_millions                 FROM                     total_sales             )             SELECT                 MAX(CASE WHEN intervention = '1 before' THEN total_sales_millions ELSE NULL END) AS total_sales_milllions_before,                 MAX(CASE WHEN intervention = '2 after' THEN total_sales_millions ELSE NULL END) AS total_sales_milllions_after,                  MAX(CASE WHEN intervention = '1 before' THEN sales_pct ELSE NULL END) AS sales_pct_before,                 MAX(CASE WHEN intervention = '2 after' THEN sales_pct ELSE NULL END) AS sales_pct_after,                  MAX(CASE WHEN intervention = '2 after' THEN sales_pct_change ELSE NULL END) AS sales_pct_change,                 MAX(CASE WHEN intervention = '2 after' THEN sales_difference_millions ELSE NULL END) AS sales_difference_millions             FROM                 sales_metrics;             \"\"\" In\u00a0[254]: Copied! <pre>query = sales_metrics_query(year=2020, weeks=4)\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = sales_metrics_query(year=2020, weeks=4)  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[254]: total_sales_milllions_before total_sales_milllions_after sales_pct_before sales_pct_after sales_pct_change sales_difference_millions 0 2345.878357 2318.994168 50.288156 49.711844 -1.146018 -26.884189 In\u00a0[255]: Copied! <pre>query = sales_metrics_query(year=2020, weeks=12)\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = sales_metrics_query(year=2020, weeks=12)  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[255]: total_sales_milllions_before total_sales_milllions_after sales_pct_before sales_pct_after sales_pct_change sales_difference_millions 0 7126.273147 6973.947758 50.540152 49.459848 -2.137518 -152.325389 In\u00a0[280]: Copied! <pre>query_results: List[pd.DataFrame] = []\nfor year, weeks in product([2018, 2019, 2020], [4, 12]):\n    query: str = sales_metrics_query(year=year, weeks=weeks)\n    data: pd.DataFrame = athena.query(\n        database=database, query=query, ctas_approach=ctas_approach\n    )\n    data[\"year\"] = year\n    query_results.append(data)\n</pre> query_results: List[pd.DataFrame] = [] for year, weeks in product([2018, 2019, 2020], [4, 12]):     query: str = sales_metrics_query(year=year, weeks=weeks)     data: pd.DataFrame = athena.query(         database=database, query=query, ctas_approach=ctas_approach     )     data[\"year\"] = year     query_results.append(data) In\u00a0[289]: Copied! <pre>combined_result: pd.DataFrame = pd.concat(query_results, axis=0)\ncombined_result\n</pre> combined_result: pd.DataFrame = pd.concat(query_results, axis=0) combined_result Out[289]: total_sales_milllions_before total_sales_milllions_after sales_pct_before sales_pct_after sales_pct_change sales_difference_millions year 0 2125.140813 2129.242917 49.951790 50.048210 0.193027 4.102104 2018 0 6396.562306 6500.818508 49.595824 50.404176 1.629879 104.256202 2018 0 2249.989797 2252.326389 49.974051 50.025949 0.103849 2.336592 2019 0 6883.386390 6862.646103 50.075441 49.924559 -0.301309 -20.740287 2019 0 2345.878357 2318.994168 50.288156 49.711844 -1.146018 -26.884189 2020 0 7126.273147 6973.947758 50.540152 49.459848 -2.137518 -152.325389 2020 In\u00a0[294]: Copied! <pre>def sales_metrics_by_group_query(group_col: str, year: int, weeks: int) -&gt; str:\n    return f\"\"\" \n            WITH before_after AS (\n                SELECT\n                    CASE \n                        WHEN week_date &lt; DATE '{year}-06-15' THEN '1 before' \n                        WHEN week_date &gt;= DATE '{year}-06-15' THEN '2 after' \n                    END AS intervention,\n                    sales,\n                    {group_col}\n                FROM\n                    data_mart.sales\n                WHERE\n                    1 = 1\n                    AND week_date &gt;= DATE_ADD('WEEK', -{weeks}, DATE '{year}-06-15') \n                    AND week_date &lt; DATE_ADD('WEEK', {weeks}, DATE '{year}-06-15')\n            ),\n\n            total_sales AS (\n                SELECT\n                    {group_col},\n                    intervention,\n                    SUM(sales) / 1000000.0 AS total_sales_millions,\n                    SUM(sales) * 100.0 / SUM(SUM(sales)) OVER(PARTITION BY {group_col}) AS sales_pct\n                FROM\n                    before_after\n                GROUP BY\n                    {group_col},\n                    intervention\n            ),\n\n            sales_metrics AS (\n                SELECT\n                    {group_col},\n                    intervention,\n                    total_sales_millions,\n                    sales_pct,\n                    100.0 * ((total_sales_millions / LAG(total_sales_millions, 1) OVER(PARTITION BY {group_col} ORDER BY intervention)) - 1) AS sales_pct_change,\n                    (total_sales_millions - LAG(total_sales_millions, 1) OVER(PARTITION BY {group_col} ORDER BY intervention)) AS sales_difference_millions\n                FROM\n                    total_sales\n            )\n            SELECT\n                {group_col},\n\n                MAX(CASE WHEN intervention = '1 before' THEN total_sales_millions ELSE NULL END) AS total_sales_millions_before,\n                MAX(CASE WHEN intervention = '2 after' THEN total_sales_millions ELSE NULL END) AS total_sales_millions_after,\n\n                MAX(CASE WHEN intervention = '2 after' THEN sales_pct_change ELSE NULL END) AS sales_pct_change,\n                MAX(CASE WHEN intervention = '2 after' THEN sales_difference_millions ELSE NULL END) AS sales_difference_millions\n            FROM\n                sales_metrics\n            GROUP BY\n                {group_col}\n            ORDER BY\n                {group_col};\n            \"\"\"\n</pre> def sales_metrics_by_group_query(group_col: str, year: int, weeks: int) -&gt; str:     return f\"\"\"              WITH before_after AS (                 SELECT                     CASE                          WHEN week_date &lt; DATE '{year}-06-15' THEN '1 before'                          WHEN week_date &gt;= DATE '{year}-06-15' THEN '2 after'                      END AS intervention,                     sales,                     {group_col}                 FROM                     data_mart.sales                 WHERE                     1 = 1                     AND week_date &gt;= DATE_ADD('WEEK', -{weeks}, DATE '{year}-06-15')                      AND week_date &lt; DATE_ADD('WEEK', {weeks}, DATE '{year}-06-15')             ),              total_sales AS (                 SELECT                     {group_col},                     intervention,                     SUM(sales) / 1000000.0 AS total_sales_millions,                     SUM(sales) * 100.0 / SUM(SUM(sales)) OVER(PARTITION BY {group_col}) AS sales_pct                 FROM                     before_after                 GROUP BY                     {group_col},                     intervention             ),              sales_metrics AS (                 SELECT                     {group_col},                     intervention,                     total_sales_millions,                     sales_pct,                     100.0 * ((total_sales_millions / LAG(total_sales_millions, 1) OVER(PARTITION BY {group_col} ORDER BY intervention)) - 1) AS sales_pct_change,                     (total_sales_millions - LAG(total_sales_millions, 1) OVER(PARTITION BY {group_col} ORDER BY intervention)) AS sales_difference_millions                 FROM                     total_sales             )             SELECT                 {group_col},                  MAX(CASE WHEN intervention = '1 before' THEN total_sales_millions ELSE NULL END) AS total_sales_millions_before,                 MAX(CASE WHEN intervention = '2 after' THEN total_sales_millions ELSE NULL END) AS total_sales_millions_after,                  MAX(CASE WHEN intervention = '2 after' THEN sales_pct_change ELSE NULL END) AS sales_pct_change,                 MAX(CASE WHEN intervention = '2 after' THEN sales_difference_millions ELSE NULL END) AS sales_difference_millions             FROM                 sales_metrics             GROUP BY                 {group_col}             ORDER BY                 {group_col};             \"\"\" In\u00a0[295]: Copied! <pre>for group_col in [\"region\", \"platform\", \"age_band\", \"demographics\", \"customer_type\"]:\n    query = sales_metrics_by_group_query(group_col=group_col, year=2020, weeks=12)\n    athena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> for group_col in [\"region\", \"platform\", \"age_band\", \"demographics\", \"customer_type\"]:     query = sales_metrics_by_group_query(group_col=group_col, year=2020, weeks=12)     athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[295]: region total_sales_millions_before total_sales_millions_after sales_pct_change sales_difference_millions 0 AFRICA 1709.537097 1700.390303 -0.535045 -9.146794 1 ASIA 1637.244468 1583.807621 -3.263828 -53.436847 2 CANADA 426.438454 418.264441 -1.916810 -8.174013 3 EUROPE 108.886567 114.038959 4.731889 5.152392 4 OCEANIA 2354.116791 2282.795687 -3.029633 -71.321104 5 SOUTH AMERICA 213.036208 208.452033 -2.151829 -4.584175 6 USA 677.013562 666.198714 -1.597434 -10.814848 Out[295]: platform total_sales_millions_before total_sales_millions_after sales_pct_change sales_difference_millions 0 Retail 6906.861113 6738.777284 -2.433578 -168.083829 1 Shopify 219.412034 235.170474 7.182122 15.758440 Out[295]: age_band total_sales_millions_before total_sales_millions_after sales_pct_change sales_difference_millions 0 Middle Aged 1164.847638 1141.853349 -1.974017 -22.994289 1 Retirees 2395.264509 2365.714998 -1.233664 -29.549511 2 Unknown 2764.354472 2671.961443 -3.342300 -92.393029 3 Young Adults 801.806528 794.417968 -0.921489 -7.388560 Out[295]: demographics total_sales_millions_before total_sales_millions_after sales_pct_change sales_difference_millions 0 Couples 2033.589642 2015.977285 -0.866072 -17.612357 1 Families 2328.329033 2286.009030 -1.817613 -42.320003 2 Unknown 2764.354472 2671.961443 -3.342300 -92.393029 Out[295]: customer_type total_sales_millions_before total_sales_millions_after sales_pct_change sales_difference_millions 0 Existing 3690.116419 3606.243459 -2.272908 -83.872960 1 Guest 2573.436309 2496.233635 -2.999984 -77.202674 2 New 862.720419 871.470664 1.014262 8.750245 In\u00a0[298]: Copied! <pre>athena.drop_table(database=database, table=\"sales\", wait=wait)\n</pre> athena.drop_table(database=database, table=\"sales\", wait=wait) <pre>Query executed successfully\n</pre>"},{"location":"data_mart/#global","title":"Global\u00b6","text":""},{"location":"data_mart/#problem-statement","title":"Problem Statement\u00b6","text":"<p>Data Mart is an international supermarket specializing in fresh produce, operating through both retail and online platforms using a multi-region strategy. In June 2020, the company introduced sustainability-driven changes by implementing sustainable packaging methods across all products and stages of operation, from production to delivery.</p> <p>The objective of this analysis is to evaluate the impact of these changes on sales performance and address the following key questions:</p> <ol> <li>What was the quantifiable impact of the sustainable packaging initiative on overall sales performance?</li> <li>Which platform, region, segment, and customer type experienced the most significant impact from the changes?</li> <li>What strategies can be implemented to minimize potential negative effects on sales when introducing similar sustainability updates in the future?</li> </ol> <p>The analysis will utilize the available dataset, <code>data_mart.weekly_sales</code>, to generate actionable insights and recommendations.</p>"},{"location":"data_mart/#entity-relationship-diagram","title":"Entity Relationship Diagram\u00b6","text":""},{"location":"data_mart/#weekly-sales","title":"Weekly Sales\u00b6","text":"<ul> <li><p>week_date: The starting date of the sales week for each record.</p> </li> <li><p>region: Represents the geographical area of operations within Data Mart's multi-region strategy.</p> </li> <li><p>platform: Indicates whether sales occurred through the retail channel or the online Shopify storefront.</p> </li> <li><p>customer_segment: Categorizes customers based on demographic and age-related groupings.</p> </li> <li><p>customer_type: Provides additional demographic details, such as lifestyle or purchasing behavior.</p> </li> <li><p>transactions: The count of unique purchases made during the corresponding sales week.</p> </li> <li><p>sales: The total dollar amount of purchases made in the corresponding sales week.</p> </li> </ul>"},{"location":"data_mart/#tables","title":"Tables\u00b6","text":""},{"location":"data_mart/#sanity-checks","title":"Sanity Checks\u00b6","text":""},{"location":"data_mart/#duplicates","title":"Duplicates\u00b6","text":""},{"location":"data_mart/#nullity","title":"Nullity\u00b6","text":""},{"location":"data_mart/#eda","title":"EDA\u00b6","text":""},{"location":"data_mart/#unique-categories","title":"Unique Categories\u00b6","text":""},{"location":"data_mart/#sales-distribution","title":"Sales Distribution\u00b6","text":""},{"location":"data_mart/#transactions-count-distribution","title":"Transactions Count Distribution\u00b6","text":""},{"location":"data_mart/#sampling-period","title":"Sampling Period\u00b6","text":""},{"location":"data_mart/#data-cleaning","title":"Data Cleaning\u00b6","text":"<p>For <code>week_number</code>, the approach is to use <code>CEIL</code> function to round up the division of <code>DAY_OF_YEAR</code> by 7:</p> <pre>CAST(CEIL(DAY_OF_YEAR(week_date) / 7.0) AS INTEGER) AS week_number\n</pre>"},{"location":"data_mart/#data-exploration","title":"Data Exploration\u00b6","text":""},{"location":"data_mart/#q1","title":"Q1\u00b6","text":"<p>What day of the week is used for each <code>week_date</code> value?</p>"},{"location":"data_mart/#q2","title":"Q2\u00b6","text":"<p>What range of week numbers are missing from the dataset?</p>"},{"location":"data_mart/#q3","title":"Q3\u00b6","text":"<p>How many total <code>transactions</code> were there for each year in the dataset?</p>"},{"location":"data_mart/#q4","title":"Q4\u00b6","text":"<p>What is the total sales for each region for each month?</p>"},{"location":"data_mart/#q5","title":"Q5\u00b6","text":"<p>What is the total count of transactions for each platform?</p>"},{"location":"data_mart/#q6","title":"Q6\u00b6","text":"<p>What is the percentage of sales for Retail vs Shopify for each month?</p>"},{"location":"data_mart/#q7","title":"Q7\u00b6","text":"<p>What is the percentage of sales by demographic for each year in the dataset?</p>"},{"location":"data_mart/#q8","title":"Q8\u00b6","text":"<p>Which <code>age_band</code> and <code>demographic</code> values contribute the most to Retail sales?</p>"},{"location":"data_mart/#q9","title":"Q9\u00b6","text":"<p>Can we use the <code>avg_transaction</code> column to find the average transaction size for each year for Retail vs Shopify? If not - how would you calculate it instead?</p> <p>$$ \\text{Average Transaction} = \\frac{\\text{Total Sales in Dollars}}{\\text{Total Transactions}} $$</p> <p>This formula essentially calculates the average sales amount per transaction for each combination of the columns, i.e., <code>week_date</code>, <code>segment</code>, <code>customer_type</code>, <code>platform</code>, and <code>region</code>.</p> <p>In order to calculate the average transasction size for each year for Retail vs Shopify, we must group by <code>year</code>, <code>platform</code>, and <code>week_date</code> and then calculate the average transaction size.</p>"},{"location":"data_mart/#before-after-analysis","title":"Before &amp; After Analysis\u00b6","text":"<p>The intervention date is 2020-06-15.</p>"},{"location":"data_mart/#q1","title":"Q1\u00b6","text":"<p>What is the total sales for the 4 weeks before and after 2020-06-15? What is the growth or reduction rate in actual values and percentage of sales?</p>"},{"location":"data_mart/#not-including-the-last-day","title":"Not Including the Last Day\u00b6","text":"<pre>AND week_date &gt;= DATE_ADD('WEEK', -4, DATE '2020-06-15') \nAND week_date &lt; DATE_ADD('WEEK', 4, DATE '2020-06-15')\n</pre>"},{"location":"data_mart/#including-the-last-day","title":"Including the Last Day\u00b6","text":"<pre>AND week_date BETWEEN DATE_ADD('WEEK', -4, DATE '2020-06-15') AND DATE_ADD('WEEK', 4, DATE '2020-06-15')\n</pre>"},{"location":"data_mart/#before-after-cte","title":"Before &amp; After CTE\u00b6","text":""},{"location":"data_mart/#total-sales-cte","title":"Total Sales CTE\u00b6","text":""},{"location":"data_mart/#sales-metrics-cte","title":"Sales Metrics CTE\u00b6","text":""},{"location":"data_mart/#final-query-reshape-long-to-wide","title":"Final Query (Reshape Long to Wide)\u00b6","text":""},{"location":"data_mart/#q2","title":"Q2\u00b6","text":"<p>What about the entire 12 weeks before and after?</p>"},{"location":"data_mart/#q3","title":"Q3\u00b6","text":"<p>How do the sale metrics for these 2 periods before and after compare with the previous years in 2018 and 2019?</p>"},{"location":"data_mart/#bonus","title":"Bonus\u00b6","text":"<p>Which areas of the business have the highest negative impact in sales metrics performance in 2020 for the 12 week before and after period?</p> <ul> <li>Region</li> <li>Platform</li> <li>Age Band</li> <li>Demographic</li> <li>Customer Type</li> </ul>"},{"location":"data_mart/#clean-up","title":"Clean Up\u00b6","text":""},{"location":"erd/","title":"Entity Relationship Diagrams","text":""},{"location":"erd/#dannys-diner","title":"Danny's Diner","text":""},{"location":"erd/#erd","title":"ERD","text":"Danny's Diner"},{"location":"erd/#ddl","title":"DDL","text":""},{"location":"erd/#members","title":"Members","text":"<pre><code>CREATE EXTERNAL TABLE IF NOT EXISTS dannys_diner.members (\n  customer_id CHAR(1) COMMENT 'Unique identifier for the customer, represented as a single character',\n  join_date TIMESTAMP COMMENT 'Date and time when the customer joined the loyalty program'\n)\nCOMMENT 'The members table captures the dates when each customer joined the beta version of the Dannys Diner loyalty program'\nSTORED AS PARQUET\nLOCATION 's3://sql-case-studies/dannys_diner/members/'\nTBLPROPERTIES ('classification'='parquet', 'parquet.compress'='SNAPPY');\n</code></pre>"},{"location":"erd/#menu","title":"Menu","text":"<pre><code>CREATE EXTERNAL TABLE IF NOT EXISTS dannys_diner.menu (\n  product_id INT COMMENT 'Unique identifier for the product',\n  product_name STRING COMMENT 'Name of the product',\n  price DOUBLE COMMENT 'Price of the product'\n)\nCOMMENT 'The menu table maps the product IDs to the actual product names and prices'\nSTORED AS PARQUET\nLOCATION 's3://sql-case-studies/dannys_diner/menu/'\nTBLPROPERTIES ('classification'='parquet', 'parquet.compress'='SNAPPY');\n</code></pre>"},{"location":"erd/#sales","title":"Sales","text":"<pre><code>CREATE EXTERNAL TABLE IF NOT EXISTS dannys_diner.sales (\n  customer_id CHAR(1) COMMENT 'Unique identifier for the customer, represented as a single character',\n  order_date TIMESTAMP COMMENT 'Date and time when the order was placed',\n  product_id INT COMMENT 'Unique identifier for the product purchased'\n)\nCOMMENT 'The sales table captures all customer ID level purchases with order dates and product IDs'\nSTORED AS PARQUET\nLOCATION 's3://sql-case-studies/dannys_diner/sales/'\nTBLPROPERTIES ('classification'='parquet', 'parquet.compress'='SNAPPY');\n</code></pre>"},{"location":"erd/#data-bank","title":"Data Bank","text":""},{"location":"erd/#erd_1","title":"ERD","text":"Data Bank"},{"location":"erd/#ddl_1","title":"DDL","text":""},{"location":"erd/#region","title":"Region","text":"<pre><code>CREATE EXTERNAL TABLE IF NOT EXISTS data_bank.regions (\n  region_id INT COMMENT 'Unique identifier for the region',\n  region_name VARCHAR(9) COMMENT 'Name of the region, up to 9 characters'\n)\nCOMMENT 'The region table contains the region IDs and names'\nSTORED AS PARQUET\nLOCATION 's3://sql-case-studies/data_bank/regions/'\nTBLPROPERTIES ('classification'='parquet', 'parquet.compress'='SNAPPY');\n</code></pre>"},{"location":"erd/#customer-nodes","title":"Customer Nodes","text":"<pre><code>CREATE EXTERNAL TABLE IF NOT EXISTS data_bank.customer_nodes (\n  customer_id INT COMMENT 'Unique identifier for the customer',\n  region_id INT COMMENT 'Unique identifier for the region',\n  node_id INT COMMENT 'Unique identifier for the node within the region',\n  start_date TIMESTAMP COMMENT 'Date and time when the customer was assigned to the node',\n  end_date TIMESTAMP COMMENT 'Date and time when the customer was removed from the node, NULL if still active'\n)\nCOMMENT 'The customer nodes table stores the customer IDs, region IDs, node IDs, and the start and end dates that the customer was assigned to the node'\nSTORED AS PARQUET\nLOCATION 's3://sql-case-studies/data_bank/customer_nodes/'\nTBLPROPERTIES ('classification'='parquet', 'parquet.compress'='SNAPPY');\n</code></pre>"},{"location":"erd/#customer-transactions","title":"Customer Transactions","text":"<pre><code>CREATE EXTERNAL TABLE IF NOT EXISTS data_bank.customer_transactions (\n  customer_id INT COMMENT 'Unique identifier for the customer',\n  txn_date TIMESTAMP COMMENT 'Date and time of the transaction',\n  txn_type VARCHAR(10) COMMENT 'Type of transaction: deposit, withdrawal, or purchase',\n  txn_amount DOUBLE COMMENT 'Amount involved in the transaction'\n)\nCOMMENT 'The customer transactions table stores all customer deposits, withdrawals, and purchases'\nSTORED AS PARQUET\nLOCATION 's3://sql-case-studies/data_bank/customer_transactions/'\nTBLPROPERTIES ('classification'='parquet', 'parquet.compress'='SNAPPY');\n</code></pre>"},{"location":"erd/#trading","title":"Trading","text":""},{"location":"erd/#erd_2","title":"ERD","text":"Daily Bitcoin Prices"},{"location":"erd/#ddl_2","title":"DDL","text":""},{"location":"erd/#daily-bitcoin-price","title":"Daily Bitcoin Price","text":"<pre><code>CREATE EXTERNAL TABLE IF NOT EXISTS trading.daily_btc (\n  market_date TIMESTAMP COMMENT 'Cryptocurrency markets trade daily with no holidays',\n  open_price DOUBLE COMMENT '$ USD price at the beginning of the day',\n  high_price DOUBLE COMMENT 'Intra-day highest sell price in $ USD',\n  low_price DOUBLE COMMENT 'Intra-day lowest sell price in $ USD',\n  close_price DOUBLE COMMENT '$ USD price at the end of the day',\n  adjusted_close_price DOUBLE COMMENT '$ USD price after splits and dividend distributions',\n  volume DOUBLE COMMENT 'The daily amount of traded units of cryptocurrency'\n)\nCOMMENT 'Daily Bitcoin trading data containing the open, high, low, close, adjusted close prices, and volume'\nSTORED AS PARQUET\nLOCATION 's3://sql-case-studies/trading/daily_btc/'\nTBLPROPERTIES ('classification'='parquet', 'parquet.compress'='SNAPPY');\n</code></pre>"},{"location":"erd/#clique-bait","title":"Clique Bait","text":""},{"location":"erd/#erd_3","title":"ERD","text":"Clique Bait"},{"location":"erd/#ddl_3","title":"DDL","text":""},{"location":"erd/#users","title":"Users","text":"<pre><code>CREATE EXTERNAL TABLE IF NOT EXISTS clique_bait.users (\n  user_id INT COMMENT 'Unique identifier for the user',\n  cookie_id VARCHAR(6) COMMENT 'Unique identifier for the user\u2019s browser session, represented as a cookie ID',\n  start_date TIMESTAMP COMMENT 'Date and time when the user first visited the website'\n)\nCOMMENT 'The users table stores customers who visit the Clique Bait website and their tagged cookie IDs'\nSTORED AS PARQUET\nLOCATION 's3://sql-case-studies/clique_bait/users/'\nTBLPROPERTIES ('classification'='parquet', 'parquet.compress'='SNAPPY');\n</code></pre>"},{"location":"erd/#events","title":"Events","text":"<pre><code>CREATE EXTERNAL TABLE IF NOT EXISTS clique_bait.events (\n  visit_id VARCHAR(6) COMMENT 'Unique identifier for the visit session',\n  cookie_id VARCHAR(6) COMMENT \"Unique identifier for the user's browser session, represented as a cookie ID\",\n  page_id INT COMMENT 'Unique identifier for the page viewed during the event',\n  event_type INT COMMENT 'Type of event that occurred (e.g., page view, add to cart, purchase)',\n  sequence_number INT COMMENT 'Order of the event in the sequence of actions during the visit',\n  event_time TIMESTAMP COMMENT 'Date and time when the event occurred'\n)\nCOMMENT 'The events table captures all customers visits that are logged at the cookie ID level'\nSTORED AS PARQUET\nLOCATION 's3://sql-case-studies/clique_bait/events/'\nTBLPROPERTIES ('classification'='parquet', 'parquet.compress'='SNAPPY');\n</code></pre>"},{"location":"erd/#event-identifier","title":"Event Identifier","text":"<pre><code>CREATE EXTERNAL TABLE IF NOT EXISTS clique_bait.event_identifier (\n  event_type INT COMMENT 'Unique identifier for the type of event',\n  event_name VARCHAR(100) COMMENT 'Name or description of the event type'\n)\nCOMMENT 'The event identifier table stores the types of events that are captured by the system'\nSTORED AS PARQUET\nLOCATION 's3://sql-case-studies/clique_bait/event_identifier/'\nTBLPROPERTIES ('classification'='parquet', 'parquet.compress'='SNAPPY');\n</code></pre>"},{"location":"erd/#campaign-identifer","title":"Campaign Identifer","text":"<pre><code>CREATE EXTERNAL TABLE IF NOT EXISTS clique_bait.campaign_identifier (\n  campaign_id INT COMMENT 'Unique identifier for the campaign',\n  products VARCHAR(3) COMMENT 'List of products associated with the campaign, represented as a short code',\n  campaign_name VARCHAR(100) COMMENT 'Name of the campaign',\n  start_date TIMESTAMP COMMENT 'Date and time when the campaign started',\n  end_date TIMESTAMP COMMENT 'Date and time when the campaign ended'\n)\nCOMMENT 'The campaign identifier table stores the three campaigns run by Clique Bait so far'\nSTORED AS PARQUET\nLOCATION 's3://sql-case-studies/clique_bait/campaign_identifier/'\nTBLPROPERTIES ('classification'='parquet', 'parquet.compress'='SNAPPY');\n</code></pre>"},{"location":"erd/#page-hierachy","title":"Page Hierachy","text":"<pre><code>CREATE EXTERNAL TABLE IF NOT EXISTS clique_bait.page_hierarchy (\n  page_id INT COMMENT 'Unique identifier for the page',\n  page_name VARCHAR(50) COMMENT 'Name of the page on the website',\n  product_category VARCHAR(50) COMMENT 'Category of the product featured on the page',\n  product_id INT COMMENT 'Unique identifier for the product featured on the page'\n)\nCOMMENT 'The page hierarchy table lists all pages on the Clique Bait website'\nSTORED AS PARQUET\nLOCATION 's3://sql-case-studies/clique_bait/page_hierarchy/'\nTBLPROPERTIES ('classification'='parquet', 'parquet.compress'='SNAPPY');\n</code></pre>"},{"location":"erd/#fresh-segments","title":"Fresh Segments","text":""},{"location":"erd/#erd_4","title":"ERD","text":"Fresh Segments"},{"location":"erd/#ddl_4","title":"DDL","text":""},{"location":"erd/#interest-map","title":"Interest Map","text":"<pre><code>CREATE EXTERNAL TABLE IF NOT EXISTS fresh_segments.interest_map (\n  id INT COMMENT 'Unique identifier for the interest',\n  interest_name VARCHAR(100) COMMENT 'Name of the interest',\n  interest_summary VARCHAR(500) COMMENT 'Brief summary or description of the interest',\n  created_at TIMESTAMP COMMENT 'Timestamp when the record was created',\n  last_modified TIMESTAMP COMMENT 'Timestamp when the record was last modified'\n)\nCOMMENT 'The interest map table links the interest IDs with the relevant interest names and summaries'\nSTORED AS PARQUET\nLOCATION 's3://sql-case-studies/fresh_segments/interest_map/'\nTBLPROPERTIES ('classification'='parquet', 'parquet.compress'='SNAPPY');\n</code></pre>"},{"location":"erd/#interest-metrics","title":"Interest Metrics","text":"<pre><code>CREATE EXTERNAL TABLE IF NOT EXISTS fresh_segments.interest_metrics (\n  record_month INT COMMENT 'Represents the month of the record',\n  record_year INT COMMENT 'Represents the year of the record',\n  month_year VARCHAR(7) COMMENT 'Month and year concatenated as a string in the format MM-YYYY',\n  interest_id INT COMMENT 'Unique identifier for the interest',\n  composition DOUBLE COMMENT 'Represents the composition percentage of the interest (e.g., 11.89% of the client\u2019s customer list interacted with the interest)',\n  index_value DOUBLE COMMENT 'Index value indicating how much higher the composition value is compared to the average composition value for all Fresh Segments clients\u2019 customers for this interest in the same month (e.g., 6.19 means 6.19x the average)',\n  ranking INT COMMENT 'Ranking of the interest based on the index value in the given month year (e.g., 1 for the highest index value)',\n  percentile_ranking DOUBLE COMMENT 'Percentile ranking of the interest based on its index value, indicating its position relative to other interests in the same month year (e.g., 99.86 means it is in the top 0.14%)'\n)\nCOMMENT \"The interest metrics table represents the performance of specific interests based on the client's customer base\"\nSTORED AS PARQUET\nLOCATION 's3://sql-case-studies/fresh_segments/interest_metrics/'\nTBLPROPERTIES ('classification'='parquet', 'parquet.compress'='SNAPPY');\n</code></pre>"},{"location":"erd/#data-mart","title":"Data Mart","text":""},{"location":"erd/#erd_5","title":"ERD","text":"Data Mart"},{"location":"erd/#ddl_5","title":"DDL","text":""},{"location":"erd/#weekly-sales","title":"Weekly Sales","text":"<pre><code>CREATE EXTERNAL TABLE IF NOT EXISTS data_mart.weekly_sales (\n    week_date TIMESTAMP COMMENT 'The starting date of the sales week for each record',\n    region VARCHAR(20) COMMENT \"Represents the geographical area of operations within Data Mart's multi-region strategy\",\n    platform VARCHAR(10) COMMENT 'Indicates whether sales occurred through the retail channel or the online Shopify storefront',\n    segment VARCHAR(10) COMMENT 'Categorizes customers based on demographic and age-related groupings',\n    customer_type VARCHAR(10) COMMENT 'Provides additional demographic details, such as lifestyle or purchasing behavior',\n    transactions INT COMMENT 'The count of unique purchases made during the corresponding sales week',\n    sales DOUBLE COMMENT 'The total dollar amount of purchases made in the corresponding sales week'\n) \nCOMMENT 'Sales data containing weekly transaction and sales information by region, platform, segment, and customer type' \nSTORED AS PARQUET LOCATION 's3://sql-case-studies/data_mart/weekly_sales/' TBLPROPERTIES (\n    'classification' = 'parquet',\n    'parquet.compress' = 'SNAPPY'\n);\n</code></pre>"},{"location":"erd/#balanced-tree","title":"Balanced Tree","text":""},{"location":"erd/#erd_6","title":"ERD","text":"Balanced Tree"},{"location":"erd/#ddl_6","title":"DDL","text":""},{"location":"erd/#sales_1","title":"Sales","text":"<pre><code>CREATE EXTERNAL TABLE IF NOT EXISTS balanced_tree.sales (\n  prod_id VARCHAR(6) COMMENT 'Unique identifier for the product',\n  qty SMALLINT COMMENT 'Quantity of the product purchased in the transaction',\n  price SMALLINT COMMENT 'Price of the product in the transaction',\n  discount SMALLINT COMMENT 'Discount percentage applied to the product',\n  member VARCHAR(1) COMMENT 'Membership status of the buyer (e.g., t for true, f for false)',\n  txn_id VARCHAR(6) COMMENT 'Unique identifier for the transaction',\n  start_txn_time TIMESTAMP COMMENT 'Timestamp of when the transaction started'\n)\nCOMMENT 'This table contains product-level transaction data, including quantities, prices, discounts, membership status, transaction IDs, and transaction timestamps'\nSTORED AS PARQUET\nLOCATION 's3://sql-case-studies/balanced_tree/sales/'\nTBLPROPERTIES ('classification'='parquet', 'parquet.compress'='SNAPPY');\n</code></pre>"},{"location":"erd/#product-details","title":"Product Details","text":"<pre><code>CREATE EXTERNAL TABLE IF NOT EXISTS balanced_tree.product_details (\n  product_id VARCHAR(6) COMMENT 'Unique identifier for the product',\n  price SMALLINT COMMENT 'Price of the product in the store',\n  product_name VARCHAR(50) COMMENT 'Name of the product',\n  category_id SMALLINT COMMENT 'Unique identifier for the category',\n  segment_id SMALLINT COMMENT 'Unique identifier for the segment',\n  style_id SMALLINT COMMENT 'Unique identifier for the style',\n  category_name VARCHAR(10) COMMENT 'Name of the category',\n  segment_name VARCHAR(10) COMMENT 'Name of the segment',\n  style_name VARCHAR(50) COMMENT 'Name of the style'\n)\nCOMMENT 'The product details table includes all information about the products featured in the store'\nSTORED AS PARQUET\nLOCATION 's3://sql-case-studies/balanced_tree/product_details/'\nTBLPROPERTIES ('classification'='parquet', 'parquet.compress'='SNAPPY');\n</code></pre>"},{"location":"erd/#product-prices","title":"Product Prices","text":"<pre><code>CREATE EXTERNAL TABLE IF NOT EXISTS balanced_tree.product_prices (\n  id SMALLINT COMMENT 'Unique identifier for the price record',\n  product_id VARCHAR(6) COMMENT 'Unique identifier for the product',\n  price SMALLINT COMMENT 'Price of the product in the store'\n)\nCOMMENT 'This table stores the pricing information for products, including product identifiers and their corresponding prices'\nSTORED AS PARQUET\nLOCATION 's3://sql-case-studies/balanced_tree/product_prices/'\nTBLPROPERTIES ('classification'='parquet', 'parquet.compress'='SNAPPY');\n</code></pre>"},{"location":"erd/#product-hierarchy","title":"Product Hierarchy","text":"<pre><code>CREATE EXTERNAL TABLE IF NOT EXISTS balanced_tree.product_hierarchy (\n  id SMALLINT COMMENT 'Unique identifier for the entry in the hierarchy',\n  parent_id SMALLINT COMMENT 'Parent identifier for the current level (NULL for top-level categories)',\n  level_text VARCHAR(30) COMMENT 'Name or description of the hierarchy level (e.g., product type or style)',\n  level_name VARCHAR(20) COMMENT 'Name of the hierarchy level (e.g., Category, Segment, or Style)'\n)\nCOMMENT 'This table represents the hierarchical structure of product categories, segments, and styles for the store, and each entry defines the relationship between levels, starting from top-level categories down to individual product styles'\nSTORED AS PARQUET\nLOCATION 's3://sql-case-studies/balanced_tree/product_hierarchy/'\nTBLPROPERTIES ('classification'='parquet', 'parquet.compress'='SNAPPY');\n</code></pre>"},{"location":"erd/#foodie-fi","title":"Foodie Fi","text":""},{"location":"erd/#erd_7","title":"ERD","text":"Foodie Fi"},{"location":"erd/#ddl_7","title":"DDL","text":""},{"location":"erd/#plans","title":"Plans","text":"<pre><code>CREATE EXTERNAL TABLE IF NOT EXISTS foodie_fi.plans (\n  plan_id TINYINT COMMENT 'Unique identifier for the plan',\n  plan_name VARCHAR(20) COMMENT 'Name of the subscription plan',\n  price FLOAT COMMENT 'Price of the subscription plan'\n)\nCOMMENT 'The plans table contains information about the different subscription plans available, including churn events'\nSTORED AS PARQUET\nLOCATION 's3://sql-case-studies/foodie_fi/plans/'\nTBLPROPERTIES ('classification'='parquet', 'parquet.compress'='SNAPPY');\n</code></pre>"},{"location":"erd/#subscriptions","title":"Subscriptions","text":"<pre><code>CREATE EXTERNAL TABLE IF NOT EXISTS foodie_fi.subscriptions (\n  customer_id SMALLINT COMMENT 'Unique identifier for the customer',\n  plan_id TINYINT COMMENT 'Unique identifier for the plan associated with the subscription',\n  start_date TIMESTAMP COMMENT 'Start date of the subscription'\n)\nCOMMENT 'The subscriptions table stores information about customer subscriptions to various plans'\nSTORED AS PARQUET\nLOCATION 's3://sql-case-studies/foodie_fi/subscriptions/'\nTBLPROPERTIES ('classification'='parquet', 'parquet.compress'='SNAPPY');\n</code></pre>"},{"location":"foodie_fi/","title":"Foodie Fi","text":"In\u00a0[1]: Copied! <pre>from IPython.core.interactiveshell import InteractiveShell\n\nInteractiveShell.ast_node_interactivity = \"all\"\n\nimport os\nimport sys\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsys.path.append(\"../../../\")\nfrom src.athena import Athena\nfrom src.utils import create_session\n</pre> from IPython.core.interactiveshell import InteractiveShell  InteractiveShell.ast_node_interactivity = \"all\"  import os import sys  import matplotlib.pyplot as plt import seaborn as sns  sys.path.append(\"../../../\") from src.athena import Athena from src.utils import create_session In\u00a0[11]: Copied! <pre>boto3_session = create_session(\n    profile_name=\"dev\",\n    role_arn=os.getenv(\"ATHENA_IAM_ROLE_ARN\"),\n)\n\nwait = True\nctas_approach = False\n\ndatabase = \"foodie_fi\"\ntables = [\"plans\", \"subscriptions\"]\nsql_path = \"../sql/\"\n\nathena = Athena(boto3_session=boto3_session, s3_output=os.getenv(\"ATHENA_S3_OUTPUT\"))\nathena\n</pre> boto3_session = create_session(     profile_name=\"dev\",     role_arn=os.getenv(\"ATHENA_IAM_ROLE_ARN\"), )  wait = True ctas_approach = False  database = \"foodie_fi\" tables = [\"plans\", \"subscriptions\"] sql_path = \"../sql/\"  athena = Athena(boto3_session=boto3_session, s3_output=os.getenv(\"ATHENA_S3_OUTPUT\")) athena Out[11]: <pre>Athena(boto3_session=Session(region_name='us-east-1'), s3_output=s3://sql-case-studies/query_results)</pre> In\u00a0[4]: Copied! <pre>for table in tables:\n    athena.query(\n        database=database,\n        query=f\"\"\" \n                SELECT\n                    *\n                FROM\n                    {database}.{table} TABLESAMPLE BERNOULLI(50);\n              \"\"\",\n        ctas_approach=ctas_approach,\n    )\n</pre> for table in tables:     athena.query(         database=database,         query=f\"\"\"                  SELECT                     *                 FROM                     {database}.{table} TABLESAMPLE BERNOULLI(50);               \"\"\",         ctas_approach=ctas_approach,     ) Out[4]: plan_id plan_name price 0 0 trial 0.0 1 1 basic monthly 9.9 2 2 pro monthly 19.9 3 3 pro annual 199.0 Out[4]: customer_id plan_id start_date 0 1 1 2020-08-08 1 2 3 2020-09-27 2 3 0 2020-01-13 3 3 1 2020-01-20 4 4 0 2020-01-17 ... ... ... ... 1249 998 0 2020-10-12 1250 998 2 2020-10-19 1251 999 0 2020-10-23 1252 1000 2 2020-03-26 1253 1000 4 2020-06-04 <p>1254 rows \u00d7 3 columns</p> In\u00a0[6]: Copied! <pre>query = \"\"\" \nSELECT\n    COUNT(DISTINCT customer_id) AS customer_count\nFROM\n    foodie_fi.subscriptions;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  SELECT     COUNT(DISTINCT customer_id) AS customer_count FROM     foodie_fi.subscriptions; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[6]: customer_count 0 1000 In\u00a0[18]: Copied! <pre>query = \"\"\" \nSELECT\n    DATE_TRUNC('MONTH', start_date) AS month_year,\n    COUNT(DISTINCT customer_id) AS count\nFROM\n    foodie_fi.subscriptions AS s\n        LEFT JOIN foodie_fi.plans AS p ON s.plan_id = p.plan_id\nWHERE\n    1 = 1\n    AND p.plan_name = 'trial'\nGROUP BY\n    DATE_TRUNC('MONTH', start_date)\nORDER BY\n    DATE_TRUNC('MONTH', start_date) ASC;\n\"\"\"\n\nmonthly_trial_counts = athena.query(\n    database=database, query=query, ctas_approach=ctas_approach\n)\nmonthly_trial_counts\n</pre> query = \"\"\"  SELECT     DATE_TRUNC('MONTH', start_date) AS month_year,     COUNT(DISTINCT customer_id) AS count FROM     foodie_fi.subscriptions AS s         LEFT JOIN foodie_fi.plans AS p ON s.plan_id = p.plan_id WHERE     1 = 1     AND p.plan_name = 'trial' GROUP BY     DATE_TRUNC('MONTH', start_date) ORDER BY     DATE_TRUNC('MONTH', start_date) ASC; \"\"\"  monthly_trial_counts = athena.query(     database=database, query=query, ctas_approach=ctas_approach ) monthly_trial_counts Out[18]: month_year count 0 2020-01-01 88 1 2020-02-01 68 2 2020-03-01 94 3 2020-04-01 81 4 2020-05-01 88 5 2020-06-01 79 6 2020-07-01 89 7 2020-08-01 88 8 2020-09-01 87 9 2020-10-01 79 10 2020-11-01 75 11 2020-12-01 84 In\u00a0[21]: Copied! <pre>monthly_trial_counts[\"month_label\"] = monthly_trial_counts[\"month_year\"].dt.strftime(\n    \"%b\"\n)\n\nplt.figure(figsize=(10, 6))\nplt.bar(\n    monthly_trial_counts[\"month_label\"],\n    monthly_trial_counts[\"count\"],\n    alpha=0.7,\n    edgecolor=\"black\",\n)\n\n# Adding labels on top of the bars\nfor i, count in enumerate(monthly_trial_counts[\"count\"]):\n    plt.text(i, count + 1, str(count), ha=\"center\", va=\"bottom\", fontsize=10)\n\nplt.title(\"Monthly Trial Counts\", fontsize=14)\nplt.xlabel(\"Month\", fontsize=12)\nplt.ylabel(\"Count\", fontsize=12)\nplt.xticks(fontsize=10)\nplt.yticks(fontsize=10)\nplt.grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\nplt.tight_layout()\nplt.show();\n</pre> monthly_trial_counts[\"month_label\"] = monthly_trial_counts[\"month_year\"].dt.strftime(     \"%b\" )  plt.figure(figsize=(10, 6)) plt.bar(     monthly_trial_counts[\"month_label\"],     monthly_trial_counts[\"count\"],     alpha=0.7,     edgecolor=\"black\", )  # Adding labels on top of the bars for i, count in enumerate(monthly_trial_counts[\"count\"]):     plt.text(i, count + 1, str(count), ha=\"center\", va=\"bottom\", fontsize=10)  plt.title(\"Monthly Trial Counts\", fontsize=14) plt.xlabel(\"Month\", fontsize=12) plt.ylabel(\"Count\", fontsize=12) plt.xticks(fontsize=10) plt.yticks(fontsize=10) plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.6) plt.tight_layout() plt.show(); In\u00a0[24]: Copied! <pre>query = \"\"\" \nSELECT\n    p.plan_name,\n    MIN(s.start_date) AS first_start_date,\n    MAX(s.start_date) AS last_start_date\nFROM\n    foodie_fi.subscriptions AS s \n        LEFT JOIN foodie_fi.plans AS p ON s.plan_id = p.plan_id\nGROUP BY\n    p.plan_name;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  SELECT     p.plan_name,     MIN(s.start_date) AS first_start_date,     MAX(s.start_date) AS last_start_date FROM     foodie_fi.subscriptions AS s          LEFT JOIN foodie_fi.plans AS p ON s.plan_id = p.plan_id GROUP BY     p.plan_name; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[24]: plan_name first_start_date last_start_date 0 trial 2020-01-01 2020-12-30 1 basic monthly 2020-01-08 2021-01-06 2 churn 2020-01-12 2021-04-29 3 pro monthly 2020-01-08 2021-04-28 4 pro annual 2020-01-10 2021-04-30 In\u00a0[35]: Copied! <pre>query = \"\"\" \nSELECT\n    p.plan_name,\n    COUNT(*) AS events\nFROM\n    foodie_fi.subscriptions AS s \n        LEFT JOIN foodie_fi.plans AS p ON s.plan_id = p.plan_id\nWHERE\n    EXTRACT(YEAR FROM s.start_date) &gt; 2020\nGROUP BY\n    p.plan_name\nORDER BY\n    events DESC;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  SELECT     p.plan_name,     COUNT(*) AS events FROM     foodie_fi.subscriptions AS s          LEFT JOIN foodie_fi.plans AS p ON s.plan_id = p.plan_id WHERE     EXTRACT(YEAR FROM s.start_date) &gt; 2020 GROUP BY     p.plan_name ORDER BY     events DESC; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[35]: plan_name events 0 churn 71 1 pro annual 63 2 pro monthly 60 3 basic monthly 8 In\u00a0[43]: Copied! <pre>query = \"\"\" \nSELECT\n    SUM(CASE WHEN p.plan_name = 'churn' THEN 1.0 ELSE 0.0 END) AS count_churned,\n    ROUND((SUM(CASE WHEN p.plan_name = 'churn' THEN 1.0 ELSE 0.0 END) / COUNT(DISTINCT s.customer_id)) * 100.0, 1) AS pct_churned\nFROM\n    foodie_fi.subscriptions AS s\n        LEFT JOIN foodie_fi.plans AS p ON s.plan_id = p.plan_id;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  SELECT     SUM(CASE WHEN p.plan_name = 'churn' THEN 1.0 ELSE 0.0 END) AS count_churned,     ROUND((SUM(CASE WHEN p.plan_name = 'churn' THEN 1.0 ELSE 0.0 END) / COUNT(DISTINCT s.customer_id)) * 100.0, 1) AS pct_churned FROM     foodie_fi.subscriptions AS s         LEFT JOIN foodie_fi.plans AS p ON s.plan_id = p.plan_id; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[43]: count_churned pct_churned 0 307.0 30.7 In\u00a0[50]: Copied! <pre>query = \"\"\" \nSELECT\n    s.customer_id,\n    ARRAY_AGG(p.plan_name ORDER BY s.start_date ASC) AS plan_progression,\n    ARRAY['trial', 'churn'] AS churn_pattern\nFROM\n    foodie_fi.subscriptions AS s\n        LEFT JOIN foodie_fi.plans AS p ON s.plan_id = p.plan_id\nGROUP BY\n    s.customer_id;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  SELECT     s.customer_id,     ARRAY_AGG(p.plan_name ORDER BY s.start_date ASC) AS plan_progression,     ARRAY['trial', 'churn'] AS churn_pattern FROM     foodie_fi.subscriptions AS s         LEFT JOIN foodie_fi.plans AS p ON s.plan_id = p.plan_id GROUP BY     s.customer_id; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[50]: customer_id plan_progression churn_pattern 0 9 [trial, pro annual] [trial, churn] 1 13 [trial, basic monthly, pro monthly] [trial, churn] 2 15 [trial, pro monthly, churn] [trial, churn] 3 20 [trial, basic monthly, pro annual] [trial, churn] 4 26 [trial, pro monthly] [trial, churn] ... ... ... ... 995 987 [trial, churn] [trial, churn] 996 989 [trial, pro monthly, pro annual] [trial, churn] 997 993 [trial, churn] [trial, churn] 998 995 [trial, basic monthly, pro monthly] [trial, churn] 999 996 [trial, basic monthly, churn] [trial, churn] <p>1000 rows \u00d7 3 columns</p> In\u00a0[58]: Copied! <pre>query = \"\"\" \nWITH customer_journeys AS (\n    SELECT\n        s.customer_id,\n        ARRAY_AGG(p.plan_name ORDER BY s.start_date ASC) AS plan_progression,\n        ARRAY['trial', 'churn'] AS churn_pattern\n    FROM\n        foodie_fi.subscriptions AS s\n            LEFT JOIN foodie_fi.plans AS p ON s.plan_id = p.plan_id\n    GROUP BY\n        s.customer_id\n)\n\nSELECT\n    COUNT(DISTINCT customer_id) AS churn_after_trial_count,\n    (CAST(COUNT(DISTINCT customer_id) AS DOUBLE) / (SELECT COUNT(DISTINCT customer_id) FROM foodie_fi.subscriptions)) * 100.0 AS churn_after_trial_pct\nFROM\n    customer_journeys\nWHERE\n    1 = 1\n    AND CARDINALITY(plan_progression) = 2\n    AND CARDINALITY(ARRAY_INTERSECT(churn_pattern, plan_progression)) = CARDINALITY(churn_pattern);\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  WITH customer_journeys AS (     SELECT         s.customer_id,         ARRAY_AGG(p.plan_name ORDER BY s.start_date ASC) AS plan_progression,         ARRAY['trial', 'churn'] AS churn_pattern     FROM         foodie_fi.subscriptions AS s             LEFT JOIN foodie_fi.plans AS p ON s.plan_id = p.plan_id     GROUP BY         s.customer_id )  SELECT     COUNT(DISTINCT customer_id) AS churn_after_trial_count,     (CAST(COUNT(DISTINCT customer_id) AS DOUBLE) / (SELECT COUNT(DISTINCT customer_id) FROM foodie_fi.subscriptions)) * 100.0 AS churn_after_trial_pct FROM     customer_journeys WHERE     1 = 1     AND CARDINALITY(plan_progression) = 2     AND CARDINALITY(ARRAY_INTERSECT(churn_pattern, plan_progression)) = CARDINALITY(churn_pattern); \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[58]: churn_after_trial_count churn_after_trial_pct 0 92 9.2 In\u00a0[65]: Copied! <pre>query = \"\"\" \nWITH ranked_data AS (\n    SELECT\n        s.customer_id,\n        p.plan_name,\n        RANK() OVER(PARTITION BY s.customer_id ORDER BY s.start_date ASC) AS ranks\n    FROM\n        foodie_fi.subscriptions AS s\n            LEFT JOIN foodie_fi.plans AS p ON s.plan_id = p.plan_id\n)\nSELECT\n    DISTINCT plan_name\nFROM\n    ranked_data\nWHERE\n    ranks = 1;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  WITH ranked_data AS (     SELECT         s.customer_id,         p.plan_name,         RANK() OVER(PARTITION BY s.customer_id ORDER BY s.start_date ASC) AS ranks     FROM         foodie_fi.subscriptions AS s             LEFT JOIN foodie_fi.plans AS p ON s.plan_id = p.plan_id ) SELECT     DISTINCT plan_name FROM     ranked_data WHERE     ranks = 1; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[65]: plan_name 0 trial <p>Given that this assumption is valid, we can simply check the second <code>ranks = 2</code> plan for each customer to determine if they churned after their trial.</p> In\u00a0[76]: Copied! <pre>query = \"\"\" \nWITH ranked_data AS (\n    SELECT\n        s.customer_id,\n        p.plan_name,\n        RANK() OVER(PARTITION BY s.customer_id ORDER BY s.start_date ASC) AS ranks\n    FROM\n        foodie_fi.subscriptions AS s\n            LEFT JOIN foodie_fi.plans AS p ON s.plan_id = p.plan_id\n)\nSELECT\n    SUM(CASE WHEN plan_name = 'churn' THEN 1.0 ELSE 0.0 END) AS churn_after_trial_count,\n    (SUM(CASE WHEN plan_name = 'churn' THEN 1.0 ELSE 0.0 END) / COUNT(DISTINCT customer_id)) * 100.0 AS churn_after_trial_pct\nFROM\n    ranked_data\nWHERE\n    1 = 1\n    AND ranks = 2;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  WITH ranked_data AS (     SELECT         s.customer_id,         p.plan_name,         RANK() OVER(PARTITION BY s.customer_id ORDER BY s.start_date ASC) AS ranks     FROM         foodie_fi.subscriptions AS s             LEFT JOIN foodie_fi.plans AS p ON s.plan_id = p.plan_id ) SELECT     SUM(CASE WHEN plan_name = 'churn' THEN 1.0 ELSE 0.0 END) AS churn_after_trial_count,     (SUM(CASE WHEN plan_name = 'churn' THEN 1.0 ELSE 0.0 END) / COUNT(DISTINCT customer_id)) * 100.0 AS churn_after_trial_pct FROM     ranked_data WHERE     1 = 1     AND ranks = 2; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[76]: churn_after_trial_count churn_after_trial_pct 0 92.0 9.2 In\u00a0[84]: Copied! <pre>query = \"\"\" \nWITH ranked_data AS (\n    SELECT\n        s.customer_id,\n        p.plan_name,\n        RANK() OVER(PARTITION BY s.customer_id ORDER BY s.start_date ASC) AS ranks\n    FROM\n        foodie_fi.subscriptions AS s\n            LEFT JOIN foodie_fi.plans AS p ON s.plan_id = p.plan_id\n)\nSELECT\n    plan_name,\n    COUNT(DISTINCT customer_id) AS customer_count,\n    CAST(COUNT(DISTINCT customer_id) AS DOUBLE) / SUM(COUNT(DISTINCT customer_id)) OVER() * 100.0 AS customer_pct\nFROM\n    ranked_data\nWHERE\n    1 = 1\n    AND ranks = 2\nGROUP BY\n    plan_name\nORDER BY\n    customer_count DESC;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  WITH ranked_data AS (     SELECT         s.customer_id,         p.plan_name,         RANK() OVER(PARTITION BY s.customer_id ORDER BY s.start_date ASC) AS ranks     FROM         foodie_fi.subscriptions AS s             LEFT JOIN foodie_fi.plans AS p ON s.plan_id = p.plan_id ) SELECT     plan_name,     COUNT(DISTINCT customer_id) AS customer_count,     CAST(COUNT(DISTINCT customer_id) AS DOUBLE) / SUM(COUNT(DISTINCT customer_id)) OVER() * 100.0 AS customer_pct FROM     ranked_data WHERE     1 = 1     AND ranks = 2 GROUP BY     plan_name ORDER BY     customer_count DESC; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[84]: plan_name customer_count customer_pct 0 basic monthly 546 54.6 1 pro monthly 325 32.5 2 churn 92 9.2 3 pro annual 37 3.7 In\u00a0[96]: Copied! <pre>query = \"\"\" \nWITH ranked_start_dates AS (\n    SELECT\n        s.customer_id,\n        p.plan_name,\n        RANK() OVER(PARTITION BY s.customer_id ORDER BY s.start_date DESC) AS ranks\n    FROM\n        foodie_fi.subscriptions AS s\n            LEFT JOIN foodie_fi.plans AS p ON s.plan_id = p.plan_id\n    WHERE\n        1 = 1\n        AND s.start_date &lt;= DATE '2020-12-31'\n)\nSELECT\n    plan_name,\n    COUNT(DISTINCT customer_id) AS customer_count,\n    CAST(COUNT(DISTINCT customer_id) AS DOUBLE) / SUM(COUNT(DISTINCT customer_id)) OVER() * 100.0 AS customer_pct\nFROM\n    ranked_start_dates\nWHERE\n    1 = 1\n    AND ranks = 1\nGROUP BY\n    plan_name\nORDER BY\n    customer_count DESC;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  WITH ranked_start_dates AS (     SELECT         s.customer_id,         p.plan_name,         RANK() OVER(PARTITION BY s.customer_id ORDER BY s.start_date DESC) AS ranks     FROM         foodie_fi.subscriptions AS s             LEFT JOIN foodie_fi.plans AS p ON s.plan_id = p.plan_id     WHERE         1 = 1         AND s.start_date &lt;= DATE '2020-12-31' ) SELECT     plan_name,     COUNT(DISTINCT customer_id) AS customer_count,     CAST(COUNT(DISTINCT customer_id) AS DOUBLE) / SUM(COUNT(DISTINCT customer_id)) OVER() * 100.0 AS customer_pct FROM     ranked_start_dates WHERE     1 = 1     AND ranks = 1 GROUP BY     plan_name ORDER BY     customer_count DESC; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[96]: plan_name customer_count customer_pct 0 pro monthly 326 32.6 1 churn 236 23.6 2 basic monthly 224 22.4 3 pro annual 195 19.5 4 trial 19 1.9 In\u00a0[100]: Copied! <pre>query = \"\"\" \nSELECT\n    COUNT(DISTINCT customer_id) AS upgrades_to_annual\nFROM\n    foodie_fi.subscriptions AS s \n        LEFT JOIN foodie_fi.plans AS p ON s.plan_id = p.plan_id\nWHERE\n    1 = 1\n    AND EXTRACT(YEAR FROM s.start_date) = 2020\n    AND p.plan_name = 'pro annual';\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  SELECT     COUNT(DISTINCT customer_id) AS upgrades_to_annual FROM     foodie_fi.subscriptions AS s          LEFT JOIN foodie_fi.plans AS p ON s.plan_id = p.plan_id WHERE     1 = 1     AND EXTRACT(YEAR FROM s.start_date) = 2020     AND p.plan_name = 'pro annual'; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[100]: upgrades_to_annual 0 195 In\u00a0[112]: Copied! <pre>query = \"\"\" \nWITH customer_journeys AS (\n    SELECT\n        s.customer_id,\n        MIN(s.start_date) AS first_update_date\n    FROM\n        foodie_fi.subscriptions AS s\n            LEFT JOIN foodie_fi.plans AS p ON s.plan_id = p.plan_id\n    WHERE\n        1 = 1\n        AND p.plan_name = 'pro annual'\n    GROUP BY\n        s.customer_id\n),\n\ncustomer_start_dates AS (\n    SELECT\n        customer_id,\n        MIN(start_date) AS joined_date\n    FROM\n        foodie_fi.subscriptions\n    GROUP BY\n        customer_id\n)\n\nSELECT\n    cj.customer_id,\n    cj.first_update_date,\n    csd.joined_date,\n    DATE_DIFF('DAY', csd.joined_date, cj.first_update_date) AS day_diff\nFROM\n    customer_journeys AS cj\n        LEFT JOIN customer_start_dates AS csd ON cj.customer_id = csd.customer_id;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  WITH customer_journeys AS (     SELECT         s.customer_id,         MIN(s.start_date) AS first_update_date     FROM         foodie_fi.subscriptions AS s             LEFT JOIN foodie_fi.plans AS p ON s.plan_id = p.plan_id     WHERE         1 = 1         AND p.plan_name = 'pro annual'     GROUP BY         s.customer_id ),  customer_start_dates AS (     SELECT         customer_id,         MIN(start_date) AS joined_date     FROM         foodie_fi.subscriptions     GROUP BY         customer_id )  SELECT     cj.customer_id,     cj.first_update_date,     csd.joined_date,     DATE_DIFF('DAY', csd.joined_date, cj.first_update_date) AS day_diff FROM     customer_journeys AS cj         LEFT JOIN customer_start_dates AS csd ON cj.customer_id = csd.customer_id; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[112]: customer_id first_update_date joined_date day_diff 0 9 2020-12-14 2020-12-07 7 1 20 2020-06-05 2020-04-08 58 2 28 2020-07-07 2020-06-30 7 3 38 2020-11-09 2020-10-02 38 4 44 2020-03-24 2020-03-17 7 ... ... ... ... ... 253 956 2021-01-12 2020-02-20 327 254 958 2021-01-22 2020-07-06 200 255 961 2020-11-21 2020-09-12 70 256 967 2021-04-15 2020-08-21 237 257 978 2020-11-03 2020-08-27 68 <p>258 rows \u00d7 4 columns</p> In\u00a0[113]: Copied! <pre>query = \"\"\" \nWITH customer_journeys AS (\n    SELECT\n        s.customer_id,\n        MIN(s.start_date) AS first_update_date\n    FROM\n        foodie_fi.subscriptions AS s\n            LEFT JOIN foodie_fi.plans AS p ON s.plan_id = p.plan_id\n    WHERE\n        1 = 1\n        AND p.plan_name = 'pro annual'\n    GROUP BY\n        s.customer_id\n),\n\ncustomer_start_dates AS (\n    SELECT\n        customer_id,\n        MIN(start_date) AS joined_date\n    FROM\n        foodie_fi.subscriptions\n    GROUP BY\n        customer_id\n)\n\nSELECT\n    AVG(DATE_DIFF('DAY', csd.joined_date, cj.first_update_date)) AS avg_days_to_upgrade\nFROM\n    customer_journeys AS cj\n        LEFT JOIN customer_start_dates AS csd ON cj.customer_id = csd.customer_id;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  WITH customer_journeys AS (     SELECT         s.customer_id,         MIN(s.start_date) AS first_update_date     FROM         foodie_fi.subscriptions AS s             LEFT JOIN foodie_fi.plans AS p ON s.plan_id = p.plan_id     WHERE         1 = 1         AND p.plan_name = 'pro annual'     GROUP BY         s.customer_id ),  customer_start_dates AS (     SELECT         customer_id,         MIN(start_date) AS joined_date     FROM         foodie_fi.subscriptions     GROUP BY         customer_id )  SELECT     AVG(DATE_DIFF('DAY', csd.joined_date, cj.first_update_date)) AS avg_days_to_upgrade FROM     customer_journeys AS cj         LEFT JOIN customer_start_dates AS csd ON cj.customer_id = csd.customer_id; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[113]: avg_days_to_upgrade 0 104.620155 In\u00a0[140]: Copied! <pre>query = \"\"\" \nWITH customer_journeys AS (\n    SELECT\n        s.customer_id,\n        MIN(s.start_date) AS first_update_date\n    FROM\n        foodie_fi.subscriptions AS s\n            LEFT JOIN foodie_fi.plans AS p ON s.plan_id = p.plan_id\n    WHERE\n        1 = 1\n        AND p.plan_name = 'pro annual'\n    GROUP BY\n        s.customer_id\n),\n\ncustomer_start_dates AS (\n    SELECT\n        customer_id,\n        MIN(start_date) AS joined_date\n    FROM\n        foodie_fi.subscriptions\n    GROUP BY\n        customer_id\n)\n\nSELECT\n    DATE_DIFF('DAY', csd.joined_date, cj.first_update_date) AS day_diff,\n    FLOOR(DATE_DIFF('DAY', csd.joined_date, cj.first_update_date) / 30) AS bin_number\nFROM\n    customer_journeys AS cj\n        LEFT JOIN customer_start_dates AS csd ON cj.customer_id = csd.customer_id;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  WITH customer_journeys AS (     SELECT         s.customer_id,         MIN(s.start_date) AS first_update_date     FROM         foodie_fi.subscriptions AS s             LEFT JOIN foodie_fi.plans AS p ON s.plan_id = p.plan_id     WHERE         1 = 1         AND p.plan_name = 'pro annual'     GROUP BY         s.customer_id ),  customer_start_dates AS (     SELECT         customer_id,         MIN(start_date) AS joined_date     FROM         foodie_fi.subscriptions     GROUP BY         customer_id )  SELECT     DATE_DIFF('DAY', csd.joined_date, cj.first_update_date) AS day_diff,     FLOOR(DATE_DIFF('DAY', csd.joined_date, cj.first_update_date) / 30) AS bin_number FROM     customer_journeys AS cj         LEFT JOIN customer_start_dates AS csd ON cj.customer_id = csd.customer_id; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[140]: day_diff bin_number 0 7 0 1 158 5 2 160 5 3 82 2 4 7 0 ... ... ... 253 133 4 254 78 2 255 68 2 256 129 4 257 36 1 <p>258 rows \u00d7 2 columns</p> <p>For a bin number $n$, the formulas for computing the start and end of each 30-day bin:</p> <p>$$ \\text{bin\\_start} = n \\times 30 $$</p> <p>$$ \\text{bin\\_end} = (n + 1) \\times 30 - 1 $$</p> In\u00a0[137]: Copied! <pre>query = \"\"\" \nWITH customer_journeys AS (\n    SELECT\n        s.customer_id,\n        MIN(s.start_date) AS first_update_date\n    FROM\n        foodie_fi.subscriptions AS s\n            LEFT JOIN foodie_fi.plans AS p ON s.plan_id = p.plan_id\n    WHERE\n        1 = 1\n        AND p.plan_name = 'pro annual'\n    GROUP BY\n        s.customer_id\n),\n\ncustomer_start_dates AS (\n    SELECT\n        customer_id,\n        MIN(start_date) AS joined_date\n    FROM\n        foodie_fi.subscriptions\n    GROUP BY\n        customer_id\n),\n\ndiff_bins AS (\n    SELECT\n        DATE_DIFF('DAY', csd.joined_date, cj.first_update_date) AS day_diff,\n        FLOOR(DATE_DIFF('DAY', csd.joined_date, cj.first_update_date) / 30) AS bin_number\n    FROM\n        customer_journeys AS cj\n            LEFT JOIN customer_start_dates AS csd ON cj.customer_id = csd.customer_id\n)\n\nSELECT\n    CONCAT(CAST(bin_number * 30 AS VARCHAR), ' - ', CAST((bin_number + 1) * 30 - 1 AS VARCHAR), ' days') AS breakdown_period,\n    COUNT(*) as count\nFROM\n    diff_bins\nGROUP BY\n    bin_number\nORDER BY\n    bin_number;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  WITH customer_journeys AS (     SELECT         s.customer_id,         MIN(s.start_date) AS first_update_date     FROM         foodie_fi.subscriptions AS s             LEFT JOIN foodie_fi.plans AS p ON s.plan_id = p.plan_id     WHERE         1 = 1         AND p.plan_name = 'pro annual'     GROUP BY         s.customer_id ),  customer_start_dates AS (     SELECT         customer_id,         MIN(start_date) AS joined_date     FROM         foodie_fi.subscriptions     GROUP BY         customer_id ),  diff_bins AS (     SELECT         DATE_DIFF('DAY', csd.joined_date, cj.first_update_date) AS day_diff,         FLOOR(DATE_DIFF('DAY', csd.joined_date, cj.first_update_date) / 30) AS bin_number     FROM         customer_journeys AS cj             LEFT JOIN customer_start_dates AS csd ON cj.customer_id = csd.customer_id )  SELECT     CONCAT(CAST(bin_number * 30 AS VARCHAR), ' - ', CAST((bin_number + 1) * 30 - 1 AS VARCHAR), ' days') AS breakdown_period,     COUNT(*) as count FROM     diff_bins GROUP BY     bin_number ORDER BY     bin_number; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[137]: breakdown_period count 0 0 - 29 days 48 1 30 - 59 days 25 2 60 - 89 days 33 3 90 - 119 days 35 4 120 - 149 days 43 5 150 - 179 days 35 6 180 - 209 days 27 7 210 - 239 days 4 8 240 - 269 days 5 9 270 - 299 days 1 10 300 - 329 days 1 11 330 - 359 days 1 In\u00a0[192]: Copied! <pre>query = \"\"\" \nSELECT\n    s.customer_id,\n    s.start_date,\n    LAG(s.start_date, 1) OVER(PARTITION BY customer_id ORDER BY s.start_date ASC) AS previous_start_date,\n    p.plan_name,\n    LAG(p.plan_name, 1) OVER(PARTITION BY customer_id ORDER BY s.start_date ASC) AS previous_plan_name \nFROM\n    foodie_fi.subscriptions AS s \n        LEFT JOIN foodie_fi.plans AS p ON s.plan_id = p.plan_id\nORDER BY\n    s.customer_id,\n    s.start_date;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  SELECT     s.customer_id,     s.start_date,     LAG(s.start_date, 1) OVER(PARTITION BY customer_id ORDER BY s.start_date ASC) AS previous_start_date,     p.plan_name,     LAG(p.plan_name, 1) OVER(PARTITION BY customer_id ORDER BY s.start_date ASC) AS previous_plan_name  FROM     foodie_fi.subscriptions AS s          LEFT JOIN foodie_fi.plans AS p ON s.plan_id = p.plan_id ORDER BY     s.customer_id,     s.start_date; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[192]: customer_id start_date previous_start_date plan_name previous_plan_name 0 1 2020-08-01 NaT trial &lt;NA&gt; 1 1 2020-08-08 2020-08-01 basic monthly trial 2 2 2020-09-20 NaT trial &lt;NA&gt; 3 2 2020-09-27 2020-09-20 pro annual trial 4 3 2020-01-13 NaT trial &lt;NA&gt; ... ... ... ... ... ... 2645 999 2020-10-30 2020-10-23 pro monthly trial 2646 999 2020-12-01 2020-10-30 churn pro monthly 2647 1000 2020-03-19 NaT trial &lt;NA&gt; 2648 1000 2020-03-26 2020-03-19 pro monthly trial 2649 1000 2020-06-04 2020-03-26 churn pro monthly <p>2650 rows \u00d7 5 columns</p> In\u00a0[175]: Copied! <pre>query = \"\"\" \nWITH ranked_data AS (\n    SELECT\n        s.customer_id,\n        s.start_date,\n        p.plan_name,\n        LAG(p.plan_name, 1) OVER(PARTITION BY customer_id ORDER BY s.start_date ASC) AS previous_plan_name \n    FROM\n        foodie_fi.subscriptions AS s \n            LEFT JOIN foodie_fi.plans AS p ON s.plan_id = p.plan_id\n)\nSELECT\n    COUNT(DISTINCT customer_id) AS downgrades_count\nFROM\n    ranked_data\nWHERE\n    1 = 1\n    AND plan_name = 'basic monthly'\n    AND previous_plan_name = 'pro monthly'\n    AND EXTRACT(YEAR FROM start_date) = 2020;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  WITH ranked_data AS (     SELECT         s.customer_id,         s.start_date,         p.plan_name,         LAG(p.plan_name, 1) OVER(PARTITION BY customer_id ORDER BY s.start_date ASC) AS previous_plan_name      FROM         foodie_fi.subscriptions AS s              LEFT JOIN foodie_fi.plans AS p ON s.plan_id = p.plan_id ) SELECT     COUNT(DISTINCT customer_id) AS downgrades_count FROM     ranked_data WHERE     1 = 1     AND plan_name = 'basic monthly'     AND previous_plan_name = 'pro monthly'     AND EXTRACT(YEAR FROM start_date) = 2020; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[175]: downgrades_count 0 0 In\u00a0[191]: Copied! <pre>query = \"\"\" \nSELECT\n    s.customer_id,\n    s.start_date,\n    LEAD(s.start_date, 1) OVER(PARTITION BY customer_id ORDER BY s.start_date ASC) AS next_start_date,\n    p.plan_name,\n    LEAD(p.plan_name, 1) OVER(PARTITION BY customer_id ORDER BY s.start_date ASC) AS next_plan_name \nFROM\n    foodie_fi.subscriptions AS s \n        LEFT JOIN foodie_fi.plans AS p ON s.plan_id = p.plan_id\nORDER BY\n    s.customer_id,\n    s.start_date;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  SELECT     s.customer_id,     s.start_date,     LEAD(s.start_date, 1) OVER(PARTITION BY customer_id ORDER BY s.start_date ASC) AS next_start_date,     p.plan_name,     LEAD(p.plan_name, 1) OVER(PARTITION BY customer_id ORDER BY s.start_date ASC) AS next_plan_name  FROM     foodie_fi.subscriptions AS s          LEFT JOIN foodie_fi.plans AS p ON s.plan_id = p.plan_id ORDER BY     s.customer_id,     s.start_date; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[191]: customer_id start_date next_start_date plan_name next_plan_name 0 1 2020-08-01 2020-08-08 trial basic monthly 1 1 2020-08-08 NaT basic monthly &lt;NA&gt; 2 2 2020-09-20 2020-09-27 trial pro annual 3 2 2020-09-27 NaT pro annual &lt;NA&gt; 4 3 2020-01-13 2020-01-20 trial basic monthly ... ... ... ... ... ... 2645 999 2020-10-30 2020-12-01 pro monthly churn 2646 999 2020-12-01 NaT churn &lt;NA&gt; 2647 1000 2020-03-19 2020-03-26 trial pro monthly 2648 1000 2020-03-26 2020-06-04 pro monthly churn 2649 1000 2020-06-04 NaT churn &lt;NA&gt; <p>2650 rows \u00d7 5 columns</p> In\u00a0[185]: Copied! <pre>query = \"\"\" \nWITH ranked_data AS (\n    SELECT\n        s.customer_id,\n        s.start_date,\n        LEAD(s.start_date, 1) OVER(PARTITION BY customer_id ORDER BY s.start_date ASC) AS next_start_date,\n        p.plan_name,\n        LEAD(p.plan_name, 1) OVER(PARTITION BY customer_id ORDER BY s.start_date ASC) AS next_plan_name \n    FROM\n        foodie_fi.subscriptions AS s \n            LEFT JOIN foodie_fi.plans AS p ON s.plan_id = p.plan_id\n)\nSELECT\n    COUNT(DISTINCT customer_id) AS upgrades_count\nFROM\n    ranked_data\nWHERE\n    1 = 1\n    AND plan_name = 'basic monthly'\n    AND next_plan_name = 'pro monthly'\n    AND EXTRACT(YEAR FROM next_start_date) = 2020;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  WITH ranked_data AS (     SELECT         s.customer_id,         s.start_date,         LEAD(s.start_date, 1) OVER(PARTITION BY customer_id ORDER BY s.start_date ASC) AS next_start_date,         p.plan_name,         LEAD(p.plan_name, 1) OVER(PARTITION BY customer_id ORDER BY s.start_date ASC) AS next_plan_name      FROM         foodie_fi.subscriptions AS s              LEFT JOIN foodie_fi.plans AS p ON s.plan_id = p.plan_id ) SELECT     COUNT(DISTINCT customer_id) AS upgrades_count FROM     ranked_data WHERE     1 = 1     AND plan_name = 'basic monthly'     AND next_plan_name = 'pro monthly'     AND EXTRACT(YEAR FROM next_start_date) = 2020; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[185]: upgrades_count 0 163 In\u00a0[226]: Copied! <pre>query = \"\"\" \nWITH transitions AS (\n    SELECT\n        s.customer_id,\n        s.start_date,\n        LEAD(s.start_date, 1) OVER(PARTITION BY customer_id ORDER BY s.start_date ASC) AS next_start_date,\n        p.plan_name,\n        LEAD(p.plan_name, 1) OVER(PARTITION BY customer_id ORDER BY s.start_date ASC) AS next_plan_name \n    FROM\n        foodie_fi.subscriptions AS s \n            LEFT JOIN foodie_fi.plans AS p ON s.plan_id = p.plan_id\n),\n\ntransition_counts AS (\n    SELECT\n        plan_name,\n        next_plan_name,\n        COUNT(DISTINCT customer_id) AS transition_count\n    FROM\n        transitions\n    WHERE\n        next_plan_name IS NOT NULL\n    GROUP BY\n        plan_name, next_plan_name\n)\n\nSELECT\n    plan_name,\n    COALESCE(SUM(CASE WHEN next_plan_name = 'trial' THEN transition_count END), 0) AS trial,\n    COALESCE(SUM(CASE WHEN next_plan_name = 'basic monthly' THEN transition_count END), 0) AS basic_monthly,\n    COALESCE(SUM(CASE WHEN next_plan_name = 'pro monthly' THEN transition_count END), 0) AS pro_monthly,\n    COALESCE(SUM(CASE WHEN next_plan_name = 'pro annual' THEN transition_count END), 0) AS pro_annual,\n    COALESCE(SUM(CASE WHEN next_plan_name = 'churn' THEN transition_count END), 0) AS churn\nFROM\n    transition_counts\nGROUP BY\n    plan_name\nORDER BY\n    plan_name;\n\"\"\"\n\ntransition_matrix_counts = (\n    athena.query(database=database, query=query, ctas_approach=ctas_approach)\n    .set_index(\"plan_name\")\n    .astype(int)\n)\ntransition_matrix_counts\n</pre> query = \"\"\"  WITH transitions AS (     SELECT         s.customer_id,         s.start_date,         LEAD(s.start_date, 1) OVER(PARTITION BY customer_id ORDER BY s.start_date ASC) AS next_start_date,         p.plan_name,         LEAD(p.plan_name, 1) OVER(PARTITION BY customer_id ORDER BY s.start_date ASC) AS next_plan_name      FROM         foodie_fi.subscriptions AS s              LEFT JOIN foodie_fi.plans AS p ON s.plan_id = p.plan_id ),  transition_counts AS (     SELECT         plan_name,         next_plan_name,         COUNT(DISTINCT customer_id) AS transition_count     FROM         transitions     WHERE         next_plan_name IS NOT NULL     GROUP BY         plan_name, next_plan_name )  SELECT     plan_name,     COALESCE(SUM(CASE WHEN next_plan_name = 'trial' THEN transition_count END), 0) AS trial,     COALESCE(SUM(CASE WHEN next_plan_name = 'basic monthly' THEN transition_count END), 0) AS basic_monthly,     COALESCE(SUM(CASE WHEN next_plan_name = 'pro monthly' THEN transition_count END), 0) AS pro_monthly,     COALESCE(SUM(CASE WHEN next_plan_name = 'pro annual' THEN transition_count END), 0) AS pro_annual,     COALESCE(SUM(CASE WHEN next_plan_name = 'churn' THEN transition_count END), 0) AS churn FROM     transition_counts GROUP BY     plan_name ORDER BY     plan_name; \"\"\"  transition_matrix_counts = (     athena.query(database=database, query=query, ctas_approach=ctas_approach)     .set_index(\"plan_name\")     .astype(int) ) transition_matrix_counts Out[226]: trial basic_monthly pro_monthly pro_annual churn plan_name basic monthly 0 0 214 110 97 pro annual 0 0 0 0 6 pro monthly 0 0 0 111 112 trial 0 546 325 37 92 In\u00a0[220]: Copied! <pre>plt.figure(figsize=(10, 6))\nsns.heatmap(transition_matrix_counts, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=True)\nplt.title(\"Transition Heatmap (Counts)\")\nplt.xlabel(\"Next Plan\")\nplt.ylabel(\"Current Plan\")\nplt.show();\n</pre> plt.figure(figsize=(10, 6)) sns.heatmap(transition_matrix_counts, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=True) plt.title(\"Transition Heatmap (Counts)\") plt.xlabel(\"Next Plan\") plt.ylabel(\"Current Plan\") plt.show(); In\u00a0[228]: Copied! <pre>query = \"\"\" \nWITH transitions AS (\n    SELECT\n        s.customer_id,\n        s.start_date,\n        LEAD(s.start_date, 1) OVER(PARTITION BY s.customer_id ORDER BY s.start_date ASC) AS next_start_date,\n        p.plan_name,\n        LEAD(p.plan_name, 1) OVER(PARTITION BY s.customer_id ORDER BY s.start_date ASC) AS next_plan_name\n    FROM\n        foodie_fi.subscriptions AS s\n    LEFT JOIN\n        foodie_fi.plans AS p\n    ON\n        s.plan_id = p.plan_id\n),\n\ntransition_counts AS (\n    SELECT\n        plan_name,\n        next_plan_name,\n        COUNT(DISTINCT customer_id) AS transition_count\n    FROM\n        transitions\n    WHERE\n        next_plan_name IS NOT NULL\n    GROUP BY\n        plan_name, next_plan_name\n),\n\nrow_totals AS (\n    SELECT\n        plan_name,\n        SUM(transition_count) AS total_transitions\n    FROM\n        transition_counts\n    GROUP BY\n        plan_name\n)\n\nSELECT\n    tc.plan_name,\n    ROUND(COALESCE(SUM(CASE WHEN tc.next_plan_name = 'trial' THEN CAST(tc.transition_count AS DOUBLE) / rt.total_transitions END), 0), 4) AS trial,\n    ROUND(COALESCE(SUM(CASE WHEN tc.next_plan_name = 'basic monthly' THEN CAST(tc.transition_count AS DOUBLE) / rt.total_transitions END), 0), 4) AS basic_monthly,\n    ROUND(COALESCE(SUM(CASE WHEN tc.next_plan_name = 'pro monthly' THEN CAST(tc.transition_count AS DOUBLE) / rt.total_transitions END), 0), 4) AS pro_monthly,\n    ROUND(COALESCE(SUM(CASE WHEN tc.next_plan_name = 'pro annual' THEN CAST(tc.transition_count AS DOUBLE) / rt.total_transitions END), 0), 4) AS pro_annual,\n    ROUND(COALESCE(SUM(CASE WHEN tc.next_plan_name = 'churn' THEN CAST(tc.transition_count AS DOUBLE) / rt.total_transitions END), 0), 4) AS churn\nFROM\n    transition_counts AS tc\nJOIN\n    row_totals AS rt\nON\n    tc.plan_name = rt.plan_name\nGROUP BY\n    tc.plan_name\nORDER BY\n    tc.plan_name;\n\"\"\"\n\ntransition_matrix_probs = (\n    athena.query(database=database, query=query, ctas_approach=ctas_approach)\n    .set_index(\"plan_name\")\n    .astype(float)\n)\ntransition_matrix_probs\n</pre> query = \"\"\"  WITH transitions AS (     SELECT         s.customer_id,         s.start_date,         LEAD(s.start_date, 1) OVER(PARTITION BY s.customer_id ORDER BY s.start_date ASC) AS next_start_date,         p.plan_name,         LEAD(p.plan_name, 1) OVER(PARTITION BY s.customer_id ORDER BY s.start_date ASC) AS next_plan_name     FROM         foodie_fi.subscriptions AS s     LEFT JOIN         foodie_fi.plans AS p     ON         s.plan_id = p.plan_id ),  transition_counts AS (     SELECT         plan_name,         next_plan_name,         COUNT(DISTINCT customer_id) AS transition_count     FROM         transitions     WHERE         next_plan_name IS NOT NULL     GROUP BY         plan_name, next_plan_name ),  row_totals AS (     SELECT         plan_name,         SUM(transition_count) AS total_transitions     FROM         transition_counts     GROUP BY         plan_name )  SELECT     tc.plan_name,     ROUND(COALESCE(SUM(CASE WHEN tc.next_plan_name = 'trial' THEN CAST(tc.transition_count AS DOUBLE) / rt.total_transitions END), 0), 4) AS trial,     ROUND(COALESCE(SUM(CASE WHEN tc.next_plan_name = 'basic monthly' THEN CAST(tc.transition_count AS DOUBLE) / rt.total_transitions END), 0), 4) AS basic_monthly,     ROUND(COALESCE(SUM(CASE WHEN tc.next_plan_name = 'pro monthly' THEN CAST(tc.transition_count AS DOUBLE) / rt.total_transitions END), 0), 4) AS pro_monthly,     ROUND(COALESCE(SUM(CASE WHEN tc.next_plan_name = 'pro annual' THEN CAST(tc.transition_count AS DOUBLE) / rt.total_transitions END), 0), 4) AS pro_annual,     ROUND(COALESCE(SUM(CASE WHEN tc.next_plan_name = 'churn' THEN CAST(tc.transition_count AS DOUBLE) / rt.total_transitions END), 0), 4) AS churn FROM     transition_counts AS tc JOIN     row_totals AS rt ON     tc.plan_name = rt.plan_name GROUP BY     tc.plan_name ORDER BY     tc.plan_name; \"\"\"  transition_matrix_probs = (     athena.query(database=database, query=query, ctas_approach=ctas_approach)     .set_index(\"plan_name\")     .astype(float) ) transition_matrix_probs Out[228]: trial basic_monthly pro_monthly pro_annual churn plan_name basic monthly 0.0 0.000 0.5083 0.2613 0.2304 pro annual 0.0 0.000 0.0000 0.0000 1.0000 pro monthly 0.0 0.000 0.0000 0.4978 0.5022 trial 0.0 0.546 0.3250 0.0370 0.0920 In\u00a0[229]: Copied! <pre>plt.figure(figsize=(10, 6))\nsns.heatmap(transition_matrix_probs, annot=True, fmt=\".2f\", cmap=\"Blues\", cbar=True)\nplt.title(\"Transition Heatmap (Probabilities)\")\nplt.xlabel(\"Next Plan\")\nplt.ylabel(\"Current Plan\")\nplt.show();\n</pre> plt.figure(figsize=(10, 6)) sns.heatmap(transition_matrix_probs, annot=True, fmt=\".2f\", cmap=\"Blues\", cbar=True) plt.title(\"Transition Heatmap (Probabilities)\") plt.xlabel(\"Next Plan\") plt.ylabel(\"Current Plan\") plt.show(); In\u00a0[8]: Copied! <pre>query = \"\"\" \nSELECT\n    customer_id,\n    plan_id,\n    start_date,\n\n    LEAD (plan_id) OVER (\n        PARTITION BY\n            customer_id\n        ORDER BY\n            start_date ASC\n    ) AS lead_plan_id,\n\n    LEAD (start_date) OVER (\n        PARTITION BY\n            customer_id\n        ORDER BY\n            start_date ASC\n    ) AS lead_start_date\n\nFROM\n    foodie_fi.subscriptions\nWHERE\n    year (start_date) = 2020\n    AND plan_id != 0;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  SELECT     customer_id,     plan_id,     start_date,      LEAD (plan_id) OVER (         PARTITION BY             customer_id         ORDER BY             start_date ASC     ) AS lead_plan_id,      LEAD (start_date) OVER (         PARTITION BY             customer_id         ORDER BY             start_date ASC     ) AS lead_start_date  FROM     foodie_fi.subscriptions WHERE     year (start_date) = 2020     AND plan_id != 0; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[8]: customer_id plan_id start_date lead_plan_id lead_start_date 0 9 3 2020-12-14 &lt;NA&gt; NaT 1 13 1 2020-12-22 &lt;NA&gt; NaT 2 15 2 2020-03-24 4 2020-04-29 3 15 4 2020-04-29 &lt;NA&gt; NaT 4 20 1 2020-04-15 3 2020-06-05 ... ... ... ... ... ... 1443 997 1 2020-08-03 2 2020-08-26 1444 997 2 2020-08-26 4 2020-11-14 1445 997 4 2020-11-14 &lt;NA&gt; NaT 1446 999 2 2020-10-30 4 2020-12-01 1447 999 4 2020-12-01 &lt;NA&gt; NaT <p>1448 rows \u00d7 5 columns</p> In\u00a0[242]: Copied! <pre>query = \"\"\"\nWITH lead_plans AS (\n    SELECT\n        customer_id,\n        plan_id,\n        start_date,\n        LEAD (plan_id) OVER (\n            PARTITION BY\n                customer_id\n            ORDER BY\n                start_date\n        ) AS lead_plan_id,\n        LEAD (start_date) OVER (\n            PARTITION BY\n                customer_id\n            ORDER BY\n                start_date\n        ) AS lead_start_date\n    FROM\n        foodie_fi.subscriptions\n    WHERE\n        year (start_date) = 2020\n        AND plan_id != 0\n)\n\nSELECT\n    customer_id,\n    plan_id,\n    start_date,\n    DATE_DIFF('MONTH', start_date, DATE '2020-12-31') AS month_diff\nFROM\n    lead_plans\nWHERE\n    lead_plan_id IS NULL\n    AND plan_id NOT IN (3, 4) -- Exclude churn and annual plans;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\" WITH lead_plans AS (     SELECT         customer_id,         plan_id,         start_date,         LEAD (plan_id) OVER (             PARTITION BY                 customer_id             ORDER BY                 start_date         ) AS lead_plan_id,         LEAD (start_date) OVER (             PARTITION BY                 customer_id             ORDER BY                 start_date         ) AS lead_start_date     FROM         foodie_fi.subscriptions     WHERE         year (start_date) = 2020         AND plan_id != 0 )  SELECT     customer_id,     plan_id,     start_date,     DATE_DIFF('MONTH', start_date, DATE '2020-12-31') AS month_diff FROM     lead_plans WHERE     lead_plan_id IS NULL     AND plan_id NOT IN (3, 4) -- Exclude churn and annual plans; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[242]: customer_id plan_id start_date month_diff 0 13 1 2020-12-22 0 1 26 2 2020-12-15 0 2 34 1 2020-12-27 0 3 41 2 2020-05-23 7 4 54 2 2020-05-30 7 ... ... ... ... ... 545 963 2 2020-01-11 11 546 965 1 2020-06-26 6 547 980 2 2020-06-19 6 548 989 2 2020-09-10 3 549 995 2 2020-12-06 0 <p>550 rows \u00d7 4 columns</p> <p>Query:</p> <p>The second query calculates monthly payment dates for the customers identified in Step 1.</p> <ul> <li><p>Logic:</p> <ul> <li>Use <code>SEQUENCE(0, month_diff)</code> to generate a series of numbers (0 to <code>month_diff</code>).</li> <li>Use <code>DATE_ADD('MONTH', seq, start_date)</code> to compute each monthly payment date.</li> <li><code>CROSS JOIN UNNEST</code> applies the sequence to each customer, creating a row for each payment.</li> </ul> </li> </ul> <p>Output:</p> <p>Each row represents a payment for a customer, with <code>customer_id</code>, <code>plan_id</code>, and <code>payment_date</code>.</p> In\u00a0[9]: Copied! <pre>query = \"\"\" \nWITH lead_plans AS (\n    SELECT\n        customer_id,\n        plan_id,\n        start_date,\n        LEAD (plan_id) OVER (\n            PARTITION BY\n                customer_id\n            ORDER BY\n                start_date\n        ) AS lead_plan_id,\n        LEAD (start_date) OVER (\n            PARTITION BY\n                customer_id\n            ORDER BY\n                start_date\n        ) AS lead_start_date\n    FROM\n        foodie_fi.subscriptions\n    WHERE\n        year (start_date) = 2020\n        AND plan_id != 0\n),\n\n-- Case 1: Non-churn monthly customers\n\ncase_1 AS (\n    SELECT\n        customer_id,\n        plan_id,\n        start_date,\n        DATE_DIFF('MONTH', start_date, DATE '2020-12-31') AS month_diff\n    FROM\n        lead_plans\n    WHERE\n        lead_plan_id IS NULL\n        AND plan_id NOT IN (3, 4) -- Exclude churn and annual plans\n)\n\n-- Generate payments for case 1 customers\n\nSELECT\n    customer_id,\n    plan_id,\n    DATE_ADD('MONTH', seq, start_date) AS payment_date\nFROM\n    case_1\n    CROSS JOIN UNNEST (SEQUENCE (0, month_diff)) AS t (seq);\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  WITH lead_plans AS (     SELECT         customer_id,         plan_id,         start_date,         LEAD (plan_id) OVER (             PARTITION BY                 customer_id             ORDER BY                 start_date         ) AS lead_plan_id,         LEAD (start_date) OVER (             PARTITION BY                 customer_id             ORDER BY                 start_date         ) AS lead_start_date     FROM         foodie_fi.subscriptions     WHERE         year (start_date) = 2020         AND plan_id != 0 ),  -- Case 1: Non-churn monthly customers  case_1 AS (     SELECT         customer_id,         plan_id,         start_date,         DATE_DIFF('MONTH', start_date, DATE '2020-12-31') AS month_diff     FROM         lead_plans     WHERE         lead_plan_id IS NULL         AND plan_id NOT IN (3, 4) -- Exclude churn and annual plans )  -- Generate payments for case 1 customers  SELECT     customer_id,     plan_id,     DATE_ADD('MONTH', seq, start_date) AS payment_date FROM     case_1     CROSS JOIN UNNEST (SEQUENCE (0, month_diff)) AS t (seq); \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[9]: customer_id plan_id payment_date 0 1 1 2020-08-08 1 1 1 2020-09-08 2 1 1 2020-10-08 3 1 1 2020-11-08 4 1 1 2020-12-08 ... ... ... ... 2768 994 2 2020-08-27 2769 994 2 2020-09-27 2770 994 2 2020-10-27 2771 994 2 2020-11-27 2772 994 2 2020-12-27 <p>2773 rows \u00d7 3 columns</p> In\u00a0[249]: Copied! <pre>query = \"\"\"\nWITH lead_plans AS (\n    SELECT\n        customer_id,\n        plan_id,\n        start_date,\n        LEAD (plan_id) OVER (\n            PARTITION BY\n                customer_id\n            ORDER BY\n                start_date\n        ) AS lead_plan_id,\n        LEAD (start_date) OVER (\n            PARTITION BY\n                customer_id\n            ORDER BY\n                start_date\n        ) AS lead_start_date\n    FROM\n        foodie_fi.subscriptions\n    WHERE\n        year (start_date) = 2020\n        AND plan_id != 0\n)\n\nSELECT\n    customer_id,\n    plan_id,\n    start_date,\n    DATE_DIFF(\n        'MONTH',\n        start_date,\n        DATE_ADD('DAY', -1, lead_start_date)\n    ) AS month_diff\nFROM\n    lead_plans\nWHERE\n    lead_plan_id = 4;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\" WITH lead_plans AS (     SELECT         customer_id,         plan_id,         start_date,         LEAD (plan_id) OVER (             PARTITION BY                 customer_id             ORDER BY                 start_date         ) AS lead_plan_id,         LEAD (start_date) OVER (             PARTITION BY                 customer_id             ORDER BY                 start_date         ) AS lead_start_date     FROM         foodie_fi.subscriptions     WHERE         year (start_date) = 2020         AND plan_id != 0 )  SELECT     customer_id,     plan_id,     start_date,     DATE_DIFF(         'MONTH',         start_date,         DATE_ADD('DAY', -1, lead_start_date)     ) AS month_diff FROM     lead_plans WHERE     lead_plan_id = 4; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[249]: customer_id plan_id start_date month_diff 0 21 2 2020-06-03 3 1 81 2 2020-06-05 4 2 98 2 2020-01-22 2 3 113 2 2020-09-13 1 4 116 1 2020-05-30 3 ... ... ... ... ... 141 851 1 2020-07-25 4 142 865 2 2020-04-03 3 143 881 2 2020-10-24 1 144 962 2 2020-09-23 1 145 1000 2 2020-03-26 2 <p>146 rows \u00d7 4 columns</p> <p>Query:</p> <p>The second query calculates monthly payment dates for customers identified in Step 1.</p> <ul> <li><p>Logic:</p> <ul> <li><p>Use <code>SEQUENCE(0, month_diff)</code> to generate a series from 0 to <code>month_diff</code>.</p> </li> <li><p>Compute each payment date using <code>DATE_ADD('MONTH', seq, start_date)</code>.</p> </li> <li><p><code>CROSS JOIN UNNEST</code> applies the sequence to each customer, creating a row for every payment made until churn.</p> </li> </ul> </li> </ul> <p>Output:</p> <p>Each row represents a payment for a churned customer, with <code>customer_id</code>, <code>plan_id</code>, and <code>payment_date</code>.</p> In\u00a0[252]: Copied! <pre>query = \"\"\"\nWITH lead_plans AS (\n    SELECT\n        customer_id,\n        plan_id,\n        start_date,\n        LEAD (plan_id) OVER (\n            PARTITION BY\n                customer_id\n            ORDER BY\n                start_date\n        ) AS lead_plan_id,\n        LEAD (start_date) OVER (\n            PARTITION BY\n                customer_id\n            ORDER BY\n                start_date\n        ) AS lead_start_date\n    FROM\n        foodie_fi.subscriptions\n    WHERE\n        year (start_date) = 2020\n        AND plan_id != 0\n),\n\ncase_2 AS (\n    SELECT\n        customer_id,\n        plan_id,\n        start_date,\n        DATE_DIFF(\n            'MONTH',\n            start_date,\n            DATE_ADD('DAY', -1, lead_start_date)\n        ) AS month_diff\n    FROM\n        lead_plans\n    WHERE\n        lead_plan_id = 4 -- Churn plans only\n)\n\n-- Generate payments for churn customers\n\nSELECT\n    customer_id,\n    plan_id,\n    DATE_ADD('MONTH', seq, start_date) AS payment_date\nFROM\n    case_2\n    CROSS JOIN UNNEST (SEQUENCE (0, month_diff)) AS t (seq);\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\" WITH lead_plans AS (     SELECT         customer_id,         plan_id,         start_date,         LEAD (plan_id) OVER (             PARTITION BY                 customer_id             ORDER BY                 start_date         ) AS lead_plan_id,         LEAD (start_date) OVER (             PARTITION BY                 customer_id             ORDER BY                 start_date         ) AS lead_start_date     FROM         foodie_fi.subscriptions     WHERE         year (start_date) = 2020         AND plan_id != 0 ),  case_2 AS (     SELECT         customer_id,         plan_id,         start_date,         DATE_DIFF(             'MONTH',             start_date,             DATE_ADD('DAY', -1, lead_start_date)         ) AS month_diff     FROM         lead_plans     WHERE         lead_plan_id = 4 -- Churn plans only )  -- Generate payments for churn customers  SELECT     customer_id,     plan_id,     DATE_ADD('MONTH', seq, start_date) AS payment_date FROM     case_2     CROSS JOIN UNNEST (SEQUENCE (0, month_diff)) AS t (seq); \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[252]: customer_id plan_id payment_date 0 15 2 2020-03-24 1 15 2 2020-04-24 2 48 1 2020-01-18 3 48 1 2020-02-18 4 48 1 2020-03-18 ... ... ... ... 438 887 2 2020-09-19 439 907 2 2020-06-26 440 907 2 2020-07-26 441 907 2 2020-08-26 442 907 2 2020-09-26 <p>443 rows \u00d7 3 columns</p> In\u00a0[256]: Copied! <pre>query = \"\"\" \n-- Generate lead plans with next plan and next start date\n\nWITH lead_plans AS (\n    SELECT\n        customer_id,\n        plan_id,\n        start_date,\n        LEAD (plan_id) OVER (\n            PARTITION BY\n                customer_id\n            ORDER BY\n                start_date\n        ) AS lead_plan_id,\n        LEAD (start_date) OVER (\n            PARTITION BY\n                customer_id\n            ORDER BY\n                start_date\n        ) AS lead_start_date\n    FROM\n        foodie_fi.subscriptions\n    WHERE\n        year (start_date) = 2020\n        AND plan_id != 0\n)\n\n-- Case 3: Customers who move from basic to pro plans\n\nSELECT\n    customer_id,\n    plan_id,\n    start_date,\n    DATE_DIFF(\n        'MONTH',\n        start_date,\n        DATE_ADD('DAY', -1, lead_start_date)\n    ) AS month_diff\nFROM\n    lead_plans\nWHERE\n    plan_id = 1\n    AND lead_plan_id IN (2, 3);\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  -- Generate lead plans with next plan and next start date  WITH lead_plans AS (     SELECT         customer_id,         plan_id,         start_date,         LEAD (plan_id) OVER (             PARTITION BY                 customer_id             ORDER BY                 start_date         ) AS lead_plan_id,         LEAD (start_date) OVER (             PARTITION BY                 customer_id             ORDER BY                 start_date         ) AS lead_start_date     FROM         foodie_fi.subscriptions     WHERE         year (start_date) = 2020         AND plan_id != 0 )  -- Case 3: Customers who move from basic to pro plans  SELECT     customer_id,     plan_id,     start_date,     DATE_DIFF(         'MONTH',         start_date,         DATE_ADD('DAY', -1, lead_start_date)     ) AS month_diff FROM     lead_plans WHERE     plan_id = 1     AND lead_plan_id IN (2, 3); \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[256]: customer_id plan_id start_date month_diff 0 8 1 2020-06-18 1 1 16 1 2020-06-07 4 2 17 1 2020-08-03 4 3 39 1 2020-06-04 2 4 46 1 2020-04-26 2 ... ... ... ... ... 246 926 1 2020-07-19 2 247 930 1 2020-02-21 1 248 931 1 2020-02-03 0 249 939 1 2020-03-27 4 250 995 1 2020-06-18 5 <p>251 rows \u00d7 4 columns</p> In\u00a0[258]: Copied! <pre>query = \"\"\" \n-- Generate lead plans with next plan and next start date\n\nWITH lead_plans AS (\n    SELECT\n        customer_id,\n        plan_id,\n        start_date,\n        LEAD (plan_id) OVER (\n            PARTITION BY\n                customer_id\n            ORDER BY\n                start_date\n        ) AS lead_plan_id,\n        LEAD (start_date) OVER (\n            PARTITION BY\n                customer_id\n            ORDER BY\n                start_date\n        ) AS lead_start_date\n    FROM\n        foodie_fi.subscriptions\n    WHERE\n        year (start_date) = 2020\n        AND plan_id != 0\n),\n\n-- Case 3: Customers who move from basic to pro plans\n\ncase_3 AS (\n    SELECT\n        customer_id,\n        plan_id,\n        start_date,\n        DATE_DIFF(\n            'MONTH',\n            start_date,\n            DATE_ADD('DAY', -1, lead_start_date)\n        ) AS month_diff\n    FROM\n        lead_plans\n    WHERE\n        plan_id = 1\n        AND lead_plan_id IN (2, 3)\n)\n\n-- Generate payments for case 3 customers\n\nSELECT\n    customer_id,\n    plan_id,\n    DATE_ADD('MONTH', seq, start_date) AS payment_date\nFROM\n    case_3\n    CROSS JOIN UNNEST (SEQUENCE (0, month_diff)) AS t (seq);\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  -- Generate lead plans with next plan and next start date  WITH lead_plans AS (     SELECT         customer_id,         plan_id,         start_date,         LEAD (plan_id) OVER (             PARTITION BY                 customer_id             ORDER BY                 start_date         ) AS lead_plan_id,         LEAD (start_date) OVER (             PARTITION BY                 customer_id             ORDER BY                 start_date         ) AS lead_start_date     FROM         foodie_fi.subscriptions     WHERE         year (start_date) = 2020         AND plan_id != 0 ),  -- Case 3: Customers who move from basic to pro plans  case_3 AS (     SELECT         customer_id,         plan_id,         start_date,         DATE_DIFF(             'MONTH',             start_date,             DATE_ADD('DAY', -1, lead_start_date)         ) AS month_diff     FROM         lead_plans     WHERE         plan_id = 1         AND lead_plan_id IN (2, 3) )  -- Generate payments for case 3 customers  SELECT     customer_id,     plan_id,     DATE_ADD('MONTH', seq, start_date) AS payment_date FROM     case_3     CROSS JOIN UNNEST (SEQUENCE (0, month_diff)) AS t (seq); \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[258]: customer_id plan_id payment_date 0 20 1 2020-04-15 1 20 1 2020-05-15 2 64 1 2020-03-15 3 66 1 2020-08-06 4 66 1 2020-09-06 ... ... ... ... 803 918 1 2020-08-10 804 938 1 2020-08-08 805 938 1 2020-09-08 806 938 1 2020-10-08 807 997 1 2020-08-03 <p>808 rows \u00d7 3 columns</p> In\u00a0[\u00a0]: Copied! <pre>query = \"\"\" \n-- Generate lead plans with next plan and next start date\n\nWITH lead_plans AS (\n    SELECT\n        customer_id,\n        plan_id,\n        start_date,\n        LEAD (plan_id) OVER (\n            PARTITION BY\n                customer_id\n            ORDER BY\n                start_date\n        ) AS lead_plan_id,\n        LEAD (start_date) OVER (\n            PARTITION BY\n                customer_id\n            ORDER BY\n                start_date\n        ) AS lead_start_date\n    FROM\n        foodie_fi.subscriptions\n    WHERE\n        year (start_date) = 2020\n        AND plan_id != 0\n)\n\n-- Case 4: Pro monthly customers who upgrade to annual plans\n\nSELECT\n    customer_id,\n    plan_id,\n    start_date,\n    DATE_DIFF(\n        'MONTH',\n        start_date,\n        DATE_ADD('DAY', -1, lead_start_date)\n    ) AS month_diff\nFROM\n    lead_plans\nWHERE\n    plan_id = 2\n    AND lead_plan_id = 3;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  -- Generate lead plans with next plan and next start date  WITH lead_plans AS (     SELECT         customer_id,         plan_id,         start_date,         LEAD (plan_id) OVER (             PARTITION BY                 customer_id             ORDER BY                 start_date         ) AS lead_plan_id,         LEAD (start_date) OVER (             PARTITION BY                 customer_id             ORDER BY                 start_date         ) AS lead_start_date     FROM         foodie_fi.subscriptions     WHERE         year (start_date) = 2020         AND plan_id != 0 )  -- Case 4: Pro monthly customers who upgrade to annual plans  SELECT     customer_id,     plan_id,     start_date,     DATE_DIFF(         'MONTH',         start_date,         DATE_ADD('DAY', -1, lead_start_date)     ) AS month_diff FROM     lead_plans WHERE     plan_id = 2     AND lead_plan_id = 3; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[\u00a0]: customer_id plan_id start_date month_diff 0 19 2 2020-06-29 1 1 87 2 2020-08-15 0 2 195 2 2020-02-15 3 3 224 2 2020-02-02 2 4 250 2 2020-06-22 2 ... ... ... ... ... 65 838 2 2020-07-18 2 66 846 2 2020-03-25 5 67 854 2 2020-07-22 1 68 918 2 2020-09-01 2 69 937 2 2020-02-29 5 <p>70 rows \u00d7 4 columns</p> In\u00a0[262]: Copied! <pre>query = \"\"\" \n-- Generate lead plans with next plan and next start date\n\nWITH lead_plans AS (\n    SELECT\n        customer_id,\n        plan_id,\n        start_date,\n        LEAD (plan_id) OVER (\n            PARTITION BY\n                customer_id\n            ORDER BY\n                start_date\n        ) AS lead_plan_id,\n        LEAD (start_date) OVER (\n            PARTITION BY\n                customer_id\n            ORDER BY\n                start_date\n        ) AS lead_start_date\n    FROM\n        foodie_fi.subscriptions\n    WHERE\n        year (start_date) = 2020\n        AND plan_id != 0\n),\n\n-- Case 4: Pro monthly customers who upgrade to annual plans\n\ncase_4 AS (\n    SELECT\n        customer_id,\n        plan_id,\n        start_date,\n        DATE_DIFF(\n            'MONTH',\n            start_date,\n            DATE_ADD('DAY', -1, lead_start_date)\n        ) AS month_diff\n    FROM\n        lead_plans\n    WHERE\n        plan_id = 2\n        AND lead_plan_id = 3\n)\n\n-- Generate payments for case 4 customers\n\nSELECT\n    customer_id,\n    plan_id,\n    DATE_ADD('MONTH', seq, start_date) AS payment_date\nFROM\n    case_4\n    CROSS JOIN UNNEST (SEQUENCE (0, month_diff)) AS t (seq);\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  -- Generate lead plans with next plan and next start date  WITH lead_plans AS (     SELECT         customer_id,         plan_id,         start_date,         LEAD (plan_id) OVER (             PARTITION BY                 customer_id             ORDER BY                 start_date         ) AS lead_plan_id,         LEAD (start_date) OVER (             PARTITION BY                 customer_id             ORDER BY                 start_date         ) AS lead_start_date     FROM         foodie_fi.subscriptions     WHERE         year (start_date) = 2020         AND plan_id != 0 ),  -- Case 4: Pro monthly customers who upgrade to annual plans  case_4 AS (     SELECT         customer_id,         plan_id,         start_date,         DATE_DIFF(             'MONTH',             start_date,             DATE_ADD('DAY', -1, lead_start_date)         ) AS month_diff     FROM         lead_plans     WHERE         plan_id = 2         AND lead_plan_id = 3 )  -- Generate payments for case 4 customers  SELECT     customer_id,     plan_id,     DATE_ADD('MONTH', seq, start_date) AS payment_date FROM     case_4     CROSS JOIN UNNEST (SEQUENCE (0, month_diff)) AS t (seq); \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[262]: customer_id plan_id payment_date 0 19 2 2020-06-29 1 19 2 2020-07-29 2 87 2 2020-08-15 3 195 2 2020-02-15 4 195 2 2020-03-15 ... ... ... ... 222 888 2 2020-03-03 223 888 2 2020-04-03 224 916 2 2020-01-26 225 978 2 2020-09-03 226 978 2 2020-10-03 <p>227 rows \u00d7 3 columns</p> In\u00a0[263]: Copied! <pre>query = \"\"\" \n-- Generate lead plans with next plan and next start date\n\nWITH lead_plans AS (\n    SELECT\n        customer_id,\n        plan_id,\n        start_date,\n        LEAD (plan_id) OVER (\n            PARTITION BY\n                customer_id\n            ORDER BY\n                start_date\n        ) AS lead_plan_id,\n        LEAD (start_date) OVER (\n            PARTITION BY\n                customer_id\n            ORDER BY\n                start_date\n        ) AS lead_start_date\n    FROM\n        foodie_fi.subscriptions\n    WHERE\n        year (start_date) = 2020\n        AND plan_id != 0\n)\n\n-- Case 5: Annual pro payments\n\nSELECT\n    customer_id,\n    plan_id,\n    start_date AS payment_date\nFROM\n    lead_plans\nWHERE\n    plan_id = 3;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  -- Generate lead plans with next plan and next start date  WITH lead_plans AS (     SELECT         customer_id,         plan_id,         start_date,         LEAD (plan_id) OVER (             PARTITION BY                 customer_id             ORDER BY                 start_date         ) AS lead_plan_id,         LEAD (start_date) OVER (             PARTITION BY                 customer_id             ORDER BY                 start_date         ) AS lead_start_date     FROM         foodie_fi.subscriptions     WHERE         year (start_date) = 2020         AND plan_id != 0 )  -- Case 5: Annual pro payments  SELECT     customer_id,     plan_id,     start_date AS payment_date FROM     lead_plans WHERE     plan_id = 3; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[263]: customer_id plan_id payment_date 0 2 3 2020-09-27 1 9 3 2020-12-14 2 16 3 2020-10-21 3 17 3 2020-12-11 4 19 3 2020-08-29 ... ... ... ... 190 961 3 2020-11-21 191 969 3 2020-06-28 192 972 3 2020-02-12 193 974 3 2020-10-16 194 978 3 2020-11-03 <p>195 rows \u00d7 3 columns</p> In\u00a0[12]: Copied! <pre>query = \"\"\" \n-- Generate lead plans with next plan and next start date\n\nWITH lead_plans AS (\n    SELECT\n        customer_id,\n        plan_id,\n        start_date,\n        LEAD (plan_id) OVER (\n            PARTITION BY\n                customer_id\n            ORDER BY\n                start_date\n        ) AS lead_plan_id,\n        LEAD (start_date) OVER (\n            PARTITION BY\n                customer_id\n            ORDER BY\n                start_date\n        ) AS lead_start_date\n    FROM\n        foodie_fi.subscriptions\n    WHERE\n        year (start_date) = 2020\n        AND plan_id != 0\n),\n\n-- Case 1: Non-churn monthly customers\n\ncase_1 AS (\n    SELECT\n        customer_id,\n        plan_id,\n        start_date,\n        DATE_DIFF('MONTH', start_date, DATE '2020-12-31') AS month_diff\n    FROM\n        lead_plans\n    WHERE\n        lead_plan_id IS NULL\n        AND plan_id NOT IN (3, 4) -- Exclude churn and annual plans\n),\n\n-- Generate payments for case 1 customers\n\ncase_1_payments AS (\n    SELECT\n        customer_id,\n        plan_id,\n        DATE_ADD('MONTH', seq, start_date) AS payment_date\n    FROM\n        case_1\n        CROSS JOIN UNNEST (SEQUENCE (0, month_diff)) AS t (seq)\n),\n\n-- Case 2: Churn customers\n\ncase_2 AS (\n    SELECT\n        customer_id,\n        plan_id,\n        start_date,\n        DATE_DIFF(\n            'MONTH',\n            start_date,\n            DATE_ADD('DAY', -1, lead_start_date)\n        ) AS month_diff\n    FROM\n        lead_plans\n    WHERE\n        lead_plan_id = 4 -- Churn plans only\n),\n\n-- Generate payments for churn customers\n\ncase_2_payments AS (\n    SELECT\n        customer_id,\n        plan_id,\n        DATE_ADD('MONTH', seq, start_date) AS payment_date\n    FROM\n        case_2\n        CROSS JOIN UNNEST (SEQUENCE (0, month_diff)) AS t (seq)\n),\n\n-- Case 3: Customers who move from basic to pro plans\n\ncase_3 AS (\n    SELECT\n        customer_id,\n        plan_id,\n        start_date,\n        DATE_DIFF(\n            'MONTH',\n            start_date,\n            DATE_ADD('DAY', -1, lead_start_date)\n        ) AS month_diff\n    FROM\n        lead_plans\n    WHERE\n        plan_id = 1\n        AND lead_plan_id IN (2, 3)\n),\n\n-- Generate payments for case 3 customers\n\ncase_3_payments AS (\n    SELECT\n        customer_id,\n        plan_id,\n        DATE_ADD('MONTH', seq, start_date) AS payment_date\n    FROM\n        case_3\n        CROSS JOIN UNNEST (SEQUENCE (0, month_diff)) AS t (seq)\n),\n\n-- Case 4: Pro monthly customers who upgrade to annual plans\n\ncase_4 AS (\n    SELECT\n        customer_id,\n        plan_id,\n        start_date,\n        DATE_DIFF(\n            'MONTH',\n            start_date,\n            DATE_ADD('DAY', -1, lead_start_date)\n        ) AS month_diff\n    FROM\n        lead_plans\n    WHERE\n        plan_id = 2\n        AND lead_plan_id = 3\n),\n\n-- Generate payments for case 4 customers\n\ncase_4_payments AS (\n    SELECT\n        customer_id,\n        plan_id,\n        DATE_ADD('MONTH', seq, start_date) AS payment_date\n    FROM\n        case_4\n        CROSS JOIN UNNEST (SEQUENCE (0, month_diff)) AS t (seq)\n),\n\n-- Case 5: Annual pro payments\n\ncase_5_payments AS (\n    SELECT\n        customer_id,\n        plan_id,\n        start_date AS payment_date\n    FROM\n        lead_plans\n    WHERE\n        plan_id = 3\n),\n\n-- Union all payment cases\n\nunion_output AS (\n    SELECT\n        *\n    FROM\n        case_1_payments\n    UNION ALL\n    SELECT\n        *\n    FROM\n        case_2_payments\n    UNION ALL\n    SELECT\n        *\n    FROM\n        case_3_payments\n    UNION ALL\n    SELECT\n        *\n    FROM\n        case_4_payments\n    UNION ALL\n    SELECT\n        *\n    FROM\n        case_5_payments\n)\n\n-- Final output with pricing adjustments and payment order\n\nSELECT\n    uo.customer_id,\n    p.plan_id,\n    p.plan_name,\n    uo.payment_date,\n\n    -- Apply price deductions for basic to pro upgrades\n    \n    CASE\n        WHEN uo.plan_id IN (2, 3) AND LAG (uo.plan_id, 1) OVER w = 1 THEN p.price - 9.90\n        ELSE p.price\n    END AS amount,\n\n    ROW_NUMBER() OVER w AS payment_order\nFROM\n    union_output AS uo\n    INNER JOIN foodie_fi.plans AS p ON uo.plan_id = p.plan_id\nWINDOW\n    w AS (\n        PARTITION BY\n            uo.customer_id\n        ORDER BY\n            uo.payment_date\n    )\nORDER BY\n    uo.customer_id,\n    uo.payment_date;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  -- Generate lead plans with next plan and next start date  WITH lead_plans AS (     SELECT         customer_id,         plan_id,         start_date,         LEAD (plan_id) OVER (             PARTITION BY                 customer_id             ORDER BY                 start_date         ) AS lead_plan_id,         LEAD (start_date) OVER (             PARTITION BY                 customer_id             ORDER BY                 start_date         ) AS lead_start_date     FROM         foodie_fi.subscriptions     WHERE         year (start_date) = 2020         AND plan_id != 0 ),  -- Case 1: Non-churn monthly customers  case_1 AS (     SELECT         customer_id,         plan_id,         start_date,         DATE_DIFF('MONTH', start_date, DATE '2020-12-31') AS month_diff     FROM         lead_plans     WHERE         lead_plan_id IS NULL         AND plan_id NOT IN (3, 4) -- Exclude churn and annual plans ),  -- Generate payments for case 1 customers  case_1_payments AS (     SELECT         customer_id,         plan_id,         DATE_ADD('MONTH', seq, start_date) AS payment_date     FROM         case_1         CROSS JOIN UNNEST (SEQUENCE (0, month_diff)) AS t (seq) ),  -- Case 2: Churn customers  case_2 AS (     SELECT         customer_id,         plan_id,         start_date,         DATE_DIFF(             'MONTH',             start_date,             DATE_ADD('DAY', -1, lead_start_date)         ) AS month_diff     FROM         lead_plans     WHERE         lead_plan_id = 4 -- Churn plans only ),  -- Generate payments for churn customers  case_2_payments AS (     SELECT         customer_id,         plan_id,         DATE_ADD('MONTH', seq, start_date) AS payment_date     FROM         case_2         CROSS JOIN UNNEST (SEQUENCE (0, month_diff)) AS t (seq) ),  -- Case 3: Customers who move from basic to pro plans  case_3 AS (     SELECT         customer_id,         plan_id,         start_date,         DATE_DIFF(             'MONTH',             start_date,             DATE_ADD('DAY', -1, lead_start_date)         ) AS month_diff     FROM         lead_plans     WHERE         plan_id = 1         AND lead_plan_id IN (2, 3) ),  -- Generate payments for case 3 customers  case_3_payments AS (     SELECT         customer_id,         plan_id,         DATE_ADD('MONTH', seq, start_date) AS payment_date     FROM         case_3         CROSS JOIN UNNEST (SEQUENCE (0, month_diff)) AS t (seq) ),  -- Case 4: Pro monthly customers who upgrade to annual plans  case_4 AS (     SELECT         customer_id,         plan_id,         start_date,         DATE_DIFF(             'MONTH',             start_date,             DATE_ADD('DAY', -1, lead_start_date)         ) AS month_diff     FROM         lead_plans     WHERE         plan_id = 2         AND lead_plan_id = 3 ),  -- Generate payments for case 4 customers  case_4_payments AS (     SELECT         customer_id,         plan_id,         DATE_ADD('MONTH', seq, start_date) AS payment_date     FROM         case_4         CROSS JOIN UNNEST (SEQUENCE (0, month_diff)) AS t (seq) ),  -- Case 5: Annual pro payments  case_5_payments AS (     SELECT         customer_id,         plan_id,         start_date AS payment_date     FROM         lead_plans     WHERE         plan_id = 3 ),  -- Union all payment cases  union_output AS (     SELECT         *     FROM         case_1_payments     UNION ALL     SELECT         *     FROM         case_2_payments     UNION ALL     SELECT         *     FROM         case_3_payments     UNION ALL     SELECT         *     FROM         case_4_payments     UNION ALL     SELECT         *     FROM         case_5_payments )  -- Final output with pricing adjustments and payment order  SELECT     uo.customer_id,     p.plan_id,     p.plan_name,     uo.payment_date,      -- Apply price deductions for basic to pro upgrades          CASE         WHEN uo.plan_id IN (2, 3) AND LAG (uo.plan_id, 1) OVER w = 1 THEN p.price - 9.90         ELSE p.price     END AS amount,      ROW_NUMBER() OVER w AS payment_order FROM     union_output AS uo     INNER JOIN foodie_fi.plans AS p ON uo.plan_id = p.plan_id WINDOW     w AS (         PARTITION BY             uo.customer_id         ORDER BY             uo.payment_date     ) ORDER BY     uo.customer_id,     uo.payment_date; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[12]: customer_id plan_id plan_name payment_date amount payment_order 0 1 1 basic monthly 2020-08-08 9.9 1 1 1 1 basic monthly 2020-09-08 9.9 2 2 1 1 basic monthly 2020-10-08 9.9 3 3 1 1 basic monthly 2020-11-08 9.9 4 4 1 1 basic monthly 2020-12-08 9.9 5 ... ... ... ... ... ... ... 4441 999 2 pro monthly 2020-10-30 19.9 1 4442 999 2 pro monthly 2020-11-30 19.9 2 4443 1000 2 pro monthly 2020-03-26 19.9 1 4444 1000 2 pro monthly 2020-04-26 19.9 2 4445 1000 2 pro monthly 2020-05-26 19.9 3 <p>4446 rows \u00d7 6 columns</p>"},{"location":"foodie_fi/#global","title":"Global\u00b6","text":""},{"location":"foodie_fi/#problem-statement","title":"Problem Statement\u00b6","text":"<p>In 2020, Foodie-Fi was launched as a subscription-based streaming service focused exclusively on food-related content, offering customers unlimited access to on-demand cooking shows and videos. The service was built with a data-driven approach, aiming to leverage subscription and customer behavior data to guide strategic decisions, improve customer retention, and drive business growth.</p> <p>This case study aims to analyze Foodie-Fi's subscription data to uncover insights into customer behaviors, subscription patterns, and revenue trends, helping the business make informed decisions on pricing, features, and customer engagement strategies.</p>"},{"location":"foodie_fi/#entity-relationship-diagram","title":"Entity Relationship Diagram\u00b6","text":""},{"location":"foodie_fi/#plans-table","title":"Plans Table\u00b6","text":"<p>The <code>plans</code> table outlines the subscription options available to Foodie-Fi customers. Each plan includes unique features, pricing, and accessibility:</p> <ul> <li><p>Trial Plan: A free 7-day trial available to all customers upon signup. After the trial, customers are automatically transitioned to the Pro Monthly plan unless they cancel or choose another plan.</p> </li> <li><p>Basic Monthly Plan: Provides limited access to videos for $9.90 per month.</p> </li> <li><p>Pro Monthly Plan: Offers unlimited streaming and the ability to download videos for $19.90 per month.</p> </li> <li><p>Pro Annual Plan: Provides the same benefits as the Pro Monthly plan but at a discounted annual rate of $199.</p> </li> <li><p>Churn Plan: Represents canceled subscriptions. Customers retain access until the end of the current billing period, but the plan price is recorded as null.</p> </li> </ul>"},{"location":"foodie_fi/#subscriptions-table","title":"Subscriptions Table\u00b6","text":"<p>The <code>subscriptions</code> table captures customer subscription events and changes over time. It records the exact date when a customer transitions to a specific plan. Key behaviors reflected in this table include:</p> <ul> <li><p>Downgrades or Cancellations: If a customer cancels or downgrades from a higher plan, their current plan remains active until the billing period ends. The <code>start_date</code> indicates when the new plan becomes effective.</p> </li> <li><p>Upgrades: If a customer upgrades their subscription (e.g., from Basic to Pro), the new plan takes effect immediately, and the <code>start_date</code> reflects this change.</p> </li> <li><p>Churn: When a customer decides to cancel their subscription, their <code>start_date</code> reflects the decision date, even though access continues until the billing period ends.</p> </li> </ul>"},{"location":"foodie_fi/#tables","title":"Tables\u00b6","text":""},{"location":"foodie_fi/#data-analysis","title":"Data Analysis\u00b6","text":""},{"location":"foodie_fi/#q1","title":"Q1\u00b6","text":"<p>How many customers has Foodie-Fi ever had?</p>"},{"location":"foodie_fi/#q2","title":"Q2\u00b6","text":"<p>What is the monthly distribution of trial plan <code>start_date</code> values for our dataset?</p>"},{"location":"foodie_fi/#q3","title":"Q3\u00b6","text":"<p>What plan <code>start_date</code> values occur after the year 2020 for the dataset? Show the breakdown by count of events for each <code>plan_name</code>.</p>"},{"location":"foodie_fi/#q4","title":"Q4\u00b6","text":"<p>What is the customer count and percentage of customers who have churned rounded to 1 decimal place?</p>"},{"location":"foodie_fi/#q5","title":"Q5\u00b6","text":"<p>How many customers have churned straight after their initial free trial - what percentage is this rounded to 1 decimal place?</p>"},{"location":"foodie_fi/#gather-each-customers-plans","title":"Gather Each Customer's Plans\u00b6","text":"<p>We first gather each customer's plan history into an array of comma-separated plans.</p> <p>In addition, we also create a field called <code>churn_pattern</code> that is a two-element array that describes the transition pattern we wish to find.</p>"},{"location":"foodie_fi/#check-specific-combination-of-plans","title":"Check Specific Combination of Plans\u00b6","text":"<p>Filter for rows where the customers' journeys include the trial plan followed by a churn event.</p>"},{"location":"foodie_fi/#window-function-approach","title":"Window Function Approach\u00b6","text":"<p>First, check the assumption that the first plan for every customer is a trial:</p>"},{"location":"foodie_fi/#q6","title":"Q6\u00b6","text":"<p>What is the number and percentage of customer plans after their initial free trial?</p>"},{"location":"foodie_fi/#q7","title":"Q7\u00b6","text":"<p>What is the customer count and percentage breakdown of all 5 <code>plan_name</code> values at <code>2020-12-31</code>?</p>"},{"location":"foodie_fi/#q8","title":"Q8\u00b6","text":"<p>How many customers have upgraded to an annual plan in 2020?</p>"},{"location":"foodie_fi/#q9","title":"Q9\u00b6","text":"<p>How many days on average does it take for a customer to upgrade to an annual plan from the day they join Foodie-Fi?</p>"},{"location":"foodie_fi/#q10","title":"Q10\u00b6","text":"<p>Further breakdown the average value computed above into 30 day periods (i.e. 0-30 days, 31-60 days etc).</p>"},{"location":"foodie_fi/#bin-number-calculation","title":"Bin Number Calculation\u00b6","text":"<p>After computing the number of days between the <code>trial_start_date</code> and the <code>upgrade_date</code>, we can calculate a bin number for each <code>day_diff</code> value. This bin number will be used to group the data into 30-day periods.</p>"},{"location":"foodie_fi/#q11","title":"Q11\u00b6","text":"<p>How many customers downgraded from a pro monthly to a basic monthly plan in 2020?</p>"},{"location":"foodie_fi/#downgrade-counts","title":"Downgrade Counts\u00b6","text":""},{"location":"foodie_fi/#upgrade-counts","title":"Upgrade Counts\u00b6","text":""},{"location":"foodie_fi/#transition-matrix-analysis","title":"Transition Matrix Analysis\u00b6","text":""},{"location":"foodie_fi/#counts","title":"Counts\u00b6","text":""},{"location":"foodie_fi/#probabilities","title":"Probabilities\u00b6","text":""},{"location":"foodie_fi/#payment-table","title":"Payment Table\u00b6","text":"<p>The Foodie-Fi team wants a new <code>payments</code> table for the year 2020. This table should include the amounts paid by each customer from the subscriptions table, following these rules:</p> <ol> <li><p>Monthly Payment Schedule: Payments for monthly plans occur on the same day of the month as the original subscription's <code>start_date</code>.</p> </li> <li><p>Upgrades to Higher Plans:</p> <ul> <li><p>If a customer upgrades from a basic plan to a monthly or pro plan, the amount already paid for the current month is deducted from the upgrade cost, and the new plan starts immediately.</p> </li> <li><p>If a customer upgrades from a pro monthly plan to a pro annual plan, the payment for the annual plan is made at the end of the current monthly billing cycle, and the annual plan begins at that time.</p> </li> </ul> </li> <li><p>Churned Customers: Once a customer churns, they no longer make any payments.</p> </li> </ol>"},{"location":"foodie_fi/#approach","title":"Approach\u00b6","text":"<p>We need to partition the entire sample space into 5 cases that are disjoint and exhaustive:</p> <ul> <li><p>Case 1: Customers with monthly plans who haven't churned as of <code>2020-12-31</code>. Their payments are calculated up to the end of the year.</p> </li> <li><p>Case 2: Customers who churned in 2020, capturing payments from their start date to one day before the churn date.</p> </li> <li><p>Case 3: Customers upgrading from basic to pro monthly or annual plans, with payments calculated for the duration of their basic plan.</p> </li> <li><p>Case 4: Pro monthly customers who upgrade to pro annual plans. Payments are captured for the monthly plan before the upgrade.</p> </li> <li><p>Case 5: Customers on pro annual plans, with a single payment captured for their annual subscription.</p> </li> </ul> <p>These cases were identified through exploratory data analysis (EDA) and ensure a perfect partitioning of the sample space. It is not necessary to account for every theoretical permutation of transitions between plans but rather to focus solely on transitions that have occurred or are plausible based on the observed data.</p>"},{"location":"foodie_fi/#lead-plans-cte","title":"Lead Plans CTE\u00b6","text":"<p>The <code>lead_plans</code> CTE is the base CTE that identifies each customer's current plan and the next plan (if any) during 2020:</p> <ul> <li><p>Columns:</p> <ul> <li><code>customer_id</code>, <code>plan_id</code>, and <code>start_date</code>: The customer's current plan and its start date.</li> <li><code>lead_plan_id</code>: The next plan for the customer (using <code>LEAD(plan_id)</code>).</li> <li><code>lead_start_date</code>: The start date of the next plan (<code>LEAD(start_date)</code>).</li> </ul> </li> <li><p>Logic:</p> <ul> <li>Window Function: Groups by <code>customer_id</code> and orders plans by <code>start_date</code> in ascending order to find the next plan and its start date.</li> <li>Filters: Filters out plan_id = 0 (trial) and focuses on active subscriptions in 2020.</li> <li>Ordering: Ensures output is sorted by customer and subscription timeline.</li> </ul> </li> </ul> <p>Purpose: Tracks plan transitions for each customer in 2020.</p>"},{"location":"foodie_fi/#case-1-non-churn-monthly-customers","title":"Case 1: Non-Churn Monthly Customers\u00b6","text":"<p>Query:</p> <p>The first query selects customers on monthly plans who did not churn by <code>2020-12-31</code>.</p> <ul> <li><p>Logic:</p> <ul> <li><p>Use <code>lead_plans</code> to find subscriptions with no next plan (<code>lead_plan_id IS NULL</code>).</p> </li> <li><p>Exclude churn (<code>plan_id = 4</code>) and annual plans (<code>plan_id = 3</code>).</p> </li> <li><p>Calculate <code>month_diff</code>, the number of months from the subscription <code>start_date</code> to <code>2020-12-31</code>. This is so that we can generate monthly payment dates for each customer for each payment month in 2020.</p> </li> </ul> </li> </ul> <p>Output:</p> <p>Each row represents a qualifying customer with their monthly plan, start date, and the total months they are active in 2020.</p>"},{"location":"foodie_fi/#case-2-churn-customers","title":"Case 2: Churn Customers\u00b6","text":"<p>Query:</p> <p>The first query identifies customers who churned in 2020.</p> <ul> <li><p>Logic:</p> <ul> <li><p>Use <code>lead_plans</code> to find customers whose next plan (<code>lead_plan_id</code>) is <code>4</code> (churn).</p> </li> <li><p>Calculate <code>month_diff</code> as the number of months between the <code>start_date</code> and the day before their <code>lead_start_date</code> (churn date).</p> <ol> <li><p>Purpose of <code>DATE_DIFF('MONTH', ...)</code>:</p> <ul> <li>This calculates the number of full months between the <code>start_date</code> (when the current plan began) and the date just before the <code>lead_start_date</code> (when the next plan starts).</li> <li>Using <code>DATE_ADD('DAY', -1, lead_start_date)</code> ensures that the upgrade day itself is excluded, so the calculation only covers the period the customer was on the basic plan.</li> </ul> </li> <li><p>Why Subtract 1 Day (<code>DATE_ADD('DAY', -1, lead_start_date)</code>)?</p> <ul> <li>By default, <code>DATE_DIFF</code> counts the difference inclusive of both start and end dates if they fall on the same calendar month.</li> <li>Subtracting 1 day ensures that if the upgrade happens at the beginning of a new month, only the full months spent on the current plan are included.</li> </ul> </li> </ol> <ul> <li><p>Scenario: A customer starts a basic plan on <code>2020-03-15</code> and churns on <code>2020-06-01</code>.</p> <ul> <li>Without subtracting a day: <code>DATE_DIFF('MONTH', '2020-03-15', '2020-06-01')</code> = 3 (March \u2192 June).</li> <li>With subtraction: <code>DATE_DIFF('MONTH', '2020-03-15', '2020-05-31')</code> = 2 (March \u2192 May).</li> </ul> <p>This ensures accurate billing by capturing only the months the customer was on the current plan.</p> </li> </ul> </li> </ul> </li> </ul> <p>Output:</p> <p>Each row represents a customer who churned, with their plan, subscription start date, and the total months of payments before churning.</p>"},{"location":"foodie_fi/#case-3-customers-who-move-from-monthly-basic-to-monthly-or-annual-pro-plans","title":"Case 3: Customers Who Move from Monthly Basic to Monthly or Annual Pro plans\u00b6","text":"<p>Query:</p> <p>Select customers whose current plan is <code>1</code> (<code>basic monthly</code>) and whose next plan (<code>lead_plan_id</code>) is <code>2</code> (<code>pro monthly</code>) or <code>3</code> (<code>pro annual</code>).</p> <ul> <li><p>Columns:</p> <ul> <li><code>customer_id</code>: Identifies the customer.</li> <li><code>plan_id</code>: Current plan (<code>basic monthly</code>).</li> <li><code>start_date</code>: The start date of the basic plan.</li> <li><code>month_diff</code>: Calculates the number of months between the basic plan\u2019s <code>start_date</code> and one day before the <code>lead_start_date</code> (upgrade date).</li> </ul> </li> <li><p>Condition: <code>plan_id = 1</code> ensures we only include current basic monthly customers, and <code>lead_plan_id IN (2, 3)</code> ensures they upgraded to a pro plan.</p> </li> </ul> <p>Output:</p> <p>Each row represents a customer with their basic monthly subscription, the start date, and how long they remained on the basic plan before upgrading.</p>"},{"location":"foodie_fi/#case-4-pro-monthly-customers-who-upgrade-to-annual-plans","title":"Case 4: Pro Monthly Customers Who Upgrade to Annual Plans\u00b6","text":"<p>Query:</p> <p>Select customers whose current plan is <code>2</code> (<code>pro monthly</code>) and whose next plan (<code>lead_plan_id</code>) is <code>3</code> (<code>pro annual</code>).</p> <ul> <li><p>Columns:</p> <ul> <li><p><code>customer_id</code>: Identifies the customer.</p> </li> <li><p><code>plan_id</code>: Current plan (<code>pro monthly</code>).</p> </li> <li><p><code>start_date</code>: The start date of the monthly pro plan.</p> </li> <li><p><code>month_diff</code>: Calculates the number of months between the <code>start_date</code> of the monthly plan and one day before the <code>lead_start_date</code> (when the upgrade to the annual plan starts).</p> </li> </ul> </li> <li><p>Condition: <code>plan_id = 2</code> ensures only pro monthly plans are included, and <code>lead_plan_id = 3</code> ensures the next plan is an annual upgrade.</p> </li> </ul> <p>Output:</p> <p>Each row represents a customer with their pro monthly subscription, the start date, and the number of months they were on the monthly plan before upgrading to an annual plan.</p>"},{"location":"foodie_fi/#case-5-annual-plan-payments","title":"Case 5: Annual Plan Payments\u00b6","text":"<p>Query:</p> <p>This query identifies customers who were strictly on the pro annual plan (<code>plan_id = 3</code>) during 2020.</p> <ul> <li><p>Columns:</p> <ul> <li><code>customer_id</code>: Identifies the customer.</li> <li><code>plan_id</code>: The customer's subscription plan (<code>pro annual</code>).</li> <li><code>start_date AS payment_date</code>: The annual plan's <code>start_date</code> serves as the single payment date since annual plans are billed once upfront.</li> </ul> </li> <li><p>Condition:</p> <ul> <li><code>plan_id = 3</code>: Ensures only pro annual subscriptions are included.</li> </ul> </li> </ul>"},{"location":"foodie_fi/#final","title":"Final\u00b6","text":""},{"location":"foodie_fi/#union-of-all-payment-cases","title":"Union of All Payment Cases:\u00b6","text":"<ul> <li>Combines payments from all cases (<code>case_1</code> to <code>case_5</code>) into a single dataset (<code>union_output</code>).</li> <li>Ensures all payment scenarios are accounted for, including churn, upgrades, and annual plans.</li> </ul>"},{"location":"foodie_fi/#adjustment-logic-for-pricing","title":"Adjustment Logic for Pricing\u00b6","text":"<p>The final query applies price adjustments for customers who upgrade from lower-tier plans (e.g., basic monthly) to higher-tier plans (e.g., pro monthly or annual).</p>"},{"location":"foodie_fi/#adjustment-logic","title":"Adjustment Logic:\u00b6","text":"<pre>CASE\n    WHEN uo.plan_id IN (2, 3) AND LAG (uo.plan_id, 1) OVER w = 1 THEN p.price - 9.90\n    ELSE p.price\nEND AS amount\n</pre> <ol> <li><p>Condition:</p> <ul> <li><code>uo.plan_id IN (2, 3)</code>: The new plan is either <code>pro monthly</code> (<code>2</code>) or <code>pro annual</code> (<code>3</code>).</li> <li><code>LAG(uo.plan_id, 1) OVER w = 1</code>: The previous plan was <code>basic monthly</code> (<code>1</code>).</li> </ul> </li> <li><p>Price Deduction:</p> <ul> <li><p>If a customer upgrades from <code>basic monthly</code> (<code>9.90</code>) to <code>pro monthly</code> (<code>19.90</code>) or <code>pro annual</code> (<code>199</code>), the <code>9.90</code> already paid for the basic plan is subtracted from the new plan price.</p> </li> <li><p>The adjustment reflects that the customer has already paid for the current month on the basic plan.</p> </li> </ul> </li> <li><p>Default Case:</p> <ul> <li>For all other cases, the price is simply <code>p.price</code> (the full cost of the plan).</li> </ul> </li> </ol>"},{"location":"foodie_fi/#purpose-of-adjustments","title":"Purpose of Adjustments:\u00b6","text":"<ul> <li>Fair Billing: Ensures customers don\u2019t overpay when upgrading plans mid-cycle.</li> <li>Accurate Payments: Reflects the partial credit for the amount already paid on a lower-tier plan when upgrading.</li> </ul>"},{"location":"foodie_fi/#example-scenarios","title":"Example Scenarios:\u00b6","text":"<ol> <li><p>Upgrade from Basic Monthly to Pro Monthly:</p> <ul> <li><code>Basic Monthly Price: 9.90</code></li> <li><code>Pro Monthly Price: 19.90</code></li> <li>Adjusted Price: <code>19.90 - 9.90 = 10.00</code></li> </ul> </li> <li><p>Upgrade from Basic Monthly to Pro Annual:</p> <ul> <li><code>Basic Monthly Price: 9.90</code></li> <li><code>Pro Annual Price: 199</code></li> <li>Adjusted Price: <code>199 - 9.90 = 189.10</code></li> </ul> </li> </ol> <p>This logic ensures fairness and prevents customers from paying twice for overlapping subscription periods during plan upgrades.</p>"},{"location":"fresh_segments/","title":"Fresh Segments","text":"In\u00a0[43]: Copied! <pre>from IPython.core.interactiveshell import InteractiveShell\n\nInteractiveShell.ast_node_interactivity = \"all\"\n\nimport os\nimport sys\nfrom typing import Dict, Optional, Tuple\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport pymc as pm\nimport seaborn as sns\n\nsys.path.append(\"../../../\")\nfrom src.athena import Athena\nfrom src.utils import create_session\n</pre> from IPython.core.interactiveshell import InteractiveShell  InteractiveShell.ast_node_interactivity = \"all\"  import os import sys from typing import Dict, Optional, Tuple  import matplotlib.pyplot as plt import numpy as np import pandas as pd import pymc as pm import seaborn as sns  sys.path.append(\"../../../\") from src.athena import Athena from src.utils import create_session In\u00a0[87]: Copied! <pre>boto3_session = create_session(\n    profile_name=\"dev\",\n    role_arn=os.getenv(\"ATHENA_IAM_ROLE_ARN\"),\n)\n\nwait = True\nctas_approach = False\nstorage_format = \"PARQUET\"\nwrite_compression = \"SNAPPY\"\n\ndatabase = \"fresh_segments\"\ntables = [\"interest_map\", \"interest_metrics\"]\n\nathena = Athena(boto3_session=boto3_session, s3_output=os.getenv(\"ATHENA_S3_OUTPUT\"))\nathena\n</pre> boto3_session = create_session(     profile_name=\"dev\",     role_arn=os.getenv(\"ATHENA_IAM_ROLE_ARN\"), )  wait = True ctas_approach = False storage_format = \"PARQUET\" write_compression = \"SNAPPY\"  database = \"fresh_segments\" tables = [\"interest_map\", \"interest_metrics\"]  athena = Athena(boto3_session=boto3_session, s3_output=os.getenv(\"ATHENA_S3_OUTPUT\")) athena Out[87]: <pre>Athena(boto3_session=Session(region_name='us-east-1'), s3_output=s3://sql-case-studies/query_results)</pre> In\u00a0[3]: Copied! <pre>for table in tables:\n    athena.query(\n        database=database,\n        query=f\"\"\" \n                SELECT\n                    *\n                FROM\n                    {database}.{table} TABLESAMPLE BERNOULLI(30);\n              \"\"\",\n        ctas_approach=ctas_approach,\n    )\n</pre> for table in tables:     athena.query(         database=database,         query=f\"\"\"                  SELECT                     *                 FROM                     {database}.{table} TABLESAMPLE BERNOULLI(30);               \"\"\",         ctas_approach=ctas_approach,     ) Out[3]: id interest_name interest_summary created_at last_modified 0 1 Fitness Enthusiasts Consumers using fitness tracking apps and webs... 2016-05-26 14:57:59 2018-05-23 11:30:12 1 5 Brides &amp; Wedding Planners People researching wedding ideas and vendors. 2016-05-26 14:57:59 2018-05-23 11:30:12 2 6 Vacation Planners Consumers reading reviews of vacation destinat... 2016-05-26 14:57:59 2018-05-23 11:30:13 3 8 Business News Readers Readers of online business news content. 2016-05-26 14:57:59 2018-05-23 11:30:12 4 12 Thrift Store Shoppers Consumers shopping online for clothing at thri... 2016-05-26 14:57:59 2018-03-16 13:14:00 ... ... ... ... ... ... 366 47850 Asheville Trip Planners People researching attractions and accommodati... 2019-03-15 22:00:02 2019-03-21 15:33:09 367 48154 Elite Cycling Gear Shoppers Consumers researching and shopping for elite c... 2019-03-21 22:00:00 2019-03-22 15:31:46 368 48465 HGTV Enthusiasts People interested in HGTV shows and home remod... 2019-03-27 22:00:01 2019-04-02 16:17:29 369 50860 Food Delivery Service Users Users of online food delivery services. 2019-04-23 18:00:02 2019-04-24 18:30:04 370 51119 Skin Disorder Researchers People reading news and advice on preventing a... 2019-04-26 18:00:00 2019-04-29 14:20:04 <p>371 rows \u00d7 5 columns</p> Out[3]: record_month record_year month_year interest_id composition index_value ranking percentile_ranking 0 7 2018 07-2018 32486 11.89 6.19 1 99.86 1 7 2018 07-2018 18923 10.85 5.29 3 99.59 2 7 2018 07-2018 6110 11.57 4.79 11 98.49 3 7 2018 07-2018 4 13.97 4.53 14 98.08 4 7 2018 07-2018 17 7.89 4.15 19 97.39 ... ... ... ... ... ... ... ... ... 4307 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 1.54 0.77 1181 1.09 4308 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 1.94 0.77 1181 1.09 4309 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 1.62 0.74 1187 0.59 4310 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 1.62 0.68 1191 0.25 4311 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 1.64 0.62 1194 0.00 <p>4312 rows \u00d7 8 columns</p> In\u00a0[4]: Copied! <pre>query = \"\"\" \nWITH grouped_counts AS (\n  SELECT\n    record_month,\n    record_year,\n    month_year,\n    interest_id,\n    composition,\n    index_value,\n    ranking,\n    percentile_ranking,\n    COUNT(*) AS freq\n  FROM\n    fresh_segments.interest_metrics\n  WHERE\n    record_month IS NOT NULL\n    AND record_year IS NOT NULL\n    AND month_year IS NOT NULL\n    AND interest_id IS NOT NULL\n  GROUP BY\n    record_month,\n    record_year,\n    month_year,\n    interest_id,\n    composition,\n    index_value,\n    ranking,\n    percentile_ranking\n)\nSELECT\n  *\nFROM\n  grouped_counts\nWHERE\n  freq &gt; 1\nORDER BY\n  freq DESC;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  WITH grouped_counts AS (   SELECT     record_month,     record_year,     month_year,     interest_id,     composition,     index_value,     ranking,     percentile_ranking,     COUNT(*) AS freq   FROM     fresh_segments.interest_metrics   WHERE     record_month IS NOT NULL     AND record_year IS NOT NULL     AND month_year IS NOT NULL     AND interest_id IS NOT NULL   GROUP BY     record_month,     record_year,     month_year,     interest_id,     composition,     index_value,     ranking,     percentile_ranking ) SELECT   * FROM   grouped_counts WHERE   freq &gt; 1 ORDER BY   freq DESC; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[4]: record_month record_year month_year interest_id composition index_value ranking percentile_ranking freq <p>No duplicate are identified in the table.</p> In\u00a0[5]: Copied! <pre>query = \"\"\" \nSELECT\n    SUM(CASE WHEN record_month IS NULL THEN 1 ELSE 0 END) AS record_month_null,\n    SUM(CASE WHEN record_year IS NULL THEN 1 ELSE 0 END) AS record_year_null,\n    SUM(CASE WHEN month_year IS NULL THEN 1 ELSE 0 END) AS month_year_null,\n    SUM(CASE WHEN interest_id IS NULL THEN 1 ELSE 0 END) AS interest_id_null,\n    SUM(CASE WHEN composition IS NULL THEN 1 ELSE 0 END) AS composition_null,\n    SUM(CASE WHEN index_value IS NULL THEN 1 ELSE 0 END) AS index_value_null,\n    SUM(CASE WHEN ranking IS NULL THEN 1 ELSE 0 END) AS ranking_null,\n    SUM(CASE WHEN percentile_ranking IS NULL THEN 1 ELSE 0 END) AS percentile_ranking_null\nFROM\n    fresh_segments.interest_metrics;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  SELECT     SUM(CASE WHEN record_month IS NULL THEN 1 ELSE 0 END) AS record_month_null,     SUM(CASE WHEN record_year IS NULL THEN 1 ELSE 0 END) AS record_year_null,     SUM(CASE WHEN month_year IS NULL THEN 1 ELSE 0 END) AS month_year_null,     SUM(CASE WHEN interest_id IS NULL THEN 1 ELSE 0 END) AS interest_id_null,     SUM(CASE WHEN composition IS NULL THEN 1 ELSE 0 END) AS composition_null,     SUM(CASE WHEN index_value IS NULL THEN 1 ELSE 0 END) AS index_value_null,     SUM(CASE WHEN ranking IS NULL THEN 1 ELSE 0 END) AS ranking_null,     SUM(CASE WHEN percentile_ranking IS NULL THEN 1 ELSE 0 END) AS percentile_ranking_null FROM     fresh_segments.interest_metrics; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[5]: record_month_null record_year_null month_year_null interest_id_null composition_null index_value_null ranking_null percentile_ranking_null 0 1194 1194 1194 1193 0 0 0 0 In\u00a0[6]: Copied! <pre>query = \"\"\" \nSELECT \n    interest_id\nFROM\n    fresh_segments.interest_metrics\nWHERE\n    1 = 1\n    AND month_year IS NULL\n    AND interest_id IS NOT NULL;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  SELECT      interest_id FROM     fresh_segments.interest_metrics WHERE     1 = 1     AND month_year IS NULL     AND interest_id IS NOT NULL; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[6]: interest_id 0 21246 In\u00a0[7]: Copied! <pre>query = \"\"\" \nSELECT\n    *\nFROM\n    fresh_segments.interest_metrics\nWHERE\n    interest_id = 21246;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  SELECT     * FROM     fresh_segments.interest_metrics WHERE     interest_id = 21246; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[7]: record_month record_year month_year interest_id composition index_value ranking percentile_ranking 0 7 2018 07-2018 21246 2.26 0.65 722 0.96 1 8 2018 08-2018 21246 2.13 0.59 765 0.26 2 9 2018 09-2018 21246 2.06 0.61 774 0.77 3 10 2018 10-2018 21246 1.74 0.58 855 0.23 4 11 2018 11-2018 21246 2.25 0.78 908 2.16 5 12 2018 12-2018 21246 1.97 0.70 983 1.21 6 1 2019 01-2019 21246 2.05 0.76 954 1.95 7 2 2019 02-2019 21246 1.84 0.68 1109 1.07 8 3 2019 03-2019 21246 1.75 0.67 1123 1.14 9 4 2019 04-2019 21246 1.58 0.63 1092 0.64 10 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 21246 1.61 0.68 1191 0.25 <p>In this case, the options are:</p> <ul> <li><p>Impute the missing <code>month_year</code> value with the <code>month_year</code> value of the lateset record with the same <code>interest_id</code> value</p> </li> <li><p>Impute the missing <code>month_year</code> value with the <code>created_at</code> value from the <code>fresh_segments.interest_map</code> table</p> </li> <li><p>Drop the record</p> </li> </ul> <p>The first two options run the risk of introducing bias into the data, but dropping the record may result in a loss of valuable information. Given that there is only one record with a missing <code>month_year</code> value, we can argue that the risk of losing valuable information is low relative to the risk of introducing bias into the data by imputing date information.</p> In\u00a0[8]: Copied! <pre># New ctas table with modified column and dropping all records with missing date information\nquery = \"\"\" \nSELECT\n    record_month,\n    record_year,\n    CASE WHEN month_year IS NULL THEN NULL ELSE DATE_PARSE(CONCAT_WS('-', '01', month_year), '%d-%m-%Y') END AS month_year,\n    interest_id,\n    composition,\n    index_value,\n    ranking,\n    percentile_ranking\nFROM\n    fresh_segments.interest_metrics\nWHERE\n    month_year IS NOT NULL;\n\"\"\"\n\nathena.create_ctas_table(\n    database=database,\n    query=query,\n    ctas_table=\"interest_metrics_modified\",\n    s3_output=\"s3://sql-case-studies/fresh_segments/interest_metrics/\",\n    storage_format=storage_format,\n    write_compression=write_compression,\n    wait=wait,\n)\n\n# Drop old table\nathena.drop_table(database=database, table=\"interest_metrics\")\n</pre> # New ctas table with modified column and dropping all records with missing date information query = \"\"\"  SELECT     record_month,     record_year,     CASE WHEN month_year IS NULL THEN NULL ELSE DATE_PARSE(CONCAT_WS('-', '01', month_year), '%d-%m-%Y') END AS month_year,     interest_id,     composition,     index_value,     ranking,     percentile_ranking FROM     fresh_segments.interest_metrics WHERE     month_year IS NOT NULL; \"\"\"  athena.create_ctas_table(     database=database,     query=query,     ctas_table=\"interest_metrics_modified\",     s3_output=\"s3://sql-case-studies/fresh_segments/interest_metrics/\",     storage_format=storage_format,     write_compression=write_compression,     wait=wait, )  # Drop old table athena.drop_table(database=database, table=\"interest_metrics\") Out[8]: <pre>{'ctas_database': 'fresh_segments',\n 'ctas_table': 'interest_metrics_modified',\n 'ctas_query_metadata': _QueryMetadata(execution_id='349c49e1-5136-4e2d-baad-7e7764a0c489', dtype={'rows': 'Int64'}, parse_timestamps=[], parse_dates=[], parse_geometry=[], converters={}, binaries=[], output_location='s3://sql-case-studies/fresh_segments/interest_metrics/tables/349c49e1-5136-4e2d-baad-7e7764a0c489', manifest_location='s3://sql-case-studies/fresh_segments/interest_metrics/tables/349c49e1-5136-4e2d-baad-7e7764a0c489-manifest.csv', raw_payload={'QueryExecutionId': '349c49e1-5136-4e2d-baad-7e7764a0c489', 'Query': 'CREATE TABLE \"fresh_segments\".\"interest_metrics_modified\"\\nWITH(\\n    external_location = \\'s3://sql-case-studies/fresh_segments/interest_metrics/interest_metrics_modified\\',\\n    write_compression = \\'SNAPPY\\',\\n    format = \\'PARQUET\\')\\nAS  \\nSELECT\\n    record_month,\\n    record_year,\\n    CASE WHEN month_year IS NULL THEN NULL ELSE DATE_PARSE(CONCAT_WS(\\'-\\', \\'01\\', month_year), \\'%d-%m-%Y\\') END AS month_year,\\n    interest_id,\\n    composition,\\n    index_value,\\n    ranking,\\n    percentile_ranking\\nFROM\\n    fresh_segments.interest_metrics\\nWHERE\\n    month_year IS NOT NULL', 'StatementType': 'DDL', 'ResultConfiguration': {'OutputLocation': 's3://sql-case-studies/fresh_segments/interest_metrics/tables/349c49e1-5136-4e2d-baad-7e7764a0c489'}, 'ResultReuseConfiguration': {'ResultReuseByAgeConfiguration': {'Enabled': False}}, 'QueryExecutionContext': {'Database': 'fresh_segments'}, 'Status': {'State': 'SUCCEEDED', 'SubmissionDateTime': datetime.datetime(2025, 1, 20, 15, 8, 51, 96000, tzinfo=tzlocal()), 'CompletionDateTime': datetime.datetime(2025, 1, 20, 15, 8, 52, 488000, tzinfo=tzlocal())}, 'Statistics': {'EngineExecutionTimeInMillis': 1199, 'DataScannedInBytes': 95524, 'DataManifestLocation': 's3://sql-case-studies/fresh_segments/interest_metrics/tables/349c49e1-5136-4e2d-baad-7e7764a0c489-manifest.csv', 'TotalExecutionTimeInMillis': 1392, 'QueryQueueTimeInMillis': 70, 'ServicePreProcessingTimeInMillis': 96, 'QueryPlanningTimeInMillis': 146, 'ServiceProcessingTimeInMillis': 27, 'ResultReuseInformation': {'ReusedPreviousResult': False}}, 'WorkGroup': 'primary', 'EngineVersion': {'SelectedEngineVersion': 'AUTO', 'EffectiveEngineVersion': 'Athena engine version 3'}, 'SubstatementType': 'CREATE_TABLE_AS_SELECT'})}</pre> <pre>Query executed successfully\n</pre> In\u00a0[9]: Copied! <pre># New ctas table with original name\nquery = \"\"\" \nSELECT\n    *\nFROM\n    fresh_segments.interest_metrics_modified;\n\"\"\"\n\nathena.create_ctas_table(\n    database=database,\n    query=query,\n    ctas_table=\"interest_metrics\",\n    s3_output=\"s3://sql-case-studies/fresh_segments/interest_metrics/\",\n    storage_format=storage_format,\n    write_compression=write_compression,\n    wait=wait,\n)\n\n# Drop temporary ctas table\nathena.drop_table(database=database, table=\"interest_metrics_modified\")\n</pre> # New ctas table with original name query = \"\"\"  SELECT     * FROM     fresh_segments.interest_metrics_modified; \"\"\"  athena.create_ctas_table(     database=database,     query=query,     ctas_table=\"interest_metrics\",     s3_output=\"s3://sql-case-studies/fresh_segments/interest_metrics/\",     storage_format=storage_format,     write_compression=write_compression,     wait=wait, )  # Drop temporary ctas table athena.drop_table(database=database, table=\"interest_metrics_modified\") Out[9]: <pre>{'ctas_database': 'fresh_segments',\n 'ctas_table': 'interest_metrics',\n 'ctas_query_metadata': _QueryMetadata(execution_id='cc381175-9e88-4338-8acc-44814a4465b5', dtype={'rows': 'Int64'}, parse_timestamps=[], parse_dates=[], parse_geometry=[], converters={}, binaries=[], output_location='s3://sql-case-studies/fresh_segments/interest_metrics/tables/cc381175-9e88-4338-8acc-44814a4465b5', manifest_location='s3://sql-case-studies/fresh_segments/interest_metrics/tables/cc381175-9e88-4338-8acc-44814a4465b5-manifest.csv', raw_payload={'QueryExecutionId': 'cc381175-9e88-4338-8acc-44814a4465b5', 'Query': 'CREATE TABLE \"fresh_segments\".\"interest_metrics\"\\nWITH(\\n    external_location = \\'s3://sql-case-studies/fresh_segments/interest_metrics/interest_metrics\\',\\n    write_compression = \\'SNAPPY\\',\\n    format = \\'PARQUET\\')\\nAS  \\nSELECT\\n    *\\nFROM\\n    fresh_segments.interest_metrics_modified', 'StatementType': 'DDL', 'ResultConfiguration': {'OutputLocation': 's3://sql-case-studies/fresh_segments/interest_metrics/tables/cc381175-9e88-4338-8acc-44814a4465b5'}, 'ResultReuseConfiguration': {'ResultReuseByAgeConfiguration': {'Enabled': False}}, 'QueryExecutionContext': {'Database': 'fresh_segments'}, 'Status': {'State': 'SUCCEEDED', 'SubmissionDateTime': datetime.datetime(2025, 1, 20, 15, 8, 54, 384000, tzinfo=tzlocal()), 'CompletionDateTime': datetime.datetime(2025, 1, 20, 15, 8, 55, 549000, tzinfo=tzlocal())}, 'Statistics': {'EngineExecutionTimeInMillis': 988, 'DataScannedInBytes': 85403, 'DataManifestLocation': 's3://sql-case-studies/fresh_segments/interest_metrics/tables/cc381175-9e88-4338-8acc-44814a4465b5-manifest.csv', 'TotalExecutionTimeInMillis': 1165, 'QueryQueueTimeInMillis': 57, 'ServicePreProcessingTimeInMillis': 105, 'QueryPlanningTimeInMillis': 131, 'ServiceProcessingTimeInMillis': 15, 'ResultReuseInformation': {'ReusedPreviousResult': False}}, 'WorkGroup': 'primary', 'EngineVersion': {'SelectedEngineVersion': 'AUTO', 'EffectiveEngineVersion': 'Athena engine version 3'}, 'SubstatementType': 'CREATE_TABLE_AS_SELECT'})}</pre> <pre>Query executed successfully\n</pre> In\u00a0[10]: Copied! <pre>query = \"\"\" \nSELECT\n    * \nFROM\n    fresh_segments.interest_metrics TABLESAMPLE BERNOULLI(30);\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  SELECT     *  FROM     fresh_segments.interest_metrics TABLESAMPLE BERNOULLI(30); \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[10]: record_month record_year month_year interest_id composition index_value ranking percentile_ranking 0 7 2018 2018-07-01 6344 10.32 5.10 4 99.45 1 7 2018 2018-07-01 69 10.82 5.03 6 99.18 2 7 2018 2018-07-01 17 7.89 4.15 19 97.39 3 7 2018 2018-07-01 6286 14.10 3.82 25 96.57 4 7 2018 2018-07-01 6184 13.35 3.75 29 96.02 ... ... ... ... ... ... ... ... ... 3925 8 2019 2019-08-01 33957 1.74 0.93 1130 1.65 3926 8 2019 2019-08-01 136 1.74 0.93 1130 1.65 3927 8 2019 2019-08-01 16198 1.82 0.90 1139 0.87 3928 8 2019 2019-08-01 15884 1.68 0.86 1144 0.44 3929 8 2019 2019-08-01 6225 2.15 0.86 1144 0.44 <p>3930 rows \u00d7 8 columns</p> In\u00a0[11]: Copied! <pre>query = \"\"\" \nSELECT\n    COUNT(*) AS record_count\nFROM\n    fresh_segments.interest_metrics;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  SELECT     COUNT(*) AS record_count FROM     fresh_segments.interest_metrics; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[11]: record_count 0 13079 In\u00a0[12]: Copied! <pre>query = \"\"\" \nSELECT\n    month_year,\n    COUNT(DISTINCT interest_id) AS unique_interest_count\nFROM\n    fresh_segments.interest_metrics\nGROUP BY\n    month_year\nORDER BY\n    month_year ASC NULLS FIRST;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  SELECT     month_year,     COUNT(DISTINCT interest_id) AS unique_interest_count FROM     fresh_segments.interest_metrics GROUP BY     month_year ORDER BY     month_year ASC NULLS FIRST; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[12]: month_year unique_interest_count 0 2018-07-01 729 1 2018-08-01 767 2 2018-09-01 780 3 2018-10-01 857 4 2018-11-01 928 5 2018-12-01 995 6 2019-01-01 973 7 2019-02-01 1121 8 2019-03-01 1136 9 2019-04-01 1099 10 2019-05-01 857 11 2019-06-01 824 12 2019-07-01 864 13 2019-08-01 1149 In\u00a0[13]: Copied! <pre>query = \"\"\" \nSELECT\n    COUNT(DISTINCT met.interest_id) AS metrics_id_unique_count,\n    COUNT(DISTINCT map.id) AS map_id_unique_count,\n    SUM(CASE WHEN met.interest_id IS NULL AND map.id IS NOT NULL THEN 1 ELSE 0 END) AS in_map_not_in_metrics_count,\n    SUM(CASE WHEN met.interest_id IS NOT NULL AND map.id IS NULL THEN 1 ELSE 0 END) AS in_metrics_not_in_map_count\nFROM\n    fresh_segments.interest_metrics AS met \n        FULL JOIN fresh_segments.interest_map AS map ON met.interest_id = map.id;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  SELECT     COUNT(DISTINCT met.interest_id) AS metrics_id_unique_count,     COUNT(DISTINCT map.id) AS map_id_unique_count,     SUM(CASE WHEN met.interest_id IS NULL AND map.id IS NOT NULL THEN 1 ELSE 0 END) AS in_map_not_in_metrics_count,     SUM(CASE WHEN met.interest_id IS NOT NULL AND map.id IS NULL THEN 1 ELSE 0 END) AS in_metrics_not_in_map_count FROM     fresh_segments.interest_metrics AS met          FULL JOIN fresh_segments.interest_map AS map ON met.interest_id = map.id; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[13]: metrics_id_unique_count map_id_unique_count in_map_not_in_metrics_count in_metrics_not_in_map_count 0 1202 1209 7 0 <p>All IDs in <code>interest_metrics</code> are also in <code>interest_map</code>: This means that there are no interest IDs in <code>interest_metrics</code> that are missing from <code>interest_map</code>. In set notation:</p> <ul> <li>interest_metrics \u2216 interest_map = \u2205 (empty set)</li> </ul> <p>Some IDs in <code>interest_map</code> are not in <code>interest_metrics</code>: There are 7 interest IDs that are found in <code>interest_map</code> but do not appear in <code>interest_metrics</code>. In set notation:</p> <ul> <li>interest_map \u2216 interest_metrics = {7 IDs}`</li> </ul> In\u00a0[16]: Copied! <pre>query = \"\"\" \nSELECT\n    COUNT(id) AS raw_count\nFROM\n    fresh_segments.interest_map;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  SELECT     COUNT(id) AS raw_count FROM     fresh_segments.interest_map; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[16]: raw_count 0 1209 In\u00a0[17]: Copied! <pre>query = \"\"\" \nSELECT\n    met.interest_id,\n    met.record_month,\n    met.record_year,\n    met.month_year,\n    met.composition,\n    met.index_value,\n    met.ranking,\n    met.percentile_ranking,\n    map.interest_name,\n    map.interest_summary,\n    map.created_at,\n    map.last_modified\nFROM\n    fresh_segments.interest_metrics AS met \n        LEFT JOIN fresh_segments.interest_map AS map ON met.interest_id = map.id\nWHERE\n    met.interest_id = 21246;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  SELECT     met.interest_id,     met.record_month,     met.record_year,     met.month_year,     met.composition,     met.index_value,     met.ranking,     met.percentile_ranking,     map.interest_name,     map.interest_summary,     map.created_at,     map.last_modified FROM     fresh_segments.interest_metrics AS met          LEFT JOIN fresh_segments.interest_map AS map ON met.interest_id = map.id WHERE     met.interest_id = 21246; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[17]: interest_id record_month record_year month_year composition index_value ranking percentile_ranking interest_name interest_summary created_at last_modified 0 21246 7 2018 2018-07-01 2.26 0.65 722 0.96 Readers of El Salvadoran Content People reading news from El Salvadoran media s... 2018-06-11 17:50:04 2018-06-11 17:50:04 1 21246 8 2018 2018-08-01 2.13 0.59 765 0.26 Readers of El Salvadoran Content People reading news from El Salvadoran media s... 2018-06-11 17:50:04 2018-06-11 17:50:04 2 21246 9 2018 2018-09-01 2.06 0.61 774 0.77 Readers of El Salvadoran Content People reading news from El Salvadoran media s... 2018-06-11 17:50:04 2018-06-11 17:50:04 3 21246 10 2018 2018-10-01 1.74 0.58 855 0.23 Readers of El Salvadoran Content People reading news from El Salvadoran media s... 2018-06-11 17:50:04 2018-06-11 17:50:04 4 21246 11 2018 2018-11-01 2.25 0.78 908 2.16 Readers of El Salvadoran Content People reading news from El Salvadoran media s... 2018-06-11 17:50:04 2018-06-11 17:50:04 5 21246 12 2018 2018-12-01 1.97 0.70 983 1.21 Readers of El Salvadoran Content People reading news from El Salvadoran media s... 2018-06-11 17:50:04 2018-06-11 17:50:04 6 21246 1 2019 2019-01-01 2.05 0.76 954 1.95 Readers of El Salvadoran Content People reading news from El Salvadoran media s... 2018-06-11 17:50:04 2018-06-11 17:50:04 7 21246 2 2019 2019-02-01 1.84 0.68 1109 1.07 Readers of El Salvadoran Content People reading news from El Salvadoran media s... 2018-06-11 17:50:04 2018-06-11 17:50:04 8 21246 3 2019 2019-03-01 1.75 0.67 1123 1.14 Readers of El Salvadoran Content People reading news from El Salvadoran media s... 2018-06-11 17:50:04 2018-06-11 17:50:04 9 21246 4 2019 2019-04-01 1.58 0.63 1092 0.64 Readers of El Salvadoran Content People reading news from El Salvadoran media s... 2018-06-11 17:50:04 2018-06-11 17:50:04 In\u00a0[67]: Copied! <pre>query = \"\"\" \nSELECT\n    met.interest_id,\n    met.record_month,\n    met.record_year,\n    met.month_year,\n    met.composition,\n    met.index_value,\n    met.ranking,\n    met.percentile_ranking,\n    map.interest_name,\n    map.interest_summary,\n    map.created_at,\n    map.last_modified\nFROM\n    fresh_segments.interest_metrics AS met \n        LEFT JOIN fresh_segments.interest_map AS map ON met.interest_id = map.id\nWHERE\n    met.month_year &lt; map.created_at;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  SELECT     met.interest_id,     met.record_month,     met.record_year,     met.month_year,     met.composition,     met.index_value,     met.ranking,     met.percentile_ranking,     map.interest_name,     map.interest_summary,     map.created_at,     map.last_modified FROM     fresh_segments.interest_metrics AS met          LEFT JOIN fresh_segments.interest_map AS map ON met.interest_id = map.id WHERE     met.month_year &lt; map.created_at; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[67]: interest_id record_month record_year month_year composition index_value ranking percentile_ranking interest_name interest_summary created_at last_modified 0 32704 7 2018 2018-07-01 8.04 2.27 225 69.14 Major Airline Customers People visiting sites for major airline brands... 2018-07-06 14:35:04 2018-07-06 14:35:04 1 33191 7 2018 2018-07-01 3.99 2.11 283 61.18 Online Shoppers People who spend money online 2018-07-17 10:40:03 2018-07-17 10:46:58 2 32703 7 2018 2018-07-01 5.53 1.80 375 48.56 School Supply Shoppers Consumers shopping for classroom supplies for ... 2018-07-06 14:35:04 2018-07-06 14:35:04 3 32701 7 2018 2018-07-01 4.23 1.41 483 33.74 Womens Equality Advocates People visiting sites advocating for womens eq... 2018-07-06 14:35:03 2018-07-06 14:35:03 4 32705 7 2018 2018-07-01 4.38 1.34 505 30.73 Certified Events Professionals Professionals reading industry news and resear... 2018-07-06 14:35:04 2018-07-06 14:35:04 ... ... ... ... ... ... ... ... ... ... ... ... ... 183 49972 4 2019 2019-04-01 2.11 1.15 722 34.30 Horseback Riding Enthusiasts People reading horseback riding news and resou... 2019-04-15 18:00:00 2019-04-24 17:40:04 184 49502 4 2019 2019-04-01 1.87 1.12 768 30.12 Veterinarians Veterinarians 2019-04-08 18:00:05 2019-07-09 13:57:13 185 49974 4 2019 2019-04-01 2.00 1.10 799 27.30 Agricultural and Food Issues Researchers People researching organizations for food and ... 2019-04-15 18:00:00 2019-04-24 17:40:04 186 49973 4 2019 2019-04-01 1.66 1.03 910 17.20 Farm Finance Researchers People researching financial institutions spec... 2019-04-15 18:00:00 2019-04-24 17:40:04 187 51678 5 2019 2019-05-01 1.71 1.53 475 44.57 Plumbers Professionals reading industry news and resear... 2019-05-06 22:00:00 2019-05-07 18:50:04 <p>188 rows \u00d7 12 columns</p> <p>Some <code>month_year</code> values are before the <code>created_at</code> values because the former is less granular than the latter. The <code>month_year</code> value is the month and year of the interest metrics, while the <code>created_at</code> value is the timestamp of when the interest was created. We also preprocessed the <code>month_year</code> to be the first day of the month, so it is expected that some <code>month_year</code> values will be before the <code>created_at</code> values.</p> <p>We can verify that there are no records where the difference between the <code>month_year</code> and <code>created_at</code> values is greater than 1 month, which would suggest that there are data quality issues.</p> In\u00a0[18]: Copied! <pre>query = \"\"\" \nSELECT\n    met.interest_id,\n    met.record_month,\n    met.record_year,\n    met.month_year,\n    met.composition,\n    met.index_value,\n    met.ranking,\n    met.percentile_ranking,\n    map.interest_name,\n    map.interest_summary,\n    map.created_at,\n    map.last_modified\nFROM\n    fresh_segments.interest_metrics AS met \n        LEFT JOIN fresh_segments.interest_map AS map ON met.interest_id = map.id\nWHERE\n    met.month_year &lt; map.created_at\n    AND DATE_DIFF('DAY', met.month_year, map.created_at) &gt; 30;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  SELECT     met.interest_id,     met.record_month,     met.record_year,     met.month_year,     met.composition,     met.index_value,     met.ranking,     met.percentile_ranking,     map.interest_name,     map.interest_summary,     map.created_at,     map.last_modified FROM     fresh_segments.interest_metrics AS met          LEFT JOIN fresh_segments.interest_map AS map ON met.interest_id = map.id WHERE     met.month_year &lt; map.created_at     AND DATE_DIFF('DAY', met.month_year, map.created_at) &gt; 30; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[18]: interest_id record_month record_year month_year composition index_value ranking percentile_ranking interest_name interest_summary created_at last_modified <p>Another way to verfiy is to use <code>DATE_TRUNC</code> function to truncate the <code>created_at</code> values to the month level (i.e., first day of each <code>created_at</code> month) and compare the <code>month_year</code> values with the truncated <code>created_at</code> values.</p> In\u00a0[19]: Copied! <pre>query = \"\"\" \nSELECT\n    met.interest_id,\n    met.record_month,\n    met.record_year,\n    met.month_year,\n    met.composition,\n    met.index_value,\n    met.ranking,\n    met.percentile_ranking,\n    map.interest_name,\n    map.interest_summary,\n    map.created_at,\n    map.last_modified\nFROM\n    fresh_segments.interest_metrics AS met \n        LEFT JOIN fresh_segments.interest_map AS map ON met.interest_id = map.id\nWHERE\n    met.month_year &lt; DATE_TRUNC('MONTH', map.created_at);\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  SELECT     met.interest_id,     met.record_month,     met.record_year,     met.month_year,     met.composition,     met.index_value,     met.ranking,     met.percentile_ranking,     map.interest_name,     map.interest_summary,     map.created_at,     map.last_modified FROM     fresh_segments.interest_metrics AS met          LEFT JOIN fresh_segments.interest_map AS map ON met.interest_id = map.id WHERE     met.month_year &lt; DATE_TRUNC('MONTH', map.created_at); \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[19]: interest_id record_month record_year month_year composition index_value ranking percentile_ranking interest_name interest_summary created_at last_modified In\u00a0[33]: Copied! <pre>query = \"\"\" \nSELECT\n    map.interest_name,\n    COUNT(DISTINCT met.month_year) AS total_monthS\nFROM\n    fresh_segments.interest_metrics AS met \n        LEFT JOIN fresh_segments.interest_map AS map ON met.interest_id = map.id\nGROUP BY\n    interest_name\nHAVING \n    COUNT(DISTINCT met.month_year) = (\n        SELECT COUNT(DISTINCT month_year)\n        FROM fresh_segments.interest_metrics\n    )\nORDER BY\n    interest_name ASC;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  SELECT     map.interest_name,     COUNT(DISTINCT met.month_year) AS total_monthS FROM     fresh_segments.interest_metrics AS met          LEFT JOIN fresh_segments.interest_map AS map ON met.interest_id = map.id GROUP BY     interest_name HAVING      COUNT(DISTINCT met.month_year) = (         SELECT COUNT(DISTINCT month_year)         FROM fresh_segments.interest_metrics     ) ORDER BY     interest_name ASC; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[33]: interest_name total_monthS 0 Accounting &amp; CPA Continuing Education Researchers 14 1 Affordable Hotel Bookers 14 2 Aftermarket Accessories Shoppers 14 3 Alabama Trip Planners 14 4 Alaskan Cruise Planners 14 ... ... ... 475 World Cup Enthusiasts 14 476 Yachting Enthusiasts 14 477 Yale University Fans 14 478 Yogis 14 479 Zoo Visitors 14 <p>480 rows \u00d7 2 columns</p> In\u00a0[38]: Copied! <pre>query = \"\"\" \nWITH count_month_year AS (\n    SELECT\n        interest_id,\n        COUNT(DISTINCT month_year) AS total_months\n    FROM\n        fresh_segments.interest_metrics\n    GROUP BY\n        interest_id\n)\nSELECT\n    total_months,\n    COUNT(DISTINCT interest_id) AS interest_counts\nFROM\n    count_month_year\nGROUP BY\n    total_months\nORDER BY\n    total_months DESC;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  WITH count_month_year AS (     SELECT         interest_id,         COUNT(DISTINCT month_year) AS total_months     FROM         fresh_segments.interest_metrics     GROUP BY         interest_id ) SELECT     total_months,     COUNT(DISTINCT interest_id) AS interest_counts FROM     count_month_year GROUP BY     total_months ORDER BY     total_months DESC; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[38]: total_months interest_counts 0 14 480 1 13 82 2 12 65 3 11 94 4 10 86 5 9 95 6 8 67 7 7 90 8 6 33 9 5 38 10 4 32 11 3 15 12 2 12 13 1 13 In\u00a0[51]: Copied! <pre>query = \"\"\" \nWITH count_month_year AS (\n    SELECT\n        interest_id,\n        COUNT(DISTINCT month_year) AS total_months\n    FROM\n        fresh_segments.interest_metrics\n    GROUP BY\n        interest_id\n),\n\ncount_interest_by_total_months AS (\n    SELECT\n        total_months,\n        COUNT(DISTINCT interest_id) AS interest_counts\n    FROM\n        count_month_year\n    GROUP BY\n        total_months\n)\n\nSELECT\n    total_months,\n    interest_counts,\n    ROUND(\n        100.0 * SUM(interest_counts) OVER (\n            ORDER BY total_months DESC \n            ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n        ) / SUM(interest_counts) OVER(),\n        2\n    ) AS cumulative_percentage\nFROM\n    count_interest_by_total_months\nORDER BY\n    total_months DESC;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  WITH count_month_year AS (     SELECT         interest_id,         COUNT(DISTINCT month_year) AS total_months     FROM         fresh_segments.interest_metrics     GROUP BY         interest_id ),  count_interest_by_total_months AS (     SELECT         total_months,         COUNT(DISTINCT interest_id) AS interest_counts     FROM         count_month_year     GROUP BY         total_months )  SELECT     total_months,     interest_counts,     ROUND(         100.0 * SUM(interest_counts) OVER (             ORDER BY total_months DESC              ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW         ) / SUM(interest_counts) OVER(),         2     ) AS cumulative_percentage FROM     count_interest_by_total_months ORDER BY     total_months DESC; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[51]: total_months interest_counts cumulative_percentage 0 14 480 39.93 1 13 82 46.76 2 12 65 52.16 3 11 94 59.98 4 10 86 67.14 5 9 95 75.04 6 8 67 80.62 7 7 90 88.10 8 6 33 90.85 9 5 38 94.01 10 4 32 96.67 11 3 15 97.92 12 2 12 98.92 13 1 13 100.00 <p>There are 90 distinct interests with data available for at least 6 of the total month_year values in the dataset. This corresponds to the point where the cumulative percentage surpasses $90\\%$.</p> <p>This indicates that the majority (i.e., $90\\%$) of the interests in the dataset have data coverage across fewer than 6 months, emphasizing a significant drop-off in the number of interests with longer-term data availability since the max <code>total_months</code> value is 14.</p> In\u00a0[66]: Copied! <pre>query = \"\"\" \nWITH interest_ids_to_remove AS (\n    SELECT\n        interest_id\n    FROM\n        fresh_segments.interest_metrics\n    GROUP BY\n        interest_id\n    HAVING \n        COUNT(DISTINCT month_year) &lt; 6\n)\n\nSELECT\n    COUNT(met.interest_id) AS remove_row_count\nFROM\n    fresh_segments.interest_metrics AS met \n        LEFT JOIN interest_ids_to_remove AS remove ON met.interest_id = remove.interest_id\nWHERE\n    1 = 1\n    AND remove.interest_id IS NOT NULL;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  WITH interest_ids_to_remove AS (     SELECT         interest_id     FROM         fresh_segments.interest_metrics     GROUP BY         interest_id     HAVING          COUNT(DISTINCT month_year) &lt; 6 )  SELECT     COUNT(met.interest_id) AS remove_row_count FROM     fresh_segments.interest_metrics AS met          LEFT JOIN interest_ids_to_remove AS remove ON met.interest_id = remove.interest_id WHERE     1 = 1     AND remove.interest_id IS NOT NULL; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[66]: remove_row_count 0 400 In\u00a0[71]: Copied! <pre>query = \"\"\" \nSELECT\n    month_year,\n    COUNT(DISTINCT interest_id) AS num_unique_interest\nFROM\n    fresh_segments.interest_metrics\nGROUP BY\n    month_year\nORDER BY\n    num_unique_interest DESC;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  SELECT     month_year,     COUNT(DISTINCT interest_id) AS num_unique_interest FROM     fresh_segments.interest_metrics GROUP BY     month_year ORDER BY     num_unique_interest DESC; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[71]: month_year num_unique_interest 0 2019-08-01 1149 1 2019-03-01 1136 2 2019-02-01 1121 3 2019-04-01 1099 4 2018-12-01 995 5 2019-01-01 973 6 2018-11-01 928 7 2019-07-01 864 8 2018-10-01 857 9 2019-05-01 857 10 2019-06-01 824 11 2018-09-01 780 12 2018-08-01 767 13 2018-07-01 729 <p>Metrics computed for interests with limited availability during the sampling period may lead to less reliable or, even worse, biased results. Furthermore, conducting statistical hypothesis testing on smaller sample sizes may reduce the statistical power, making it more challenging to detect true effects.</p> In\u00a0[32]: Copied! <pre>query = \"\"\" \nWITH compositions AS (\n    SELECT\n        map.interest_name,\n        met.month_year,\n        met.composition,\n        DENSE_RANK() OVER (\n            PARTITION BY map.interest_name\n            ORDER BY met.composition DESC\n        ) AS comp_ranks\n    FROM\n        fresh_segments.interest_metrics AS met\n    LEFT JOIN \n        fresh_segments.interest_map AS map \n    ON \n        met.interest_id = map.id\n),\n\ntop_10 AS (\n    SELECT\n        interest_name,\n        month_year,\n        composition,\n        'top_10' AS rank\n    FROM\n        compositions\n    WHERE\n        comp_ranks = 1\n    ORDER BY\n        composition DESC\n    LIMIT \n        10\n),\n\nbottom_10 AS (\n    SELECT\n        interest_name,\n        month_year,\n        composition,\n        'bottom_10' AS rank\n    FROM\n        compositions\n    WHERE\n        comp_ranks = 1\n    ORDER BY \n        composition ASC\n    LIMIT \n        10\n),\n\nrow_bind AS (\n    SELECT * FROM top_10\n    UNION\n    SELECT * FROM bottom_10\n)\nSELECT\n    *\nFROM\n    row_bind\nORDER BY \n    rank DESC,\n    composition DESC;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  WITH compositions AS (     SELECT         map.interest_name,         met.month_year,         met.composition,         DENSE_RANK() OVER (             PARTITION BY map.interest_name             ORDER BY met.composition DESC         ) AS comp_ranks     FROM         fresh_segments.interest_metrics AS met     LEFT JOIN          fresh_segments.interest_map AS map      ON          met.interest_id = map.id ),  top_10 AS (     SELECT         interest_name,         month_year,         composition,         'top_10' AS rank     FROM         compositions     WHERE         comp_ranks = 1     ORDER BY         composition DESC     LIMIT          10 ),  bottom_10 AS (     SELECT         interest_name,         month_year,         composition,         'bottom_10' AS rank     FROM         compositions     WHERE         comp_ranks = 1     ORDER BY          composition ASC     LIMIT          10 ),  row_bind AS (     SELECT * FROM top_10     UNION     SELECT * FROM bottom_10 ) SELECT     * FROM     row_bind ORDER BY      rank DESC,     composition DESC; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[32]: interest_name month_year composition rank 0 Work Comes First Travelers 2018-12-01 21.20 top_10 1 Gym Equipment Owners 2018-07-01 18.82 top_10 2 Furniture Shoppers 2018-07-01 17.44 top_10 3 Luxury Retail Shoppers 2018-07-01 17.19 top_10 4 Luxury Boutique Hotel Researchers 2018-10-01 15.15 top_10 5 Luxury Bedding Shoppers 2018-12-01 15.05 top_10 6 Shoe Shoppers 2018-07-01 14.91 top_10 7 Cosmetics and Beauty Shoppers 2018-07-01 14.23 top_10 8 Luxury Hotel Guests 2018-07-01 14.10 top_10 9 Luxury Retail Researchers 2018-07-01 13.97 top_10 10 Readers of Jamaican Content 2018-07-01 1.86 bottom_10 11 Automotive News Readers 2019-02-01 1.84 bottom_10 12 Comedy Fans 2018-07-01 1.83 bottom_10 13 World of Warcraft Enthusiasts 2019-08-01 1.82 bottom_10 14 Miami Heat Fans 2018-08-01 1.81 bottom_10 15 Online Role Playing Game Enthusiasts 2018-07-01 1.73 bottom_10 16 Hearthstone Video Game Fans 2019-08-01 1.66 bottom_10 17 Scifi Movie and TV Enthusiasts 2018-09-01 1.61 bottom_10 18 Action Movie and TV Enthusiasts 2018-09-01 1.59 bottom_10 19 The Sims Video Game Fans 2019-03-01 1.57 bottom_10 In\u00a0[52]: Copied! <pre>query = \"\"\" \nSELECT\n    map.interest_name,\n    AVG(met.ranking) AS avg_ranking,\n    COUNT(*) AS n\nFROM\n    fresh_segments.interest_metrics AS met \n        LEFT JOIN fresh_segments.interest_map AS map ON met.interest_id = map.id\nGROUP BY\n    map.interest_name\nORDER BY\n    avg_ranking DESC\nLIMIT\n    5;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  SELECT     map.interest_name,     AVG(met.ranking) AS avg_ranking,     COUNT(*) AS n FROM     fresh_segments.interest_metrics AS met          LEFT JOIN fresh_segments.interest_map AS map ON met.interest_id = map.id GROUP BY     map.interest_name ORDER BY     avg_ranking DESC LIMIT     5; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[52]: interest_name avg_ranking n 0 Hearthstone Video Game Fans 1141.0 1 1 The Sims Video Game Fans 1135.0 1 2 Grand Theft Auto Video Game Fans 1110.0 2 3 Hair Color Shoppers 1110.0 4 4 Bigfoot Folklore Enthusiasts 1078.0 2 In\u00a0[54]: Copied! <pre>query = \"\"\" \nSELECT\n    met.interest_id,\n    map.interest_name,\n    STDDEV(met.percentile_ranking) AS pct_ranking_std,\n    MIN(met.percentile_ranking) AS pct_ranking_min,\n    APPROX_PERCENTILE(met.percentile_ranking, 0.25) AS pct_ranking_25,\n    APPROX_PERCENTILE(met.percentile_ranking, 0.50) AS pct_ranking_50,\n    APPROX_PERCENTILE(met.percentile_ranking, 0.75) AS pct_ranking_75,\n    MAX(met.percentile_ranking) AS pct_ranking_max,\n    COUNT(*) AS n\nFROM\n    fresh_segments.interest_metrics AS met \n        LEFT JOIN fresh_segments.interest_map AS map ON met.interest_id = map.id\nGROUP BY\n    met.interest_id,\n    map.interest_name\nORDER BY\n    pct_ranking_std DESC\nLIMIT\n    5;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  SELECT     met.interest_id,     map.interest_name,     STDDEV(met.percentile_ranking) AS pct_ranking_std,     MIN(met.percentile_ranking) AS pct_ranking_min,     APPROX_PERCENTILE(met.percentile_ranking, 0.25) AS pct_ranking_25,     APPROX_PERCENTILE(met.percentile_ranking, 0.50) AS pct_ranking_50,     APPROX_PERCENTILE(met.percentile_ranking, 0.75) AS pct_ranking_75,     MAX(met.percentile_ranking) AS pct_ranking_max,     COUNT(*) AS n FROM     fresh_segments.interest_metrics AS met          LEFT JOIN fresh_segments.interest_map AS map ON met.interest_id = map.id GROUP BY     met.interest_id,     map.interest_name ORDER BY     pct_ranking_std DESC LIMIT     5; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[54]: interest_id interest_name pct_ranking_std pct_ranking_min pct_ranking_25 pct_ranking_50 pct_ranking_75 pct_ranking_max n 0 6260 Blockbuster Movie Fans 41.273823 2.26 2.26 60.63 60.63 60.63 2 1 131 Android Fans 30.720768 4.84 4.96 5.62 10.82 75.03 5 2 150 TV Junkies 30.363975 10.01 37.29 37.79 49.82 93.28 5 3 23 Techies 30.175047 7.92 9.46 23.85 30.90 86.69 6 4 20764 Entertainment Industry Decision Makers 28.974920 11.23 11.53 18.67 22.12 86.15 6 In\u00a0[72]: Copied! <pre>query = \"\"\" \nWITH ranked_data AS (\n    SELECT\n        interest_id,\n        month_year,\n        composition,\n        ranking,\n        percentile_ranking,\n        DENSE_RANK() OVER (PARTITION BY interest_id ORDER BY percentile_ranking DESC) AS percentile_ranking_max,\n        DENSE_RANK() OVER (PARTITION BY interest_id ORDER BY percentile_ranking ASC) AS percentile_ranking_min\n    FROM\n        fresh_segments.interest_metrics\n    WHERE\n        1 = 1\n        AND interest_id IN (\n            SELECT\n                interest_id\n            FROM\n                fresh_segments.interest_metrics\n            GROUP BY\n                interest_id\n            ORDER BY\n                STDDEV(percentile_ranking) DESC\n            LIMIT\n                5\n        )\n)\nSELECT\n    interest_id,\n    month_year,\n    composition,\n    ranking,\n    percentile_ranking\nFROM\n    ranked_data\nWHERE\n    1 = 1\n    AND (\n        percentile_ranking_max = 1\n        OR percentile_ranking_min = 1\n    );\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  WITH ranked_data AS (     SELECT         interest_id,         month_year,         composition,         ranking,         percentile_ranking,         DENSE_RANK() OVER (PARTITION BY interest_id ORDER BY percentile_ranking DESC) AS percentile_ranking_max,         DENSE_RANK() OVER (PARTITION BY interest_id ORDER BY percentile_ranking ASC) AS percentile_ranking_min     FROM         fresh_segments.interest_metrics     WHERE         1 = 1         AND interest_id IN (             SELECT                 interest_id             FROM                 fresh_segments.interest_metrics             GROUP BY                 interest_id             ORDER BY                 STDDEV(percentile_ranking) DESC             LIMIT                 5         ) ) SELECT     interest_id,     month_year,     composition,     ranking,     percentile_ranking FROM     ranked_data WHERE     1 = 1     AND (         percentile_ranking_max = 1         OR percentile_ranking_min = 1     ); \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[72]: interest_id month_year composition ranking percentile_ranking 0 6260 2018-07-01 5.27 287 60.63 1 6260 2019-08-01 1.83 1123 2.26 2 131 2018-07-01 5.09 182 75.03 3 131 2019-03-01 1.72 1081 4.84 4 150 2018-07-01 5.30 49 93.28 5 150 2019-08-01 1.94 1034 10.01 6 23 2018-07-01 5.41 97 86.69 7 23 2019-08-01 1.90 1058 7.92 8 20764 2018-07-01 5.85 101 86.15 9 20764 2019-08-01 1.91 1020 11.23 <p>One clear pattern emerges: the maximum percentile ranking values for these five interests all occur in July 2018, while the minimum percentile values, with one exception, are observed in August 2019.</p> In\u00a0[18]: Copied! <pre>query = \"\"\" \nWITH compositions AS (\n    SELECT\n        map.interest_name,\n        composition,\n        DENSE_RANK() OVER (\n            PARTITION BY map.interest_name\n            ORDER BY met.composition DESC\n        ) AS comp_ranks\n    FROM\n        fresh_segments.interest_metrics AS met\n    LEFT JOIN \n        fresh_segments.interest_map AS map \n    ON \n        met.interest_id = map.id\n),\n\ntop_10 AS (\n    SELECT\n        interest_name,\n        'top_10' AS rank\n    FROM\n        compositions\n    WHERE\n        comp_ranks = 1\n    ORDER BY\n        composition DESC\n    LIMIT \n        10\n),\n\nbottom_10 AS (\n    SELECT\n        interest_name,\n        'bottom_10' AS rank\n    FROM\n        compositions\n    WHERE\n        comp_ranks = 1\n    ORDER BY \n        composition ASC\n    LIMIT \n        10\n),\n\nrow_bind AS (\n    SELECT * FROM top_10\n    UNION\n    SELECT * FROM bottom_10\n)\nSELECT\n    map.interest_name,\n    met.month_year,\n    met.composition,\n    met.ranking,\n    rank\nFROM\n    fresh_segments.interest_metrics AS met \n        LEFT JOIN fresh_segments.interest_map AS map ON met.interest_id = map.id\n        LEFT JOIN row_bind AS rb ON map.interest_name = rb.interest_name\nWHERE\n    1 = 1\n    AND rb.interest_name IS NOT NULL\nORDER BY\n    rank DESC,\n    interest_name ASC;\n\"\"\"\n\ntop_bottom_10 = athena.query(\n    database=database, query=query, ctas_approach=ctas_approach\n)\ntop_bottom_10\n</pre> query = \"\"\"  WITH compositions AS (     SELECT         map.interest_name,         composition,         DENSE_RANK() OVER (             PARTITION BY map.interest_name             ORDER BY met.composition DESC         ) AS comp_ranks     FROM         fresh_segments.interest_metrics AS met     LEFT JOIN          fresh_segments.interest_map AS map      ON          met.interest_id = map.id ),  top_10 AS (     SELECT         interest_name,         'top_10' AS rank     FROM         compositions     WHERE         comp_ranks = 1     ORDER BY         composition DESC     LIMIT          10 ),  bottom_10 AS (     SELECT         interest_name,         'bottom_10' AS rank     FROM         compositions     WHERE         comp_ranks = 1     ORDER BY          composition ASC     LIMIT          10 ),  row_bind AS (     SELECT * FROM top_10     UNION     SELECT * FROM bottom_10 ) SELECT     map.interest_name,     met.month_year,     met.composition,     met.ranking,     rank FROM     fresh_segments.interest_metrics AS met          LEFT JOIN fresh_segments.interest_map AS map ON met.interest_id = map.id         LEFT JOIN row_bind AS rb ON map.interest_name = rb.interest_name WHERE     1 = 1     AND rb.interest_name IS NOT NULL ORDER BY     rank DESC,     interest_name ASC; \"\"\"  top_bottom_10 = athena.query(     database=database, query=query, ctas_approach=ctas_approach ) top_bottom_10 Out[18]: interest_name month_year composition ranking rank 0 Cosmetics and Beauty Shoppers 2019-04-01 5.10 695 top_10 1 Cosmetics and Beauty Shoppers 2019-02-01 6.50 584 top_10 2 Cosmetics and Beauty Shoppers 2019-03-01 5.93 619 top_10 3 Cosmetics and Beauty Shoppers 2018-08-01 7.98 389 top_10 4 Cosmetics and Beauty Shoppers 2018-12-01 6.73 487 top_10 ... ... ... ... ... ... 147 World of Warcraft Enthusiasts 2018-11-01 1.63 902 bottom_10 148 World of Warcraft Enthusiasts 2019-08-01 1.82 1139 bottom_10 149 World of Warcraft Enthusiasts 2019-03-01 1.52 1130 bottom_10 150 World of Warcraft Enthusiasts 2019-04-01 1.58 1076 bottom_10 151 World of Warcraft Enthusiasts 2018-07-01 1.74 712 bottom_10 <p>152 rows \u00d7 5 columns</p> In\u00a0[19]: Copied! <pre>fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n\ntop_10_data = top_bottom_10.loc[top_bottom_10[\"rank\"] == \"top_10\"].sort_values(\n    by=\"month_year\"\n)\nbottom_10_data = top_bottom_10.loc[top_bottom_10[\"rank\"] == \"bottom_10\"].sort_values(\n    by=\"month_year\"\n)\n\n# Top 10 Interests\nfor name, group in top_10_data.groupby(\"interest_name\"):\n    axes[0].plot(group[\"month_year\"], group[\"composition\"], label=name, marker=\"o\")\naxes[0].set_title(\"Top 10 Interests Over Time\")\naxes[0].set_xlabel(\"Month-Year\")\naxes[0].set_ylabel(\"Composition (%)\")\naxes[0].legend(title=\"Interest Name\", fontsize=8)\naxes[0].grid(True)\n\n# Bottom 10 Interests\nfor name, group in bottom_10_data.groupby(\"interest_name\"):\n    axes[1].plot(group[\"month_year\"], group[\"composition\"], label=name, marker=\"o\")\naxes[1].set_title(\"Bottom 10 Interests Over Time\")\naxes[1].set_xlabel(\"Month-Year\")\naxes[1].legend(title=\"Interest Name\", fontsize=8)\naxes[1].grid(True)\n\nplt.tight_layout()\nplt.show();\n</pre> fig, axes = plt.subplots(1, 2, figsize=(16, 8))  top_10_data = top_bottom_10.loc[top_bottom_10[\"rank\"] == \"top_10\"].sort_values(     by=\"month_year\" ) bottom_10_data = top_bottom_10.loc[top_bottom_10[\"rank\"] == \"bottom_10\"].sort_values(     by=\"month_year\" )  # Top 10 Interests for name, group in top_10_data.groupby(\"interest_name\"):     axes[0].plot(group[\"month_year\"], group[\"composition\"], label=name, marker=\"o\") axes[0].set_title(\"Top 10 Interests Over Time\") axes[0].set_xlabel(\"Month-Year\") axes[0].set_ylabel(\"Composition (%)\") axes[0].legend(title=\"Interest Name\", fontsize=8) axes[0].grid(True)  # Bottom 10 Interests for name, group in bottom_10_data.groupby(\"interest_name\"):     axes[1].plot(group[\"month_year\"], group[\"composition\"], label=name, marker=\"o\") axes[1].set_title(\"Bottom 10 Interests Over Time\") axes[1].set_xlabel(\"Month-Year\") axes[1].legend(title=\"Interest Name\", fontsize=8) axes[1].grid(True)  plt.tight_layout() plt.show(); In\u00a0[25]: Copied! <pre>def define_priors(data: pd.Series) -&gt; Dict[str, float]:\n    \"\"\"\n    Define priors based on the provided data.\n\n    - A Normal prior is used for the mean, centered around the observed mean of the data\n    - A flexible Normal prior allows for uncertainty, with its standard deviation set to twice the observed standard deviation of the data\n    - An Inverse Gamma prior is used for the variance. This is a common choice for modeling uncertainty in scale parameters like variance\n      or standard deviation. The shape (alpha) is set to 2, and the scale (beta) is informed by the data's variance (`data.var()`).\n\n    Parameters\n    ----------\n    data : pd.Series\n        The data series to calculate priors from.\n\n    Returns\n    -------\n    Dict[str, float]\n        A dictionary containing the calculated priors:\n        - mu_prior_mean: Prior mean for mu.\n        - mu_prior_std: Prior standard deviation for mu.\n        - sigma_prior_alpha: Alpha parameter for the InverseGamma prior on sigma.\n        - sigma_prior_beta: Beta parameter for the InverseGamma prior on sigma.\n    \"\"\"\n    mu_prior_mean = data.mean()\n    mu_prior_std = data.std() * 2\n    sigma_prior_alpha = 2\n    sigma_prior_beta = data.var()\n    return {\n        \"mu_prior_mean\": mu_prior_mean,\n        \"mu_prior_std\": mu_prior_std,\n        \"sigma_prior_alpha\": sigma_prior_alpha,\n        \"sigma_prior_beta\": sigma_prior_beta,\n    }\n\n\ndef run_bayesian_model(\n    data: pd.Series,\n    priors: Dict[str, float],\n    samples: Optional[int] = 2000,\n    tune: Optional[int] = 1000,\n    hdi_prob: Optional[float] = 0.95,\n) -&gt; Tuple[np.ndarray, np.ndarray, Dict[str, Tuple[float, float]]]:\n    \"\"\"\n    Run Bayesian inference on the data and return posterior samples.\n\n    Parameters\n    ----------\n    data : pd.Series\n        The observed data series.\n    priors : dict\n        A dictionary containing the priors for the model.\n    samples : Optional[int]\n        The number of posterior samples to draw, by default 2000.\n    tune : Optional[int]\n        The number of tuning steps for the sampler, by default 1000.\n    hdi_prob: Optional[float]\n        Prob for which the highest density interval will be computed, by default 0.95.\n\n\n    Returns\n    -------\n    Tuple[np.ndarray, np.ndarray, Dict[str, Tuple[float, float]]]\n        - posterior_mu: Posterior samples for the mean (mu).\n        - posterior_sigma: Posterior samples for the standard deviation (sigma).\n        - hdi_mu: HDI for the posterior mean.\n    \"\"\"\n    with pm.Model() as model:\n        # Priors\n        mu = pm.Normal(\"mu\", mu=priors[\"mu_prior_mean\"], sigma=priors[\"mu_prior_std\"])\n        sigma = pm.InverseGamma(\n            \"sigma\", alpha=priors[\"sigma_prior_alpha\"], beta=priors[\"sigma_prior_beta\"]\n        )\n\n        # Likelihood\n        composition_obs = pm.Normal(\n            \"composition_obs\", mu=mu, sigma=sigma, observed=data\n        )\n\n        # Posterior Sampling\n        trace = pm.sample(\n            samples, tune=tune, return_inferencedata=True, progressbar=False\n        )\n\n        # Calculate HDI for mu\n        hdi_mu = pm.hdi(trace.posterior[\"mu\"], hdi_prob=hdi_prob)\n\n    posterior_mu = trace.posterior[\"mu\"].values.flatten()\n    posterior_sigma = trace.posterior[\"sigma\"].values.flatten()\n    return posterior_mu, posterior_sigma, hdi_mu\n\n\ndef compute_risk_score(posterior_mu: np.ndarray) -&gt; float:\n    \"\"\"\n    Calculate the risk score as the standard deviation of the posterior mean.\n\n    Parameters\n    ----------\n    posterior_mu : np.ndarray\n        The posterior samples for the mean (mu).\n\n    Returns\n    -------\n    float\n        The calculated risk score.\n    \"\"\"\n    return np.std(posterior_mu)\n</pre> def define_priors(data: pd.Series) -&gt; Dict[str, float]:     \"\"\"     Define priors based on the provided data.      - A Normal prior is used for the mean, centered around the observed mean of the data     - A flexible Normal prior allows for uncertainty, with its standard deviation set to twice the observed standard deviation of the data     - An Inverse Gamma prior is used for the variance. This is a common choice for modeling uncertainty in scale parameters like variance       or standard deviation. The shape (alpha) is set to 2, and the scale (beta) is informed by the data's variance (`data.var()`).      Parameters     ----------     data : pd.Series         The data series to calculate priors from.      Returns     -------     Dict[str, float]         A dictionary containing the calculated priors:         - mu_prior_mean: Prior mean for mu.         - mu_prior_std: Prior standard deviation for mu.         - sigma_prior_alpha: Alpha parameter for the InverseGamma prior on sigma.         - sigma_prior_beta: Beta parameter for the InverseGamma prior on sigma.     \"\"\"     mu_prior_mean = data.mean()     mu_prior_std = data.std() * 2     sigma_prior_alpha = 2     sigma_prior_beta = data.var()     return {         \"mu_prior_mean\": mu_prior_mean,         \"mu_prior_std\": mu_prior_std,         \"sigma_prior_alpha\": sigma_prior_alpha,         \"sigma_prior_beta\": sigma_prior_beta,     }   def run_bayesian_model(     data: pd.Series,     priors: Dict[str, float],     samples: Optional[int] = 2000,     tune: Optional[int] = 1000,     hdi_prob: Optional[float] = 0.95, ) -&gt; Tuple[np.ndarray, np.ndarray, Dict[str, Tuple[float, float]]]:     \"\"\"     Run Bayesian inference on the data and return posterior samples.      Parameters     ----------     data : pd.Series         The observed data series.     priors : dict         A dictionary containing the priors for the model.     samples : Optional[int]         The number of posterior samples to draw, by default 2000.     tune : Optional[int]         The number of tuning steps for the sampler, by default 1000.     hdi_prob: Optional[float]         Prob for which the highest density interval will be computed, by default 0.95.       Returns     -------     Tuple[np.ndarray, np.ndarray, Dict[str, Tuple[float, float]]]         - posterior_mu: Posterior samples for the mean (mu).         - posterior_sigma: Posterior samples for the standard deviation (sigma).         - hdi_mu: HDI for the posterior mean.     \"\"\"     with pm.Model() as model:         # Priors         mu = pm.Normal(\"mu\", mu=priors[\"mu_prior_mean\"], sigma=priors[\"mu_prior_std\"])         sigma = pm.InverseGamma(             \"sigma\", alpha=priors[\"sigma_prior_alpha\"], beta=priors[\"sigma_prior_beta\"]         )          # Likelihood         composition_obs = pm.Normal(             \"composition_obs\", mu=mu, sigma=sigma, observed=data         )          # Posterior Sampling         trace = pm.sample(             samples, tune=tune, return_inferencedata=True, progressbar=False         )          # Calculate HDI for mu         hdi_mu = pm.hdi(trace.posterior[\"mu\"], hdi_prob=hdi_prob)      posterior_mu = trace.posterior[\"mu\"].values.flatten()     posterior_sigma = trace.posterior[\"sigma\"].values.flatten()     return posterior_mu, posterior_sigma, hdi_mu   def compute_risk_score(posterior_mu: np.ndarray) -&gt; float:     \"\"\"     Calculate the risk score as the standard deviation of the posterior mean.      Parameters     ----------     posterior_mu : np.ndarray         The posterior samples for the mean (mu).      Returns     -------     float         The calculated risk score.     \"\"\"     return np.std(posterior_mu) In\u00a0[27]: Copied! <pre>risk_scores = []\n\nfor interest_name in top_10_data[\"interest_name\"].unique():\n    # Filter data for the current interest\n    interest_data = top_10_data[top_10_data[\"interest_name\"] == interest_name][\n        \"composition\"\n    ]\n\n    # Define priors\n    priors = define_priors(interest_data)\n\n    # Run Bayesian model and get HDI\n    posterior_mu, _, hdi_mu = run_bayesian_model(interest_data, priors, hdi_prob=0.97)\n\n    # Compute risk score as the standard deviations of the posterior means\n    risk_score = compute_risk_score(posterior_mu)\n\n    risk_scores.append(\n        {\n            \"interest_name\": interest_name,\n            \"risk_score\": risk_score,\n            \"hdi_lower_mean_composition\": hdi_mu[\"mu\"].values[0],\n            \"hdi_upper_mean_composition\": hdi_mu[\"mu\"].values[1],\n        }\n    )\n\nrisk_scores_data = pd.DataFrame(risk_scores).sort_values(\"risk_score\", ascending=False)\nrisk_scores_data\n</pre> risk_scores = []  for interest_name in top_10_data[\"interest_name\"].unique():     # Filter data for the current interest     interest_data = top_10_data[top_10_data[\"interest_name\"] == interest_name][         \"composition\"     ]      # Define priors     priors = define_priors(interest_data)      # Run Bayesian model and get HDI     posterior_mu, _, hdi_mu = run_bayesian_model(interest_data, priors, hdi_prob=0.97)      # Compute risk score as the standard deviations of the posterior means     risk_score = compute_risk_score(posterior_mu)      risk_scores.append(         {             \"interest_name\": interest_name,             \"risk_score\": risk_score,             \"hdi_lower_mean_composition\": hdi_mu[\"mu\"].values[0],             \"hdi_upper_mean_composition\": hdi_mu[\"mu\"].values[1],         }     )  risk_scores_data = pd.DataFrame(risk_scores).sort_values(\"risk_score\", ascending=False) risk_scores_data <pre>Auto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [mu, sigma]\nSampling 4 chains for 1_000 tune and 2_000 draw iterations (4_000 + 8_000 draws total) took 1 seconds.\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [mu, sigma]\nSampling 4 chains for 1_000 tune and 2_000 draw iterations (4_000 + 8_000 draws total) took 1 seconds.\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [mu, sigma]\nSampling 4 chains for 1_000 tune and 2_000 draw iterations (4_000 + 8_000 draws total) took 1 seconds.\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [mu, sigma]\nSampling 4 chains for 1_000 tune and 2_000 draw iterations (4_000 + 8_000 draws total) took 1 seconds.\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [mu, sigma]\nSampling 4 chains for 1_000 tune and 2_000 draw iterations (4_000 + 8_000 draws total) took 1 seconds.\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [mu, sigma]\nSampling 4 chains for 1_000 tune and 2_000 draw iterations (4_000 + 8_000 draws total) took 1 seconds.\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [mu, sigma]\nSampling 4 chains for 1_000 tune and 2_000 draw iterations (4_000 + 8_000 draws total) took 1 seconds.\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [mu, sigma]\nSampling 4 chains for 1_000 tune and 2_000 draw iterations (4_000 + 8_000 draws total) took 1 seconds.\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [mu, sigma]\nSampling 4 chains for 1_000 tune and 2_000 draw iterations (4_000 + 8_000 draws total) took 1 seconds.\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [mu, sigma]\nSampling 4 chains for 1_000 tune and 2_000 draw iterations (4_000 + 8_000 draws total) took 1 seconds.\n</pre> Out[27]: interest_name risk_score hdi_lower_mean_composition hdi_upper_mean_composition 7 Work Comes First Travelers 1.645555 13.992587 21.492166 9 Luxury Bedding Shoppers 1.208149 8.508197 13.899054 2 Luxury Boutique Hotel Researchers 1.100884 8.039092 13.009123 5 Luxury Hotel Guests 1.001667 7.134219 11.614973 3 Gym Equipment Owners 0.944078 8.352871 12.541372 1 Luxury Retail Shoppers 0.915064 7.708057 11.805010 6 Furniture Shoppers 0.847117 7.631271 11.361080 4 Shoe Shoppers 0.811842 4.634176 8.229593 8 Cosmetics and Beauty Shoppers 0.790816 4.579328 8.025522 0 Luxury Retail Researchers 0.741084 4.474628 7.776604 In\u00a0[74]: Copied! <pre>query = \"\"\" \nWITH\n    ranked_avg_comp AS (\n        SELECT\n            met.month_year,\n            map.interest_name,\n            ROUND((met.composition / met.index_value), 2) AS avg_composition,\n            DENSE_RANK() OVER (\n                PARTITION BY\n                    met.month_year\n                ORDER BY\n                    (met.composition / met.index_value) DESC\n            ) AS ranks\n        FROM\n            fresh_segments.interest_metrics AS met\n                LEFT JOIN fresh_segments.interest_map AS map ON met.interest_id = map.id\n    )\nSELECT\n    month_year,\n    interest_name,\n    avg_composition,\n    ranks\nFROM\n    ranked_avg_comp\nWHERE\n    1 = 1\n    AND ranks &lt;= 10\nORDER BY\n    month_year ASC,\n    avg_composition DESC;\n\"\"\"\n\navg_composition_bv_month_year = athena.query(\n    database=database, query=query, ctas_approach=ctas_approach\n)\navg_composition_bv_month_year\n</pre> query = \"\"\"  WITH     ranked_avg_comp AS (         SELECT             met.month_year,             map.interest_name,             ROUND((met.composition / met.index_value), 2) AS avg_composition,             DENSE_RANK() OVER (                 PARTITION BY                     met.month_year                 ORDER BY                     (met.composition / met.index_value) DESC             ) AS ranks         FROM             fresh_segments.interest_metrics AS met                 LEFT JOIN fresh_segments.interest_map AS map ON met.interest_id = map.id     ) SELECT     month_year,     interest_name,     avg_composition,     ranks FROM     ranked_avg_comp WHERE     1 = 1     AND ranks &lt;= 10 ORDER BY     month_year ASC,     avg_composition DESC; \"\"\"  avg_composition_bv_month_year = athena.query(     database=database, query=query, ctas_approach=ctas_approach ) avg_composition_bv_month_year Out[74]: month_year interest_name avg_composition ranks 0 2018-07-01 Las Vegas Trip Planners 7.36 1 1 2018-07-01 Gym Equipment Owners 6.94 2 2 2018-07-01 Cosmetics and Beauty Shoppers 6.78 3 3 2018-07-01 Luxury Retail Shoppers 6.61 4 4 2018-07-01 Furniture Shoppers 6.51 5 ... ... ... ... ... 135 2019-08-01 Luxury Retail Shoppers 2.59 6 136 2019-08-01 Furniture Shoppers 2.59 7 137 2019-08-01 Marijuana Legalization Advocates 2.56 8 138 2019-08-01 Medicare Researchers 2.55 9 139 2019-08-01 Recently Retired Individuals 2.53 10 <p>140 rows \u00d7 4 columns</p> In\u00a0[75]: Copied! <pre>avg_comp_wide = avg_composition_bv_month_year.pivot(\n    index=\"month_year\", columns=\"interest_name\", values=\"avg_composition\"\n)\n\navg_comp_wide.index = avg_comp_wide.index.astype(pd.StringDtype())\navg_comp_wide\n</pre> avg_comp_wide = avg_composition_bv_month_year.pivot(     index=\"month_year\", columns=\"interest_name\", values=\"avg_composition\" )  avg_comp_wide.index = avg_comp_wide.index.astype(pd.StringDtype()) avg_comp_wide Out[75]: interest_name Alabama Trip Planners Asian Food Enthusiasts Chelsea Fans Christmas Celebration Researchers Cosmetics and Beauty Shoppers Cruise Travel Intenders Family Adventures Travelers Furniture Shoppers Gamers Gym Equipment Owners ... Nursing and Physicians Assistant Journal Researchers PlayStation Enthusiasts Readers of Catholic News Readers of Honduran Content Recently Retired Individuals Restaurant Supply Shoppers Solar Energy Researchers Teen Girl Clothing Shoppers Video Gamers Work Comes First Travelers month_year 2018-07-01 NaN 6.10 NaN NaN 6.78 NaN 4.85 6.51 NaN 6.94 ... NaN NaN NaN NaN 5.72 NaN NaN NaN NaN 4.80 2018-08-01 4.83 5.68 NaN NaN 6.28 NaN NaN 6.30 NaN 6.62 ... NaN NaN NaN NaN 5.58 NaN NaN NaN NaN 5.70 2018-09-01 7.27 NaN NaN 6.47 NaN NaN NaN NaN NaN NaN ... 6.70 NaN NaN 7.60 NaN 6.25 6.24 6.53 NaN 8.26 2018-10-01 7.10 NaN NaN 6.72 NaN NaN NaN NaN NaN NaN ... 7.02 NaN NaN 7.02 NaN NaN 6.50 6.78 NaN 9.14 2018-11-01 6.69 NaN NaN 6.08 NaN NaN NaN NaN NaN NaN ... 6.65 NaN NaN 7.09 NaN 5.59 7.05 5.95 NaN 8.28 2018-12-01 6.68 NaN 5.86 6.09 NaN NaN NaN NaN NaN NaN ... 6.96 NaN NaN 6.58 NaN NaN 6.55 6.38 NaN 8.31 2019-01-01 6.44 NaN NaN 5.65 NaN NaN NaN NaN NaN NaN ... 6.46 NaN 5.48 6.67 NaN NaN 7.05 5.96 NaN 7.66 2019-02-01 6.65 NaN NaN 5.98 NaN NaN NaN NaN NaN NaN ... 6.84 6.23 NaN 6.24 NaN NaN 6.58 6.29 NaN 7.66 2019-03-01 6.54 NaN NaN 5.61 NaN NaN NaN NaN NaN NaN ... 6.52 6.06 5.65 6.21 NaN NaN 6.40 6.01 NaN NaN 2019-04-01 6.21 NaN NaN NaN NaN NaN NaN NaN NaN NaN ... 6.01 5.52 5.30 6.02 NaN 5.07 6.28 5.39 NaN NaN 2019-05-01 3.34 NaN NaN NaN NaN NaN NaN NaN 3.29 NaN ... 3.15 3.55 4.08 4.41 NaN NaN 3.92 NaN 3.19 NaN 2019-06-01 NaN 2.52 NaN NaN 2.55 2.2 NaN 2.39 NaN 2.55 ... NaN NaN NaN NaN 2.27 NaN NaN NaN NaN NaN 2019-07-01 NaN 2.78 NaN NaN 2.78 NaN NaN 2.79 NaN 2.79 ... NaN NaN NaN NaN 2.72 NaN NaN NaN NaN NaN 2019-08-01 NaN 2.68 NaN NaN 2.73 NaN NaN 2.59 NaN 2.72 ... NaN NaN NaN NaN 2.53 NaN 2.66 NaN NaN NaN <p>14 rows \u00d7 30 columns</p> In\u00a0[76]: Copied! <pre>plt.figure(figsize=(12, 8))\nsns.heatmap(avg_comp_wide.T, cmap=\"coolwarm\", annot=True, fmt=\".1f\", linewidths=0.5)\nplt.title(\"Heatmap of Average Composition (Top 10 Interests)\")\nplt.xlabel(\"Month-Year\")\nplt.ylabel(\"Interest Name\")\nplt.tight_layout()\nplt.show();\n</pre> plt.figure(figsize=(12, 8)) sns.heatmap(avg_comp_wide.T, cmap=\"coolwarm\", annot=True, fmt=\".1f\", linewidths=0.5) plt.title(\"Heatmap of Average Composition (Top 10 Interests)\") plt.xlabel(\"Month-Year\") plt.ylabel(\"Interest Name\") plt.tight_layout() plt.show(); In\u00a0[73]: Copied! <pre>query = \"\"\" \nWITH\n    ranked_avg_comp AS (\n        SELECT\n            met.month_year,\n            map.interest_name,\n            ROUND((met.composition / met.index_value), 2) AS avg_composition,\n            DENSE_RANK() OVER (\n                PARTITION BY\n                    met.month_year\n                ORDER BY\n                    (met.composition / met.index_value) DESC\n            ) AS ranks\n        FROM\n            fresh_segments.interest_metrics AS met\n                LEFT JOIN fresh_segments.interest_map AS map ON met.interest_id = map.id\n    )\nSELECT\n    interest_name,\n    COUNT(*) AS count\nFROM\n    ranked_avg_comp\nWHERE\n    1 = 1\n    AND ranks &lt;= 10\nGROUP BY\n    interest_name\nORDER BY\n    count DESC;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  WITH     ranked_avg_comp AS (         SELECT             met.month_year,             map.interest_name,             ROUND((met.composition / met.index_value), 2) AS avg_composition,             DENSE_RANK() OVER (                 PARTITION BY                     met.month_year                 ORDER BY                     (met.composition / met.index_value) DESC             ) AS ranks         FROM             fresh_segments.interest_metrics AS met                 LEFT JOIN fresh_segments.interest_map AS map ON met.interest_id = map.id     ) SELECT     interest_name,     COUNT(*) AS count FROM     ranked_avg_comp WHERE     1 = 1     AND ranks &lt;= 10 GROUP BY     interest_name ORDER BY     count DESC; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[73]: interest_name count 0 Luxury Bedding Shoppers 10 1 Alabama Trip Planners 10 2 Solar Energy Researchers 10 3 Readers of Honduran Content 9 4 New Years Eve Party Ticket Purchasers 9 5 Nursing and Physicians Assistant Journal Resea... 9 6 Work Comes First Travelers 8 7 Teen Girl Clothing Shoppers 8 8 Christmas Celebration Researchers 7 9 Asian Food Enthusiasts 5 10 Recently Retired Individuals 5 11 Luxury Retail Shoppers 5 12 Gym Equipment Owners 5 13 Cosmetics and Beauty Shoppers 5 14 Las Vegas Trip Planners 5 15 Furniture Shoppers 5 16 PlayStation Enthusiasts 4 17 Readers of Catholic News 4 18 Restaurant Supply Shoppers 3 19 Medicare Researchers 3 20 Medicare Provider Researchers 2 21 Gamers 1 22 Medicare Price Shoppers 1 23 Luxury Boutique Hotel Researchers 1 24 Marijuana Legalization Advocates 1 25 Chelsea Fans 1 26 Family Adventures Travelers 1 27 Video Gamers 1 28 Cruise Travel Intenders 1 29 HDTV Researchers 1 In\u00a0[71]: Copied! <pre>query = \"\"\" \nWITH\n    ranked_avg_comp AS (\n        SELECT\n            met.month_year,\n            map.interest_name,\n            ROUND((met.composition / met.index_value), 2) AS avg_composition,\n            DENSE_RANK() OVER (\n                PARTITION BY\n                    met.month_year\n                ORDER BY\n                    (met.composition / met.index_value) DESC\n            ) AS ranks\n        FROM\n            fresh_segments.interest_metrics AS met\n                LEFT JOIN fresh_segments.interest_map AS map ON met.interest_id = map.id\n    )\nSELECT\n    month_year,\n    AVG(avg_composition) AS avg_of_avg_composition\nFROM\n    ranked_avg_comp\nWHERE\n    1 = 1\n    AND ranks &lt;= 10\nGROUP BY\n    month_year\nORDER BY\n    month_year ASC;\n\"\"\"\n\navg_of_avg_comp = athena.query(\n    database=database, query=query, ctas_approach=ctas_approach\n)\navg_of_avg_comp\n</pre> query = \"\"\"  WITH     ranked_avg_comp AS (         SELECT             met.month_year,             map.interest_name,             ROUND((met.composition / met.index_value), 2) AS avg_composition,             DENSE_RANK() OVER (                 PARTITION BY                     met.month_year                 ORDER BY                     (met.composition / met.index_value) DESC             ) AS ranks         FROM             fresh_segments.interest_metrics AS met                 LEFT JOIN fresh_segments.interest_map AS map ON met.interest_id = map.id     ) SELECT     month_year,     AVG(avg_composition) AS avg_of_avg_composition FROM     ranked_avg_comp WHERE     1 = 1     AND ranks &lt;= 10 GROUP BY     month_year ORDER BY     month_year ASC; \"\"\"  avg_of_avg_comp = athena.query(     database=database, query=query, ctas_approach=ctas_approach ) avg_of_avg_comp Out[71]: month_year avg_of_avg_composition 0 2018-07-01 6.038 1 2018-08-01 5.945 2 2018-09-01 6.895 3 2018-10-01 7.066 4 2018-11-01 6.623 5 2018-12-01 6.652 6 2019-01-01 6.399 7 2019-02-01 6.579 8 2019-03-01 6.168 9 2019-04-01 5.750 10 2019-05-01 3.537 11 2019-06-01 2.427 12 2019-07-01 2.765 13 2019-08-01 2.631 In\u00a0[72]: Copied! <pre>fig, ax = plt.subplots(figsize=(16, 8))\n\nax.plot(\n    avg_of_avg_comp[\"month_year\"], avg_of_avg_comp[\"avg_of_avg_composition\"], marker=\"o\"\n)\nax.set_title(\"Average of Average Composition Over Time\")\nax.set_xlabel(\"Month-Year\")\nplt.tight_layout()\nplt.show();\n</pre> fig, ax = plt.subplots(figsize=(16, 8))  ax.plot(     avg_of_avg_comp[\"month_year\"], avg_of_avg_comp[\"avg_of_avg_composition\"], marker=\"o\" ) ax.set_title(\"Average of Average Composition Over Time\") ax.set_xlabel(\"Month-Year\") plt.tight_layout() plt.show(); In\u00a0[156]: Copied! <pre>query = \"\"\" \nSELECT\n    met.month_year,\n    map.interest_name,\n    ROUND((met.composition / met.index_value), 2) AS avg_composition,\n    DENSE_RANK() OVER (\n        PARTITION BY\n            met.month_year\n        ORDER BY\n            CAST(met.composition AS DOUBLE) / CAST(met.index_value AS DOUBLE) DESC\n    ) AS ranks\nFROM\n    fresh_segments.interest_metrics AS met\n    LEFT JOIN fresh_segments.interest_map AS map ON met.interest_id = map.id;\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  SELECT     met.month_year,     map.interest_name,     ROUND((met.composition / met.index_value), 2) AS avg_composition,     DENSE_RANK() OVER (         PARTITION BY             met.month_year         ORDER BY             CAST(met.composition AS DOUBLE) / CAST(met.index_value AS DOUBLE) DESC     ) AS ranks FROM     fresh_segments.interest_metrics AS met     LEFT JOIN fresh_segments.interest_map AS map ON met.interest_id = map.id; \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[156]: month_year interest_name avg_composition ranks 0 2018-12-01 Work Comes First Travelers 8.31 1 1 2018-12-01 Nursing and Physicians Assistant Journal Resea... 6.96 2 2 2018-12-01 Alabama Trip Planners 6.68 3 3 2018-12-01 Luxury Bedding Shoppers 6.63 4 4 2018-12-01 Readers of Honduran Content 6.58 5 ... ... ... ... ... 13074 2019-08-01 Small Business Employees 1.14 1072 13075 2019-08-01 Toronto Blue Jays Fans 1.11 1073 13076 2019-08-01 Price Conscious Home Shoppers 1.10 1074 13077 2019-08-01 Pet Store Goers 1.06 1075 13078 2019-08-01 Health &amp; Fitness 0.96 1076 <p>13079 rows \u00d7 4 columns</p> In\u00a0[160]: Copied! <pre>query = \"\"\" \nWITH ranked_avg_comp_by_month AS (\n    SELECT\n        met.month_year,\n        map.interest_name,\n        ROUND((met.composition / met.index_value), 2) AS avg_composition,\n        DENSE_RANK() OVER (\n            PARTITION BY\n                met.month_year\n            ORDER BY\n                CAST(met.composition AS DOUBLE) / CAST(met.index_value AS DOUBLE) DESC\n        ) AS ranks\n    FROM\n        fresh_segments.interest_metrics AS met\n        LEFT JOIN fresh_segments.interest_map AS map ON met.interest_id = map.id\n)\nSELECT\n    month_year,\n    interest_name,\n    avg_composition AS max_avg_composition,\n    ROUND(AVG(avg_composition) OVER w_3_months, 2) AS three_month_moving_avg,\n    CASE \n        WHEN LAG(avg_composition, 1) OVER w IS NULL \n        THEN NULL \n        ELSE CONCAT_WS(': ', LAG(interest_name, 1) OVER w, CAST(LAG(avg_composition, 1) OVER w AS VARCHAR))\n    END AS one_month_ago,\n    CASE \n        WHEN LAG(avg_composition, 2) OVER w IS NULL \n        THEN NULL \n        ELSE CONCAT_WS(': ', LAG(interest_name, 2) OVER w, CAST(LAG(avg_composition, 2) OVER w AS VARCHAR))\n    END AS two_months_ago\nFROM\n    ranked_avg_comp_by_month\nWHERE\n    1 = 1\n    AND ranks = 1\nWINDOW\n    w AS (ORDER BY month_year ASC),\n    w_3_months AS (ORDER BY month_year ASC ROWS BETWEEN 2 PRECEDING AND CURRENT ROW);\n\"\"\"\n\nathena.query(database=database, query=query, ctas_approach=ctas_approach)\n</pre> query = \"\"\"  WITH ranked_avg_comp_by_month AS (     SELECT         met.month_year,         map.interest_name,         ROUND((met.composition / met.index_value), 2) AS avg_composition,         DENSE_RANK() OVER (             PARTITION BY                 met.month_year             ORDER BY                 CAST(met.composition AS DOUBLE) / CAST(met.index_value AS DOUBLE) DESC         ) AS ranks     FROM         fresh_segments.interest_metrics AS met         LEFT JOIN fresh_segments.interest_map AS map ON met.interest_id = map.id ) SELECT     month_year,     interest_name,     avg_composition AS max_avg_composition,     ROUND(AVG(avg_composition) OVER w_3_months, 2) AS three_month_moving_avg,     CASE          WHEN LAG(avg_composition, 1) OVER w IS NULL          THEN NULL          ELSE CONCAT_WS(': ', LAG(interest_name, 1) OVER w, CAST(LAG(avg_composition, 1) OVER w AS VARCHAR))     END AS one_month_ago,     CASE          WHEN LAG(avg_composition, 2) OVER w IS NULL          THEN NULL          ELSE CONCAT_WS(': ', LAG(interest_name, 2) OVER w, CAST(LAG(avg_composition, 2) OVER w AS VARCHAR))     END AS two_months_ago FROM     ranked_avg_comp_by_month WHERE     1 = 1     AND ranks = 1 WINDOW     w AS (ORDER BY month_year ASC),     w_3_months AS (ORDER BY month_year ASC ROWS BETWEEN 2 PRECEDING AND CURRENT ROW); \"\"\"  athena.query(database=database, query=query, ctas_approach=ctas_approach) Out[160]: month_year interest_name max_avg_composition three_month_moving_avg one_month_ago two_months_ago 0 2018-07-01 Las Vegas Trip Planners 7.36 7.36 &lt;NA&gt; &lt;NA&gt; 1 2018-08-01 Las Vegas Trip Planners 7.21 7.29 Las Vegas Trip Planners: 7.36 &lt;NA&gt; 2 2018-09-01 Work Comes First Travelers 8.26 7.61 Las Vegas Trip Planners: 7.21 Las Vegas Trip Planners: 7.36 3 2018-10-01 Work Comes First Travelers 9.14 8.20 Work Comes First Travelers: 8.26 Las Vegas Trip Planners: 7.21 4 2018-11-01 Work Comes First Travelers 8.28 8.56 Work Comes First Travelers: 9.14 Work Comes First Travelers: 8.26 5 2018-12-01 Work Comes First Travelers 8.31 8.58 Work Comes First Travelers: 8.28 Work Comes First Travelers: 9.14 6 2019-01-01 Work Comes First Travelers 7.66 8.08 Work Comes First Travelers: 8.31 Work Comes First Travelers: 8.28 7 2019-02-01 Work Comes First Travelers 7.66 7.88 Work Comes First Travelers: 7.66 Work Comes First Travelers: 8.31 8 2019-03-01 Alabama Trip Planners 6.54 7.29 Work Comes First Travelers: 7.66 Work Comes First Travelers: 7.66 9 2019-04-01 Solar Energy Researchers 6.28 6.83 Alabama Trip Planners: 6.54 Work Comes First Travelers: 7.66 10 2019-05-01 Readers of Honduran Content 4.41 5.74 Solar Energy Researchers: 6.28 Alabama Trip Planners: 6.54 11 2019-06-01 Las Vegas Trip Planners 2.77 4.49 Readers of Honduran Content: 4.41 Solar Energy Researchers: 6.28 12 2019-07-01 Las Vegas Trip Planners 2.82 3.33 Las Vegas Trip Planners: 2.77 Readers of Honduran Content: 4.41 13 2019-08-01 Cosmetics and Beauty Shoppers 2.73 2.77 Las Vegas Trip Planners: 2.82 Las Vegas Trip Planners: 2.77 In\u00a0[162]: Copied! <pre>query = \"\"\" \nWITH ranked_avg_comp_by_month AS (\n    SELECT\n        met.month_year,\n        map.interest_name,\n        ROUND((met.composition / met.index_value), 2) AS avg_composition,\n        DENSE_RANK() OVER (\n            PARTITION BY\n                met.month_year\n            ORDER BY\n                CAST(met.composition AS DOUBLE) / CAST(met.index_value AS DOUBLE) DESC\n        ) AS ranks\n    FROM\n        fresh_segments.interest_metrics AS met\n        LEFT JOIN fresh_segments.interest_map AS map ON met.interest_id = map.id\n),\n\nwindow_computations AS (\n    SELECT\n        month_year,\n        interest_name,\n        avg_composition AS max_avg_composition,\n        ROUND(AVG(avg_composition) OVER w_3_months, 2) AS three_month_moving_avg,\n        CASE \n            WHEN LAG(avg_composition, 1) OVER w IS NULL \n            THEN NULL \n            ELSE CONCAT_WS(': ', LAG(interest_name, 1) OVER w, CAST(LAG(avg_composition, 1) OVER w AS VARCHAR))\n        END AS one_month_ago,\n        CASE \n            WHEN LAG(avg_composition, 2) OVER w IS NULL \n            THEN NULL \n            ELSE CONCAT_WS(': ', LAG(interest_name, 2) OVER w, CAST(LAG(avg_composition, 2) OVER w AS VARCHAR))\n        END AS two_months_ago\n    FROM\n        ranked_avg_comp_by_month\n    WHERE\n        1 = 1\n        AND ranks = 1\n    WINDOW\n        w AS (ORDER BY month_year ASC),\n        w_3_months AS (ORDER BY month_year ASC ROWS BETWEEN 2 PRECEDING AND CURRENT ROW)\n    )\nSELECT\n    *\nFROM\n    window_computations\nWHERE\n    1 = 1\n    AND (one_month_ago IS NOT NULL AND two_months_ago IS NOT NULL)\nORDER BY\n    month_year ASC;\n\"\"\"\n\nmax_avg_comp_by_month_year = athena.query(\n    database=database, query=query, ctas_approach=ctas_approach\n)\nmax_avg_comp_by_month_year\n</pre> query = \"\"\"  WITH ranked_avg_comp_by_month AS (     SELECT         met.month_year,         map.interest_name,         ROUND((met.composition / met.index_value), 2) AS avg_composition,         DENSE_RANK() OVER (             PARTITION BY                 met.month_year             ORDER BY                 CAST(met.composition AS DOUBLE) / CAST(met.index_value AS DOUBLE) DESC         ) AS ranks     FROM         fresh_segments.interest_metrics AS met         LEFT JOIN fresh_segments.interest_map AS map ON met.interest_id = map.id ),  window_computations AS (     SELECT         month_year,         interest_name,         avg_composition AS max_avg_composition,         ROUND(AVG(avg_composition) OVER w_3_months, 2) AS three_month_moving_avg,         CASE              WHEN LAG(avg_composition, 1) OVER w IS NULL              THEN NULL              ELSE CONCAT_WS(': ', LAG(interest_name, 1) OVER w, CAST(LAG(avg_composition, 1) OVER w AS VARCHAR))         END AS one_month_ago,         CASE              WHEN LAG(avg_composition, 2) OVER w IS NULL              THEN NULL              ELSE CONCAT_WS(': ', LAG(interest_name, 2) OVER w, CAST(LAG(avg_composition, 2) OVER w AS VARCHAR))         END AS two_months_ago     FROM         ranked_avg_comp_by_month     WHERE         1 = 1         AND ranks = 1     WINDOW         w AS (ORDER BY month_year ASC),         w_3_months AS (ORDER BY month_year ASC ROWS BETWEEN 2 PRECEDING AND CURRENT ROW)     ) SELECT     * FROM     window_computations WHERE     1 = 1     AND (one_month_ago IS NOT NULL AND two_months_ago IS NOT NULL) ORDER BY     month_year ASC; \"\"\"  max_avg_comp_by_month_year = athena.query(     database=database, query=query, ctas_approach=ctas_approach ) max_avg_comp_by_month_year Out[162]: month_year interest_name max_avg_composition three_month_moving_avg one_month_ago two_months_ago 0 2018-09-01 Work Comes First Travelers 8.26 7.61 Las Vegas Trip Planners: 7.21 Las Vegas Trip Planners: 7.36 1 2018-10-01 Work Comes First Travelers 9.14 8.20 Work Comes First Travelers: 8.26 Las Vegas Trip Planners: 7.21 2 2018-11-01 Work Comes First Travelers 8.28 8.56 Work Comes First Travelers: 9.14 Work Comes First Travelers: 8.26 3 2018-12-01 Work Comes First Travelers 8.31 8.58 Work Comes First Travelers: 8.28 Work Comes First Travelers: 9.14 4 2019-01-01 Work Comes First Travelers 7.66 8.08 Work Comes First Travelers: 8.31 Work Comes First Travelers: 8.28 5 2019-02-01 Work Comes First Travelers 7.66 7.88 Work Comes First Travelers: 7.66 Work Comes First Travelers: 8.31 6 2019-03-01 Alabama Trip Planners 6.54 7.29 Work Comes First Travelers: 7.66 Work Comes First Travelers: 7.66 7 2019-04-01 Solar Energy Researchers 6.28 6.83 Alabama Trip Planners: 6.54 Work Comes First Travelers: 7.66 8 2019-05-01 Readers of Honduran Content 4.41 5.74 Solar Energy Researchers: 6.28 Alabama Trip Planners: 6.54 9 2019-06-01 Las Vegas Trip Planners 2.77 4.49 Readers of Honduran Content: 4.41 Solar Energy Researchers: 6.28 10 2019-07-01 Las Vegas Trip Planners 2.82 3.33 Las Vegas Trip Planners: 2.77 Readers of Honduran Content: 4.41 11 2019-08-01 Cosmetics and Beauty Shoppers 2.73 2.77 Las Vegas Trip Planners: 2.82 Las Vegas Trip Planners: 2.77 <ol> <li><p>Seasonality:</p> <ul> <li>The composition metric might reflect customer behaviors that vary with seasons. For instance, interest in \"Vacation Rental Accommodation Researchers\" could spike during holiday seasons or summer months.</li> </ul> </li> <li><p>Marketing Campaigns:</p> <ul> <li>Targeted advertising or promotional campaigns could drive temporary interest in specific segments, increasing their composition metrics.</li> </ul> </li> <li><p>Shifts in Business Focus:</p> <ul> <li>Fresh Segments might adjust their offerings or emphasis on particular interests, impacting customer interactions with specific segments.</li> </ul> </li> <li><p>Changes in Client Lists:</p> <ul> <li>Fluctuations in the client base (e.g., onboarding new clients or losing existing ones) could affect the overall average composition.</li> </ul> </li> <li><p>Measurement Issues:</p> <ul> <li>A sudden, unexplained change might indicate a data quality or calculation issue, especially if it doesn't align with known business activities.</li> </ul> </li> </ol>"},{"location":"fresh_segments/#global","title":"Global\u00b6","text":""},{"location":"fresh_segments/#problem-statement","title":"Problem Statement\u00b6","text":"<p>Fresh Segments is a digital marketing agency specializing in analyzing trends in online ad click behavior for businesses with targeted customer bases. The agency receives customer lists from clients and aggregates interest metrics to create a comprehensive dataset for further analysis.</p> <p>The main output includes detailed composition and rankings of various interests, highlighting the proportion of customers who engaged with online assets related to each interest on a monthly basis.</p> <p>The goal of this case study is to analyze these aggregated metrics for a sample client to provide high-level insights into their customer base and associated interests.</p>"},{"location":"fresh_segments/#entity-relationship-diagram","title":"Entity Relationship Diagram\u00b6","text":""},{"location":"fresh_segments/#tables","title":"Tables\u00b6","text":""},{"location":"fresh_segments/#duplicate-check","title":"Duplicate Check\u00b6","text":"<p>Group by all columns and count the occurrences, while excluding rows with NULL values to simplify the grouping process.</p>"},{"location":"fresh_segments/#interest-metrics","title":"Interest Metrics\u00b6","text":""},{"location":"fresh_segments/#null-check","title":"NULL Check\u00b6","text":"<p>Number of NULL values in each column.</p>"},{"location":"fresh_segments/#eda-data-preprocessing","title":"EDA &amp; Data Preprocessing\u00b6","text":""},{"location":"fresh_segments/#q1-q3","title":"Q1 &amp; Q3\u00b6","text":"<p>There are two preprocessing steps that can be helpful:</p> <ol> <li><p>Update the <code>fresh_segments.interest_metrics</code> table by modifying the <code>month_year</code> column to be a <code>DATE</code> data type with the start of the month as the day.</p> </li> <li><p>Handling the null values in the <code>fresh_segments.interest_metrics</code> table.</p> </li> </ol> <p>First, check if there are any records with non-missing interest ID but missing <code>month_year</code> value:</p>"},{"location":"fresh_segments/#missing-both-id-date","title":"Missing Both ID &amp; Date\u00b6","text":"<p>For all records that have both missing <code>month_year</code> and <code>interest_id</code> values in the <code>fresh_segments.interest_metrics</code> table, we unfortunately cannot make any good business interpretations on their interest metrics since we do not know what interest they are associated with.</p>"},{"location":"fresh_segments/#missing-date","title":"Missing Date\u00b6","text":"<p>For the record where the <code>interest_id</code> is known (specifically <code>interest_id = 21246</code>) but the <code>month_year</code> value is missing, we want to determine if there are other records in the dataset with the same <code>interest_id</code> that have a <code>month_year</code> value that is not missing. This allows us to potentially infer or cross-reference information about the missing <code>month_year</code> value.</p>"},{"location":"fresh_segments/#q2","title":"Q2\u00b6","text":"<p>What is count of records in the <code>fresh_segments.interest_metrics</code> table for each <code>month_year</code> value sorted in chronological order (earliest to latest) with the null values appearing first?</p>"},{"location":"fresh_segments/#q4","title":"Q4\u00b6","text":"<p>How many <code>interest_id</code> values exist in the <code>fresh_segments.interest_metrics</code> table but not in the <code>fresh_segments.interest_map</code> table? What about the other way around?</p> <p>Let <code>A</code> be the set of <code>interest_id</code> values in the <code>fresh_segments.interest_metrics</code> table and <code>B</code> be the set of <code>interest_id</code> values in the <code>fresh_segments.interest_map</code> table.</p>"},{"location":"fresh_segments/#a-b-b-a","title":"A \\ B &amp; B \\ A\u00b6","text":"<p>We can use a full outer join, which returns all rows from both tables regardless of whether there is a match or not:</p>"},{"location":"fresh_segments/#q5","title":"Q5\u00b6","text":"<p>Summarise the <code>id</code> values in the <code>fresh_segments.interest_map</code> by its total record count in this table.</p>"},{"location":"fresh_segments/#q6","title":"Q6\u00b6","text":"<p>What sort of table join would be most appropriate to combine the <code>fresh_segments.interest_metrics</code> and <code>fresh_segments.interest_map</code> tables?</p> <p>A left join of the <code>interest_map</code> table onto <code>interest_metrics</code> is most appropriate because the asymmetric set difference shows that all <code>interest_id</code> values in <code>interest_metrics</code> are present in <code>interest_map</code>.</p> <p>This ensures all rows from <code>interest_metrics</code> are retained, while relevant data from <code>interest_map</code> is included without unnecessary rows from <code>interest_map</code> that have no match in <code>interest_metrics</code>.</p>"},{"location":"fresh_segments/#q7","title":"Q7\u00b6","text":"<p>Are there any records in the joined table where the <code>month_year</code> value is before the <code>created_at</code> value from the <code>fresh_segments.interest_map</code> table? Are these values valid and why?</p>"},{"location":"fresh_segments/#interest-analysis","title":"Interest Analysis\u00b6","text":""},{"location":"fresh_segments/#q1","title":"Q1\u00b6","text":"<p>Which interests have been present in all <code>month_year</code> dates in our dataset?</p>"},{"location":"fresh_segments/#q2","title":"Q2\u00b6","text":"<p>Using this same <code>total_months</code> measure, calculate the cumulative percentage of all records starting at 14 months - which <code>total_months</code> value passes the $90\\%$ cumulative percentage value?</p>"},{"location":"fresh_segments/#q3","title":"Q3\u00b6","text":"<p>If we were to remove all <code>interest_id</code> values which are lower than the <code>total_months</code> value we found in the previous question - how many total data points would we be removing?</p>"},{"location":"fresh_segments/#q4","title":"Q4\u00b6","text":"<p>Does it make sense to remove these data points from a business perspective? Consider an example where an interest has data for all 14 months compared to one with significantly fewer months, and evaluate what having fewer months of data implies from a segmentation perspective. Additionally, if we include all interests regardless of the number of months they appear, how many unique interests are represented in each month?</p>"},{"location":"fresh_segments/#segment-analysis","title":"Segment Analysis\u00b6","text":""},{"location":"fresh_segments/#q1","title":"Q1\u00b6","text":"<p>Using the complete dataset - which are the top 10 and bottom 10 interests that have the largest composition values in any <code>month_year</code>? Only use the maximum composition value for each interest but keep the corresponding <code>month_year</code>.</p>"},{"location":"fresh_segments/#q2","title":"Q2\u00b6","text":"<p>Which 5 interests had the lowest average <code>ranking</code> value?</p>"},{"location":"fresh_segments/#q3","title":"Q3\u00b6","text":"<p>Which 5 interests had the largest standard deviation in their <code>percentile_ranking</code> value?</p>"},{"location":"fresh_segments/#q4","title":"Q4\u00b6","text":"<p>For the 5 interests found in the previous question - what was minimum and maximum <code>percentile_ranking</code> values for each interest and its corresponding <code>year_month</code> value? Can you describe what is happening for these 5 interests?</p>"},{"location":"fresh_segments/#q5","title":"Q5\u00b6","text":"<p>How would you describe our customers in this segment based off their composition and ranking values? What sort of products or services should we show to these customers and what should we avoid?</p>"},{"location":"fresh_segments/#top-bottom-interests-by-composition-over-time","title":"Top &amp; Bottom Interests By Composition Over Time\u00b6","text":""},{"location":"fresh_segments/#risk-analysis-for-top-10-interests","title":"Risk Analysis for Top 10 Interests\u00b6","text":"<p>To assess the risk associated with focusing on the top 10 interests, we can conduct a risk analysis by evaluating the variability in their composition values over time. One effective way to define risk is to analyze the distribution of composition values for each interest across the observed time period.</p> <p>By quantifying the fluctuations in composition, we can construct a risk score for each of the top 10 interests. These scores reflect the degree of uncertainty or variability in customer engagement with each interest. A higher risk score indicates greater variability, signaling potential uncertainty in prioritizing that interest.</p> <p>This approach allows us to interpret the risk scores as indicators of the stability of each interest\u2019s engagement levels over time, providing actionable insights for prioritization and resource allocation.</p>"},{"location":"fresh_segments/#index-analysis","title":"Index Analysis\u00b6","text":"<p>The <code>index_value</code> is a measure that can be used to reverse calculate the average composition for Fresh Segments\u2019 clients.</p> <p>Average composition can be calculated by dividing the composition column by the <code>index_value</code> column rounded to 2 decimal places.</p> <p>$$ \\text{Average Composition} = \\frac{\\text{Composition}}{\\text{Index Value}} $$</p>"},{"location":"fresh_segments/#q1","title":"Q1\u00b6","text":"<p>What is the top 10 interests by the average composition for each month?</p>"},{"location":"fresh_segments/#q2","title":"Q2\u00b6","text":"<p>For all of these top 10 interests - which interest appears the most often?</p>"},{"location":"fresh_segments/#q3","title":"Q3\u00b6","text":"<p>What is the average of the average composition for the top 10 interests for each month?</p>"},{"location":"fresh_segments/#q4","title":"Q4\u00b6","text":"<p>What is the 3 month rolling average of the max average composition value from September 2018 to August 2019?</p>"},{"location":"fresh_segments/#base-cte","title":"Base CTE\u00b6","text":""},{"location":"fresh_segments/#window-computations-previous-months-data-formatting","title":"Window Computations - Previous Months Data Formatting\u00b6","text":"<pre>CASE \n    WHEN LAG(avg_composition, 1) OVER w IS NULL \n    THEN NULL \n    ELSE CONCAT_WS(': ', LAG(interest_name, 1) OVER w, CAST(LAG(avg_composition, 1) OVER w AS VARCHAR))\nEND AS one_month_ago,\nCASE \n    WHEN LAG(avg_composition, 2) OVER w IS NULL \n    THEN NULL \n    ELSE CONCAT_WS(': ', LAG(interest_name, 2) OVER w, CAST(LAG(avg_composition, 2) OVER w AS VARCHAR))\nEND AS two_months_ago\n</pre> <p>This code creates two columns that show historical data in a formatted string:</p> <ul> <li><code>one_month_ago</code>: Shows data from the previous month</li> <li><code>two_months_ago</code>: Shows data from two months ago</li> </ul> <p>For each row, the logic works as follows:</p> <ol> <li>Check if there's missing data (<code>NULL</code>) from the previous period</li> <li>If there is missing data, output <code>NULL</code></li> <li>If data exists, combine the interest name and its composition value into a formatted string like \"Interest Name: 7.36\"</li> </ol> <p>The LAG function is what looks back in time - <code>LAG(1)</code> looks back one month, <code>LAG(2)</code> looks back two months. This gives us a rolling historical view of which interests were most popular in previous months.</p> <p>For example, if we're looking at August's data:</p> <ul> <li><code>one_month_ago</code> shows July's top interest</li> <li><code>two_months_ago</code> shows June's top interest</li> </ul>"},{"location":"fresh_segments/#final-results","title":"Final Results\u00b6","text":""},{"location":"fresh_segments/#q5","title":"Q5\u00b6","text":"<p>Provide a possible reason why the max average composition might change from month to month? Could it signal something is not quite right with the overall business model for Fresh Segments?</p>"},{"location":"fresh_segments/#potential-red-flags-for-business-model","title":"Potential Red Flags for Business Model\u00b6","text":"<p>If the changes in the maximum average composition cannot be explained by predictable factors like seasonality or campaigns, it might indicate:</p> <ul> <li>Market Misalignment: The target audience for Fresh Segments' offerings may not consistently align with their clients' needs.</li> <li>Client Retention Issues: Variability might reflect challenges in maintaining a steady client base.</li> <li>Segment Relevance: Some interest segments may not be as relevant or valuable to the clients as anticipated.</li> </ul>"},{"location":"utils/","title":"Utils","text":"<p>Create a boto3 session with the provided profile name. If an AWS role ARN is provided, the session will assume the role. There are two options for providing credentials:</p> <ol> <li> <p>Provide the profile name containing AWS credentials stored in the ~/.aws/credentials file    (e.g., access key ID, secret access key, or session token). These credentials must belong    to a principal (e.g., an IAM user) that has the necessary permissions to interact with the    AWS services required by the application.</p> </li> <li> <p>Provide the AWS role ARN, in addition to the profile name, to assume, which grants the necessary    permissions to interact with AWS services. If a role ARN is provided, the profile's associated    principal (e.g., an IAM user) must at least have the <code>sts:AssumeRole</code> permission. Additionally,    the role to be assumed must have a trust relationship with the principal that uses the profile's    credentials.</p> </li> </ol> <p>Parameters:</p> Name Type Description Default <code>profile_name</code> <code>str</code> <p>The AWS profile name to use.</p> required <code>role_arn</code> <code>Optional[str]</code> <p>The AWS role ARN to assume. The default is None.</p> <code>None</code> <code>duration_seconds</code> <code>Optional[int]</code> <p>The duration in seconds for which the credentials will be valid. The default is 3600 seconds (1 hour).</p> <code>3600</code> <p>Returns:</p> Type Description <code>Session</code> <p>A boto3 session</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from utils import create_session\n&gt;&gt;&gt; boto3_session = create_session(profile_name='my-profile', role_arn='arn:aws:iam::123456789012:role/my-role')\n</code></pre>"}]}